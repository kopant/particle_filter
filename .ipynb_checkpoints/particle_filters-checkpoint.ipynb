{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment specified in environment.yml (environment name map_matching_particle_filter)\n",
    "# conda env create --prefix ./envs -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import glob, re, time\n",
    "\n",
    "import shapely\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.geometry import shape, Point, LineString, MultiLineString\n",
    "from shapely.ops import nearest_points\n",
    "import folium\n",
    "import fiona\n",
    "import pyproj\n",
    "from functools import partial\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics.pairwise import haversine_distances  # Assumes (lat, long) in radians; version 0.22.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/LaCie/Documents/repos/particle_filter'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applications of particle filters: \n",
    "1. Car positioning by map matching, as in http://www.diva-portal.org/smash/get/diva2:316556/FULLTEXT01.pdf\n",
    "    Essentially same approach is described in Davidson, Collin, and Takala (2011). Application of particle filters to map-matching algorithm\n",
    "    Idea also similar to Newson and Krumm (2009), though HMM is used there.\n",
    "    More recent resource: Murphy, Pao, Yuen (2019). Lyft; Map matching when the map is wrong: Efficient on/off road vehicle tracking and map learning\n",
    "\n",
    "Ideas\n",
    "Rao-Blackwellization (use Kalman filter for the linear part of the dynamics model)\n",
    "\n",
    "Initial Approach:\n",
    "Use Newson and Krumm, but modify it to use particle filters instead of HMM.\n",
    "\n",
    "So, for a given route, proceed sequentially over obs, maintaining dist of probable road segments\n",
    "\n",
    "Assumptions (N&K):\n",
    "-remove obs that are not 2*meas dist sigma from previous obs  (eliminate 39% of data in N&K)\n",
    "-ignore roads 200m from obs\n",
    "-zeroize very unlikely particles\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def great_circle_dist(df):\n",
    "    \"\"\"\n",
    "    USED BELOW IN PREPROCESSING\n",
    "    df numpy array [lat, long, lat_prev, long_prev] in degrees\n",
    "    \n",
    "    Returns great-circle distance between point (lat,long) columns and (lat_prev, long_prev) columns, \n",
    "    in meters.\n",
    "    \n",
    "    Modified from https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    df = df.copy()\n",
    "    df = np.deg2rad(df)\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = df[:, 3] - df[:, 1] # lon2 - lon1 \n",
    "    dlat = df[:, 2] - df[:, 0] # lat2 - lat1 \n",
    "    a = np.add(np.square(np.sin(dlat / 2)),\n",
    "               np.multiply(np.cos(df[:, 0]), \n",
    "                           np.multiply(np.cos(df[:, 2]), np.square(np.sin(dlon / 2)))\n",
    "                          )\n",
    "              )\n",
    "    # a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = np.arcsin(np.sqrt(a)) * 2\n",
    "    # c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return r * c * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gps_transition_model:\n",
    "    \"\"\"\n",
    "    Transition model to predict probability of next state given current state; simulate next state.\n",
    "    GPS trace map matching use case. \n",
    "    \n",
    "    N&K use as a proxy an exponential function of the difference in the great-circle distance \n",
    "    between previous observations and previous road points (similar route distance).\n",
    "    Road points use map/ground-truth data. \n",
    "    -Ignore roads 200m from obs\n",
    "    -If a calculated route would require the vehicle to exceed a speed of 50 m/s (112 miles per hour), zeroize\n",
    "    \n",
    "    !!! TODO: Get driving distance from route planner??? Results anticipated to significantly suffer otherwise. !!! \n",
    "    \n",
    "    Other ideas: using dead reckoning.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta=3.0\n",
    "#                  , prob_floor=0.05\n",
    "                 , normalize=True\n",
    "                ):\n",
    "        \"\"\"\n",
    "        beta, float, parametrizes transition probability function\n",
    "        prob_floor, float between 0 and 1, is probability below which road choices are given zero probability\n",
    "        \n",
    "        # beta = 3 in mapzen, https://www.mapzen.com/blog/data-driven-map-matching/\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "#         self.prob_floor = min(max(prob_floor, 0), 1)\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def estimate_beta(self, data):\n",
    "        \"\"\"\n",
    "        NOT USED\n",
    "        \n",
    "        Using rescaled median absolute deviation (MAD). \n",
    "        Ideally using ground truth data (none here).\n",
    "        \"\"\" \n",
    "        self.beta = 3.0  # None\n",
    "        \n",
    "    def prob_transition(self, state_last2, obs_last2, dist_obs_m_prev, time_delta_sec_prev, \n",
    "                        max_allowable_diff_m=2000.0, max_allowable_mph=112.0):\n",
    "        \"\"\"\n",
    "        state_last2, float numpy array, (n * c x 4 lat, long, lat_prev, long_prev), c candidate roads and n particles\n",
    "        obs_last2, float numpy array, (1 x 4 lat, long, lat_prev, long_prev) \n",
    "        dist_obs_m_prev float distance in meters between last two obs.\n",
    "        time_delta_sec_prev float time delta in sec between last two obs.\n",
    "        max_allowable_diff_m float, ignore low probability routes as in N&K; in N&K 2000 meters.\n",
    "        max_allowable_mph, float; ignore routes resulting in unreasonable speeds. 112mph in N&K.\n",
    "        Where present, lat/long in degrees.\n",
    "        \n",
    "        returns probs of transition, (n * c x 1)\n",
    "        \"\"\"\n",
    "        RADIUS_OF_EARTH_M = 6371000\n",
    "        MILES_PER_METER = 0.000621371\n",
    "        HOURS_PER_SECOND = 3600.0\n",
    "        state_last2 = np.deg2rad(state_last2)\n",
    "        obs_last2 = np.deg2rad(obs_last2)\n",
    "        # dist_obs = abs(haversine_distances(obs_last2[:, :2], obs_last2[0, 2:]))  # (1, 1)\n",
    "        # !! TODO - Supposed to be driving distance, not haversine. N&K use a route planner. Can I use shapely???\n",
    "            # Probably fine as generally GPS measurements are taken close together in time- \n",
    "            # Otherwise would have to snap to roads using Shapely and get the distance between the two points. \n",
    "            # How does using driving distance even compare to haversine for obs? Isn't it better to compare apples to apples?\n",
    "        dist_road = \\\n",
    "        np.abs(np.array([haversine_distances(state_last2[r, :2].reshape(1,2), state_last2[r, 2:].reshape(1,2))[0] \n",
    "                         for r in range(state_last2.shape[0])]))  # (n * c, 1)\n",
    "        dist_road = dist_road * RADIUS_OF_EARTH_M  # multiply by Earth radius to get result in meters\n",
    "        diff_dist = np.abs(np.subtract(dist_road, dist_obs_m_prev))  # (n * c, 1)\n",
    "        \n",
    "        # Ignore cases where diff_dist >= max_allowable_diff_m\n",
    "        diff_dist[diff_dist >= max_allowable_diff_m] = np.float(\"inf\")  # zero out these transition probs\n",
    "        # Ignore cases where implied speed of route is >= max_allowable_mph; calculate speed in candidate routes\n",
    "        implied_speed_road = np.divide(dist_road * MILES_PER_METER, \n",
    "                               time_delta_sec_prev * HOURS_PER_SECOND)  # (n * c, 1), meters\n",
    "        diff_dist[implied_speed_road >= max_allowable_mph] = np.float(\"inf\")  # zero out these transition probs\n",
    "        \n",
    "        probs = np.exp(-diff_dist / self.beta) * (1 / self.beta)\n",
    "        # Normalize result?\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gps_sensor_model:\n",
    "    \"\"\"\n",
    "    Sensor model to predict likelihood of an obs given a particle.\n",
    "    GPS trace map matching use case. \n",
    "    \n",
    "    I don't have ground truth, so can't use ML easily, but\n",
    "    N&K use as a proxy a normal distribution of great-circle distance between observation and road, with sigma \n",
    "    estimated from the data.\n",
    "    Road points use map data. \n",
    "    -Zerioze low probability particles (diff in route distance of 2000 m. or more). \n",
    "    \n",
    "    Other ideas: semi-supervised learning. \n",
    "    \n",
    "    Thoughts: need efficient representation for road network that returns connecting roads/nodes for a given node\n",
    "    Want to represent it as a graph/network. \n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=4.07, normalize=True):\n",
    "        self.sigma = sigma\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def estimate_sigma(self, data):\n",
    "        \"\"\"\n",
    "        NOT USED \n",
    "        \n",
    "        Using Gather and Schultze, median-based. \n",
    "        Ideally using ground truth data (none here).\n",
    "        \n",
    "        I guess I could manually match some trips, then use that??\n",
    "        \"\"\" \n",
    "        self.sigma = 4.07 # meters; I have no ground truth so using N&K's empirical estimate\n",
    "#         self.sigma = None # median absolute deviation (MAD) formula adjusted \n",
    "    \n",
    "    def estimate_likelihood(self, states, obs_coords):\n",
    "        \"\"\"\n",
    "        return probability of seeing that obs given potential road state hypotheses, (k x 1)\n",
    "        \n",
    "        states Particles numpy array representing road position hypotheses, (k x 2 lat long).\n",
    "        obs_coords (1 x 2 lat long)\n",
    "        lat long in degrees\n",
    "        \"\"\"\n",
    "        RADIUS_OF_EARTH_M = 6371000\n",
    "        dist_obs_roads = np.abs(haversine_distances(np.deg2rad(states), np.deg2rad(obs_coords)))  # (k x 1)\n",
    "        dist_obs_roads = dist_obs_roads * RADIUS_OF_EARTH_M  # multiply by Earth radius to get result in meters\n",
    "        \n",
    "        probs = \\\n",
    "        np.exp(np.power(dist_obs_roads / self.sigma, 2) * (-0.5)) * \\\n",
    "        (1 / (math.sqrt(2 * math.pi) * self.sigma))\n",
    "        \n",
    "        # Normalize result?\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FOR DEBUGGING\n",
    "road_strtree; n=50; max_road_dist_m=200; prob_floor=0.01; \n",
    "sensor_model=gps_sensor_model(); transition_model=gps_transition_model(); \n",
    "viterbi_trellis=[]; viterbi_trellis_idx=[]\n",
    "\n",
    "prob_floor = min(max(prob_floor, 0), 1)\n",
    "particles = np.array([])\n",
    "weights = np.ones((1, n))\n",
    "\n",
    "fitted = False\n",
    "        \n",
    "obs_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_candidate_roads(obs):\n",
    "    \"\"\"\n",
    "    obs list of floats [lat, long]\n",
    "    max_dist maximum allowable closest distance for prospective matched roads; those whose \n",
    "    closest point is >= max_dist meters away are ignored (200 in N&K).\n",
    "    Returns numpy array float of lat,long closest point to road from obs (N&K).\n",
    "\n",
    "    Leverages shapely and shapefiles for cabspotting data.\n",
    "    Relies on great_circle_dist() helper function. \n",
    "    \"\"\"\n",
    "    # Fiona uses (long, lat) format\n",
    "    obs = Point(obs[::-1])\n",
    "    # Want to consider only matches within max_dist meters\n",
    "    # Need to transform decimal degrees into meters\n",
    "    # Should I deviate from the default value of buffer for a Point??\n",
    "    # According to 200 * 360 / (2 * np.pi * 6371000), a distance of 200m corresponds to 0.0017986432118374611 decimal degrees\n",
    "    # obs.buffer(1.0)\n",
    "    matching_roads = road_strtree.query(obs.buffer(0.002))\n",
    "    if len(matching_roads) == 0:\n",
    "        matching_roads = road_strtree.query(obs.buffer(1.0))  # This should be a big buffer\n",
    "    # If matching roads is still empty, return None\n",
    "    if len(matching_roads) == 0:\n",
    "        print(\"No matching roads found! Aborting particle filter.\")\n",
    "        return None\n",
    "    # Ordered as (obs, road); just take road's coords\n",
    "    # Take only the closest point on the candidate road (1st index)\n",
    "    closest_pts = np.array([[list(p.coords)[0] for p in nearest_points(obs, r)][1] for r in matching_roads])\n",
    "    # Calculate distance between obs and candidate_roads, to filter them\n",
    "    candidate_dist_coords = \\\n",
    "    np.concatenate([closest_pts, np.tile(np.array(obs), (closest_pts.shape[0], 1))], axis=1)\n",
    "    candidate_dists = great_circle_dist(candidate_dist_coords)\n",
    "    # Filtered result of closest point on matching candidate roads\n",
    "    # no. matches 2 (default buffer); 52 (buffer of 0.002); 74 (buffer of 1.0)\n",
    "    candidate_roads = closest_pts[candidate_dists <= max_road_dist_m]\n",
    "    return candidate_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_weights_per_particle(sampled_states, obs_coords):\n",
    "    \"\"\"\n",
    "    Estimate likelihood of particle given evidence, P(evidence|particle).\n",
    "    Uses sensor model.\n",
    "\n",
    "    obs_coords is single observation at time t, pandas series. \n",
    "    \"\"\"\n",
    "    probs = sensor_model.estimate_likelihood(sampled_states, obs_coords.values.reshape(1, 2))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HERE SECOND!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Crossproduct for each particle, all roads\n",
    "state_last2 = np.concatenate([np.concatenate((particles, \n",
    "                                              np.tile(c, (n, 1))), \n",
    "                                             axis=1) \n",
    "                              for c in candidate_roads], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 4)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_last2.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_last2 = np.array(obs_history[-2:]).reshape((1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  37.75134, -122.39488,   37.75136, -122.39527]])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_last2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_obs_m_prev=dist_prev; time_delta_sec_prev=time_delta_prev; max_allowable_diff_m=2000.0; max_allowable_mph=112.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.360479144304016"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_obs_m_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.0"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_delta_sec_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RADIUS_OF_EARTH_M = 6371000\n",
    "MILES_PER_METER = 0.000621371\n",
    "HOURS_PER_SECOND = 3600.0\n",
    "state_last2 = np.deg2rad(state_last2)\n",
    "obs_last2 = np.deg2rad(obs_last2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65888492, -2.13619363,  0.65885333, -2.13619095],\n",
       "       [ 0.65888522, -2.13619324,  0.65885333, -2.13619095],\n",
       "       [ 0.65888522, -2.13619324,  0.65885333, -2.13619095],\n",
       "       ...,\n",
       "       [ 0.65888492, -2.13619363,  0.65891336, -2.13618996],\n",
       "       [ 0.65888522, -2.13619324,  0.65891336, -2.13618996],\n",
       "       [ 0.65888522, -2.13619324,  0.65891336, -2.13618996]])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_last2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_road = \\\n",
    "np.abs(np.array([haversine_distances(state_last2[r, :2].reshape(1,2), state_last2[r, 2:].reshape(1,2))[0] \n",
    " for r in range(state_last2.shape[0])]))  # (n * c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_road = dist_road * RADIUS_OF_EARTH_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_dist = np.abs(np.subtract(dist_road, dist_obs_m_prev))  # (n * c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[167.33782252],\n",
       "       [169.1049927 ],\n",
       "       [169.1049927 ],\n",
       "       ...,\n",
       "       [147.78968822],\n",
       "       [145.7182921 ],\n",
       "       [145.7182921 ]])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 1)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46792129264360227"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(diff_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore cases where diff_dist >= max_allowable_diff_m\n",
    "diff_dist[diff_dist >= max_allowable_diff_m] = np.float(\"inf\")  # zero out these transition probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ignore cases where implied speed of route is >= max_allowable_mph; calculate speed in candidate routes\n",
    "implied_speed_road = np.divide(dist_road * MILES_PER_METER, \n",
    "                               time_delta_sec_prev * HOURS_PER_SECOND)  # (n * c, 1), meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_dist[implied_speed_road >= max_allowable_mph] = np.float(\"inf\")  # zero out these transition probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = np.exp(-diff_dist / beta) * (1 / beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = probs / np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_transition(self, state_last2, obs_last2, dist_obs_m_prev, time_delta_sec_prev, \n",
    "                        max_allowable_diff_m=2000.0, max_allowable_mph=112.0):\n",
    "        \"\"\"\n",
    "        state_last2, float numpy array, (n * c x 4 lat, long, lat_prev, long_prev), c candidate roads and n particles\n",
    "        obs_last2, float numpy array, (1 x 4 lat, long, lat_prev, long_prev) \n",
    "        dist_obs_m_prev float distance in meters between last two obs.\n",
    "        time_delta_sec_prev float time delta in sec between last two obs.\n",
    "        max_allowable_diff_m float, ignore low probability routes as in N&K; in N&K 2000 meters.\n",
    "        max_allowable_mph, float; ignore routes resulting in unreasonable speeds. 112mph in N&K.\n",
    "        Where present, lat/long in degrees.\n",
    "        \n",
    "        returns probs of transition, (n * c x 1)\n",
    "        \"\"\"\n",
    "        RADIUS_OF_EARTH_M = 6371000\n",
    "        MILES_PER_METER = 0.000621371\n",
    "        HOURS_PER_SECOND = 3600.0\n",
    "        state_last2 = np.deg2rad(state_last2)\n",
    "        obs_last2 = np.deg2rad(obs_last2)\n",
    "        # dist_obs = abs(haversine_distances(obs_last2[:, :2], obs_last2[0, 2:]))  # (1, 1)\n",
    "        # !! TODO - Supposed to be driving distance, not haversine. N&K use a route planner. Can I use shapely???\n",
    "            # Probably fine as generally GPS measurements are taken close together in time- \n",
    "            # Otherwise would have to snap to roads using Shapely and get the distance between the two points. \n",
    "            # How does using driving distance even compare to haversine for obs? Isn't it better to compare apples to apples?\n",
    "        dist_road = \\\n",
    "        np.abs(np.array([haversine_distances(state_last2[r, :2].reshape(1,2), state_last2[r, 2:].reshape(1,2))[0] \n",
    "                         for r in range(state_last2.shape[0])]))  # (n * c, 1)\n",
    "        dist_road = dist_road * RADIUS_OF_EARTH_M  # multiply by Earth radius to get result in meters\n",
    "        diff_dist = np.abs(np.subtract(dist_road, dist_obs_m_prev))  # (n * c, 1)\n",
    "        \n",
    "        # Ignore cases where diff_dist >= max_allowable_diff_m\n",
    "        diff_dist[diff_dist >= max_allowable_diff_m] = np.float(\"inf\")  # zero out these transition probs\n",
    "        # Ignore cases where implied speed of route is >= max_allowable_mph; calculate speed in candidate routes\n",
    "        implied_speed_road = np.divide(dist_road * MILES_PER_METER, \n",
    "                               time_delta_sec_prev * HOURS_PER_SECOND)  # (n * c, 1), meters\n",
    "        diff_dist[implied_speed_road >= max_allowable_mph] = np.float(\"inf\")  # zero out these transition probs\n",
    "        \n",
    "        probs = np.exp(-diff_dist / self.beta) * (1 / self.beta)\n",
    "        # Normalize result?\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = transition_model.prob_transition(state_last2, obs_last2, \n",
    "                                         dist_prev, time_delta_prev)  # (n * c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015])]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(probs)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(probs <= prob_floor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs[probs <= prob_floor] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = probs / np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015])]"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(probs)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_transition_model(candidate_roads, dist_prev, time_delta_prev):\n",
    "    \"\"\"\n",
    "    returns probabilities of transition, given current particles\n",
    "    numpy array of floats between 0 and 1, normalized, (n * c, 1)\n",
    "\n",
    "    candidate_roads numpy array float of lat,long closest point to road from obs (N&K)\n",
    "    dist_prev float, meters\n",
    "    time_delta_prev float, sec\n",
    "\n",
    "    Zeroize very small probability candidates, but don't change shape of particles.\n",
    "    Then, if some particles are very unlikely, we cull them and resample from more likely particles. \n",
    "    \"\"\"\n",
    "    # Crossproduct for each particle, all roads\n",
    "    state_last2 = np.concatenate([np.concatenate((particles, \n",
    "                                                  np.tile(c, (n, 1))), \n",
    "                                                 axis=1) \n",
    "                                  for c in candidate_roads], axis=0)  # (n * c, 4)\n",
    "    obs_last2 = np.array(obs_history[-2:]).reshape((1, 4))\n",
    "    probs = transition_model.prob_transition(state_last2, obs_last2, \n",
    "                                             dist_prev, time_delta_prev)  # (n * c, 1)\n",
    "    # Effectively, candidate roads will be possible from different original particle hypotheses\n",
    "    # Zeroise small probs, without changing array shape\n",
    "    probs[probs <= prob_floor] = 0.0\n",
    "    # Re-normalize\n",
    "    probs = probs / np.sum(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat                  37.751360\n",
       "long               -122.395270\n",
       "lat_prev             37.751340\n",
       "long_prev          -122.394880\n",
       "dist_from_prev_m     34.360479\n",
       "time_delta_sec      -28.000000\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_coords = obs[['lat', 'long']] # obs[:2]\n",
    "dist_prev, time_delta_prev = obs[['dist_from_prev_m', 'time_delta_sec']] # obs[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.0"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_delta_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_history.append(obs_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[lat      37.75134\n",
       " long   -122.39488\n",
       " Name: 0, dtype: float64,\n",
       " lat      37.75136\n",
       " long   -122.39527\n",
       " Name: 1, dtype: float64]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidate_roads = get_candidate_roads(obs_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidate_roads = candidate_roads[:, [1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  37.7495152 , -122.3947256 ],\n",
       "       [  37.7495809 , -122.3938566 ],\n",
       "       [  37.7493582 , -122.3947463 ],\n",
       "       [  37.7495152 , -122.3947256 ],\n",
       "       [  37.7497464 , -122.3964185 ],\n",
       "       [  37.7497545 , -122.3962862 ],\n",
       "       [  37.7497851 , -122.3957776 ],\n",
       "       [  37.74982094, -122.39517989],\n",
       "       [  37.7498409 , -122.3948389 ],\n",
       "       [  37.749877  , -122.3942223 ],\n",
       "       [  37.7499376 , -122.3965082 ],\n",
       "       [  37.749877  , -122.3942223 ],\n",
       "       [  37.7500518 , -122.3946562 ],\n",
       "       [  37.7501218 , -122.3965221 ],\n",
       "       [  37.7500518 , -122.3946562 ],\n",
       "       [  37.7501218 , -122.3965221 ],\n",
       "       [  37.7504893 , -122.3966962 ],\n",
       "       [  37.750809  , -122.395448  ],\n",
       "       [  37.7505055 , -122.3963543 ],\n",
       "       [  37.7510282 , -122.3947529 ],\n",
       "       [  37.7505509 , -122.3947056 ],\n",
       "       [  37.7507095 , -122.3966147 ],\n",
       "       [  37.7507267 , -122.394723  ],\n",
       "       [  37.7512915 , -122.3952268 ],\n",
       "       [  37.7508102 , -122.3935227 ],\n",
       "       [  37.7509976 , -122.3963989 ],\n",
       "       [  37.7508768 , -122.3962566 ],\n",
       "       [  37.75092   , -122.3956145 ],\n",
       "       [  37.75126053, -122.39642241],\n",
       "       [  37.7511897 , -122.3964163 ],\n",
       "       [  37.7514762 , -122.396441  ],\n",
       "       [  37.75146304, -122.39363914],\n",
       "       [  37.75138233, -122.39501493],\n",
       "       [  37.75139659, -122.39486202],\n",
       "       [  37.75141136, -122.39469148],\n",
       "       [  37.75142206, -122.39451986],\n",
       "       [  37.75136889, -122.39520357],\n",
       "       [  37.75144751, -122.39434619],\n",
       "       [  37.75148118, -122.39400795],\n",
       "       [  37.75146467, -122.39417731],\n",
       "       [  37.75151271, -122.39383611],\n",
       "       [  37.752311  , -122.396513  ],\n",
       "       [  37.7522902 , -122.3968573 ],\n",
       "       [  37.75238646, -122.39533587],\n",
       "       [  37.752432  , -122.3946262 ],\n",
       "       [  37.7523714 , -122.3968168 ],\n",
       "       [  37.752656  , -122.3964826 ],\n",
       "       [  37.75253   , -122.3946416 ],\n",
       "       [  37.752485  , -122.393619  ],\n",
       "       [  37.752311  , -122.396513  ],\n",
       "       [  37.752432  , -122.3946262 ],\n",
       "       [  37.7529547 , -122.3946691 ]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_probs = apply_transition_model(candidate_roads, \n",
    "                                      dist_prev, \n",
    "                                      time_delta_prev) # (n * c, 1) first n rows for first candidate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregate probs by candidate roads\n",
    "trans_probs_split = np.array(np.split(trans_probs, n, axis=0))  # (n, c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 52, 1)"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 1)"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(trans_probs_split), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_probs_agg = np.sum(trans_probs_split, axis=0)  # (c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.02225955]),\n",
       " array([0.02289344]),\n",
       " array([0.02319293]),\n",
       " array([0.02337173]),\n",
       " array([0.02342305]),\n",
       " array([0.02631744]),\n",
       " array([0.02634805]),\n",
       " array([0.02638661]),\n",
       " array([0.02645195]),\n",
       " array([0.02645271])]"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(trans_probs_agg)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 1)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01747876, 0.02289344, 0.02645195, 0.01790464, 0.0125603 ,\n",
       "       0.019717  , 0.01570183, 0.02096689, 0.0221151 , 0.02645271,\n",
       "       0.01441209, 0.01573466, 0.01747109, 0.00978778, 0.02225955,\n",
       "       0.01915154, 0.01905346, 0.01559727, 0.02082604, 0.02093277,\n",
       "       0.02209549, 0.02634805, 0.02342305, 0.01780147, 0.01908997,\n",
       "       0.02160459, 0.0144079 , 0.0221298 , 0.02319293, 0.01978114,\n",
       "       0.01608819, 0.0161879 , 0.01449118, 0.0153872 , 0.02631744,\n",
       "       0.01902138, 0.01444908, 0.02082876, 0.02337173, 0.02208184,\n",
       "       0.01918932, 0.02638661, 0.01915659, 0.0179054 , 0.0187635 ,\n",
       "       0.01992638, 0.01570045, 0.01918937, 0.01915583, 0.01559808,\n",
       "       0.01905266, 0.01440786])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_agg.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample new states\n",
    "sampled_states_idx = np.random.choice(range(len(candidate_roads)), \n",
    "                                      n, \n",
    "                                      list(trans_probs_agg.flatten()))  # (n, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_states_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_states = candidate_roads[sampled_states_idx]  # (n, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HERE 4/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampled_states = candidate_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 219 µs, sys: 147 µs, total: 366 µs\n",
      "Wall time: 389 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sensor_probs = estimate_weights_per_particle(sampled_states, obs_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24459976, 0.75539978])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_probs[sensor_probs > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joint_prob = sensor_probs\n",
    "joint_prob = np.multiply(\n",
    "        np.multiply(sensor_probs, np.max(trans_probs_split, axis=1)[sampled_states_idx]),  # (n, 1)\n",
    "        weights\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = joint_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample new particles\n",
    "new_particles_idx = np.random.choice(a=range(sampled_states.shape[0]), \n",
    "                                     size=n, \n",
    "                                     p=sensor_probs.flatten())  # (1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_particles = sampled_states[new_particles_idx]  # (n, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "particles = new_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimate current particle filter fit quality of hypotheses to data; should research good metrics more.\n",
    "    # Came up with this on my own. \n",
    "fit_quality = [np.max(weights), np.mean(weights), np.median(weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5048466451708705, 0.01754385964912281, 4.958278019219135e-275]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def update_dist(obs, num_iter, max_dist2=None):\n",
    "\"\"\"\n",
    "Re-sample particles from transition model given re-estimated likelihood of existing particles.\n",
    "Weighted sample with replacement. \n",
    "\n",
    "numpy array of floats [lat, long, dist_from_prev_m, time_delta_sec]\n",
    "\n",
    "Note: if find no solutions, need to remove points in a signal break until HMM 'heals.'\n",
    "If break > 180 sec, separate into two trips.\n",
    "\"\"\"\n",
    "# Update stored obs\n",
    "obs_coords = obs[['lat', 'long']] # obs[:2]\n",
    "dist_prev, time_delta_prev = obs[['dist_from_prev_m', 'time_delta_sec']] # obs[2:]\n",
    "obs_history.append(obs_coords)\n",
    "# Get candidate roads given obs\n",
    "candidate_roads = get_candidate_roads(obs_coords) # ignore distance\n",
    "candidate_roads = candidate_roads[:, [1,0]]\n",
    "# What if candidate_roads is empty?? Then, seems logical to return No Match, and abort. \n",
    "if candidate_roads is None:\n",
    "    return 'Aborted'\n",
    "# Get transition probs; for first observation, ignore this part of the algorithm \n",
    "# and treat sensor probs as prior probabilities (N&K.\n",
    "if num_iter > 1:\n",
    "    trans_probs = apply_transition_model(candidate_roads, \n",
    "                                              dist_prev, \n",
    "                                              time_delta_prev) # (n * c, 1) first n rows for first candidate, etc.\n",
    "    # Aggregate probs by candidate roads\n",
    "    trans_probs_split = np.array(np.split(trans_probs, n, axis=0))  # (n, c, 1)\n",
    "    trans_probs_agg = np.sum(trans_probs_split, axis=0)  # (c, 1)\n",
    "    # Sample new states\n",
    "    # Sample new states\n",
    "    sampled_states_idx = np.random.choice(range(len(candidate_roads)), \n",
    "                                      n, \n",
    "                                      list(trans_probs_agg.flatten()))  # (n, )\n",
    "    sampled_states = candidate_roads[sampled_states_idx]  # (n, 2)\n",
    "else:\n",
    "    sampled_states = candidate_roads\n",
    "\n",
    "# Get sensor probs\n",
    "sensor_probs = estimate_weights_per_particle(sampled_states, obs_coords)  # (n, 1)\n",
    "\n",
    "# Joint prob, for viterbi backtracking. \n",
    "if num_iter > 1:\n",
    "    joint_prob = np.multiply(\n",
    "        np.multiply(sensor_probs, np.max(trans_probs_split, axis=1)[sampled_states_idx]),  # (n, 1)\n",
    "        weights\n",
    "        )\n",
    "else:\n",
    "    joint_prob = sensor_probs\n",
    "weights = joint_prob  # Effectively setting prior probabilites in iter 1\n",
    "\n",
    "# Sample new particles\n",
    "new_particles_idx = np.random.choice(a=range(sampled_states.shape[0]), \n",
    "                                     size=n, \n",
    "                                     p=sensor_probs.flatten())  # (1, n)\n",
    "new_particles = sampled_states[new_particles_idx]  # (n, 2)\n",
    "\n",
    "# Best prior state/particle for a given candidate state, for viterbi backtracking. \n",
    "if num_iter > 1:\n",
    "    best_prior_state_idx = np.argmax(trans_probs_split, axis=1)[sampled_states_idx[new_particles_idx]]  # (1, n)\n",
    "    best_prior_state = particles[best_prior_state_idx]  # (n, 2)\n",
    "    viterbi_trellis.append(best_prior_state)\n",
    "    viterbi_trellis_idx.append(best_prior_state_idx)\n",
    "\n",
    "particles = new_particles\n",
    "\n",
    "# Estimate current particle filter fit quality of hypotheses to data; should research good metrics more.\n",
    "    # Came up with this on my own. \n",
    "fit_quality = [np.max(weights), np.mean(weights), np.median(weights)]\n",
    "\n",
    "return fit_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.copy()\n",
    "num_iter = 0\n",
    "converged = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>lat_prev</th>\n",
       "      <th>long_prev</th>\n",
       "      <th>dist_from_prev_m</th>\n",
       "      <th>time_delta_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "      <td>34.360479</td>\n",
       "      <td>-28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "      <td>91.527346</td>\n",
       "      <td>-119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "      <td>165.996049</td>\n",
       "      <td>-51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.75015</td>\n",
       "      <td>-122.39256</td>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "      <td>107.168918</td>\n",
       "      <td>-252.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       long  lat_prev  long_prev  dist_from_prev_m  time_delta_sec\n",
       "0  37.75134 -122.39488       NaN        NaN               NaN             NaN\n",
       "1  37.75136 -122.39527  37.75134 -122.39488         34.360479           -28.0\n",
       "2  37.75199 -122.39460  37.75136 -122.39527         91.527346          -119.0\n",
       "3  37.75080 -122.39346  37.75199 -122.39460        165.996049           -51.0\n",
       "4  37.75015 -122.39256  37.75080 -122.39346        107.168918          -252.0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obs = data.iloc[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat                  37.751360\n",
       "long               -122.395270\n",
       "lat_prev             37.751340\n",
       "long_prev          -122.394880\n",
       "dist_from_prev_m     34.360479\n",
       "time_delta_sec      -28.000000\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def fit(data#, max_iter=100, conv_tol=0.001\n",
    "#            ):\n",
    "\"\"\"\n",
    "Iterate over rows in data, training particle filter. \n",
    "Rows assumed to be sequentially ordered. \n",
    "\n",
    "data, numpy array (num obs, data dim)\n",
    "data cols include lat, long, dist_from_prev_m, time_delta_sec (all floats)\n",
    "\n",
    "Note, calling fit multiple times would result in nonsense results, so this is prevented. \n",
    "\"\"\"\n",
    "if fitted:\n",
    "    print(\"Already fit - Cannot fit multiple times.\")\n",
    "    return None\n",
    "data = data.copy()\n",
    "num_iter = 0\n",
    "converged = False  # Is this relevant for particle filters?\n",
    "#         while num_iter < max_iter and not converged:\n",
    "#             num_iter += 1\n",
    "for obs in data:\n",
    "    num_iter += 1\n",
    "    fit_quality = update_dist(obs, num_iter)\n",
    "    if fit_quality == \"Aborted\":\n",
    "        return \"Aborted\"\n",
    "    # Shouldn't I use DP/Viterbi for this?? \n",
    "    # Nearest neighbor filter, hungarian algorithm?? p. 601\n",
    "    # Best at any given point in time will suffer in the beginning before particle dist has converged, ie\n",
    "    # during the burn-in period.\n",
    "    # So, seems can still use DP, but proceed backward in time. Wait, that is Viterbi :) \n",
    "        # Happily I have a finite state space. \n",
    "    print(\"On iteration %d, fit quality of MAX %3.2f, MEAN %3.2f, MEDIAN %3.2f\" % \n",
    "          (num_iter, fit_quality[0], fit_quality[1], fit_quality[2]))\n",
    "print(\"Done.\")\n",
    "fitted = True\n",
    "return fit_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(y_vector_arr):\n",
    "    \"\"\"\n",
    "    Returns list of backtracked states (float lat long),\n",
    "    the imputed GPS trace that has been 'snapped' to roads.\n",
    "    Use Viterbi to get optimal path via DP - \n",
    "    happily I have a finite state space due to my constraint \n",
    "    to only maintain n particles. \n",
    "\n",
    "    Note1, Viterbi relies on the Markov property, which\n",
    "    can apply here. \n",
    "    Note2, if particle filter was aborted due to no matching candidate roads,\n",
    "    Viterbi will still be able to backtrack the trace up until that point. \n",
    "    \"\"\"\n",
    "    # Start with the last observation to the viterbi trellis\n",
    "    best_last_state_idx = np.argmax(weights)\n",
    "    best_last_state = particles[best_last_state_idx]\n",
    "    backtracked_states = [best_last_state]\n",
    "    # Backtrack through the viterbi trellis (#obs, n, 2) actual lat/long states\n",
    "    for j in range(len(viterbi_trellis) - 1, -1, -1):\n",
    "        best_last_state_idx = viterbi_trellis_idx[j][best_last_state_idx]\n",
    "        best_last_state = viterbi_trellis[j][best_last_state_idx]\n",
    "        backtracked_states.append(best_last_state)\n",
    "    # Put in chronological order\n",
    "    backtracked_states = backtracked_states[::-1]\n",
    "    return backtracked_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class particle_filter:\n",
    "    \"\"\"\n",
    "    Fit a particle filter to data given a transition and sensor model. \n",
    "    \n",
    "    n, integer, number of particles to maintain at each step in time.\n",
    "    max_road_dist_m, float, max road distance above which candidate roads aren't considered (meters). \n",
    "    prob_floor, float between 0 and 1, min probability below which candidate roads aren't considered (not in N&K).\n",
    "    Sensor model, , is some model object we will use to predict the likelihood.\n",
    "    Transition model, , is some model object we will use to predict next state.\n",
    "    ## max_iter, integer, the maximum iterations used in fitting the particle filter.\n",
    "    ## conv_tol, the convergence tolerance that will trigger early termination of fitting the particle filter. \n",
    "    \"\"\"\n",
    "    def __init__(self, road_strtree, n=50, max_road_dist_m=200, prob_floor=0.0, \n",
    "                 sensor_model=gps_sensor_model(), transition_model=gps_transition_model(), \n",
    "                 viterbi_trellis=[], viterbi_trellis_idx=[]):\n",
    "        self.road_strtree = road_strtree\n",
    "        self.n = n\n",
    "        self.max_road_dist_m = max_road_dist_m\n",
    "        self.prob_floor = min(max(prob_floor, 0), 1)\n",
    "        self.particles = np.array([])\n",
    "        self.weights = np.ones((1, self.n))\n",
    "        \n",
    "        self.sensor_model = sensor_model\n",
    "        self.transition_model = transition_model\n",
    "        self.viterbi_trellis = viterbi_trellis\n",
    "        self.viterbi_trellis_idx = viterbi_trellis_idx\n",
    "        \n",
    "        self.fitted = False\n",
    "        \n",
    "        self.obs_history = []  # Record history of observations (lat, long)\n",
    "        \n",
    "    def get_candidate_roads(self, obs):\n",
    "        \"\"\"\n",
    "        obs list of floats [lat, long]\n",
    "        max_dist maximum allowable closest distance for prospective matched roads; those whose \n",
    "        closest point is >= max_dist meters away are ignored (200 in N&K).\n",
    "        Returns numpy array float of lat,long closest point to road from obs (N&K).\n",
    "\n",
    "        Leverages shapely and shapefiles for cabspotting data.\n",
    "        Relies on great_circle_dist() helper function. \n",
    "        \"\"\"\n",
    "        # Fiona uses (long, lat) format\n",
    "        obs = Point(obs[::-1])\n",
    "        # Want to consider only matches within max_dist meters\n",
    "        # Need to transform decimal degrees into meters\n",
    "        # Should I deviate from the default value of buffer for a Point??\n",
    "        # According to 200 * 360 / (2 * np.pi * 6371000), a distance of 200m corresponds to 0.0017986432118374611 decimal degrees\n",
    "        # obs.buffer(1.0)\n",
    "        matching_roads = self.road_strtree.query(obs.buffer(0.002))\n",
    "        if len(matching_roads) == 0:\n",
    "            matching_roads = self.road_strtree.query(obs.buffer(1.0))  # This should be a big buffer\n",
    "        # If matching roads is still empty, return None\n",
    "        if len(matching_roads) == 0:\n",
    "            print(\"No matching roads found! Aborting particle filter.\")\n",
    "            return None\n",
    "        # Ordered as (obs, road); just take road's coords\n",
    "        # Take only the closest point on the candidate road (1st index)\n",
    "        closest_pts = np.array([[list(p.coords)[0] for p in nearest_points(obs, r)][1] for r in matching_roads])\n",
    "        # Calculate distance between obs and candidate_roads, to filter them\n",
    "        candidate_dist_coords = \\\n",
    "        np.concatenate([closest_pts, np.tile(np.array(obs), (closest_pts.shape[0], 1))], axis=1)\n",
    "        candidate_dists = great_circle_dist(candidate_dist_coords)\n",
    "        # Filtered result of closest point on matching candidate roads\n",
    "        # no. matches 2 (default buffer); 52 (buffer of 0.002); 74 (buffer of 1.0)\n",
    "        candidate_roads = closest_pts[candidate_dists <= self.max_road_dist_m]\n",
    "        return candidate_roads\n",
    "    \n",
    "    def estimate_weights_per_particle(self, sampled_states, obs_coords):\n",
    "        \"\"\"\n",
    "        Estimate likelihood of particle given evidence, P(evidence|particle).\n",
    "        Uses sensor model.\n",
    "        \n",
    "        obs_coords is single observation at time t, pandas series. \n",
    "        \"\"\"\n",
    "        probs = sensor_model.estimate_likelihood(sampled_states, obs_coords.values.reshape(1, 2))\n",
    "        return probs\n",
    "        \n",
    "    def apply_transition_model(self, candidate_roads, dist_prev, time_delta_prev):\n",
    "        \"\"\"\n",
    "        returns probabilities of transition, given current particles\n",
    "        numpy array of floats between 0 and 1, normalized, (n * c, 1)\n",
    "        \n",
    "        candidate_roads numpy array float of lat,long closest point to road from obs (N&K)\n",
    "        dist_prev float, meters\n",
    "        time_delta_prev float, sec\n",
    "        \n",
    "        Zeroize very small probability candidates, but don't change shape of particles.\n",
    "        Then, if some particles are very unlikely, we cull them and resample from more likely particles. \n",
    "        \"\"\"\n",
    "        # Crossproduct for each particle, all roads\n",
    "        state_last2 = np.concatenate([np.concatenate((self.particles, \n",
    "                                                      np.tile(c, (self.n, 1))), \n",
    "                                                     axis=1) \n",
    "                                      for c in candidate_roads], axis=0)  # (n * c, 4)\n",
    "        obs_last2 = np.array(self.obs_history[-2:]).reshape((1, 4))\n",
    "        probs = self.transition_model.prob_transition(state_last2, obs_last2, \n",
    "                                                      dist_prev, time_delta_prev)  # (n * c, 1)\n",
    "        # Effectively, candidate roads will be possible from different original particle hypotheses\n",
    "        # Zeroise small probs, without changing array shape\n",
    "        probs[probs <= self.prob_floor] = 0.0\n",
    "        # Re-normalize\n",
    "        probs = probs / np.sum(probs)\n",
    "        return probs\n",
    "        \n",
    "    def update_dist(self, obs, num_iter, max_dist2=None):\n",
    "        \"\"\"\n",
    "        Re-sample particles from transition model given re-estimated likelihood of existing particles.\n",
    "        Weighted sample with replacement. \n",
    "        \n",
    "        numpy array of floats [lat, long, dist_from_prev_m, time_delta_sec]\n",
    "\n",
    "        Note: if find no solutions, need to remove points in a signal break until HMM 'heals.'\n",
    "        If break > 180 sec, separate into two trips.\n",
    "        \"\"\"\n",
    "        # Update stored obs\n",
    "        obs_coords = obs[['lat', 'long']] # obs[:2]\n",
    "        dist_prev, time_delta_prev = obs[['dist_from_prev_m', 'time_delta_sec']] # obs[2:]\n",
    "        self.obs_history.append(obs_coords)\n",
    "        # Get candidate roads given obs\n",
    "        candidate_roads = self.get_candidate_roads(obs_coords) # ignore distance\n",
    "        # Reverse order to be lat, long\n",
    "        candidate_roads = candidate_roads[:, [1,0]]\n",
    "        # What if candidate_roads is empty?? Then, seems logical to return No Match, and abort. \n",
    "        if candidate_roads is None:\n",
    "            return 'Aborted'\n",
    "        # Get transition probs; for first observation, ignore this part of the algorithm \n",
    "        # and treat sensor probs as prior probabilities (N&K.\n",
    "        if num_iter > 1:\n",
    "            trans_probs = self.apply_transition_model(candidate_roads, \n",
    "                                                      dist_prev, \n",
    "                                                      time_delta_prev) # (n * c, 1) first n rows for first candidate, etc.\n",
    "            # Aggregate probs by candidate roads\n",
    "            trans_probs_split = np.array(np.split(trans_probs, n, axis=0))  # (n, c, 1)\n",
    "            trans_probs_agg = np.sum(trans_probs_split, axis=0)  # (c, 1)\n",
    "            # Sample new states\n",
    "            # Sample new states\n",
    "            sampled_states_idx = np.random.choice(range(len(candidate_roads)), \n",
    "                                                  n, \n",
    "                                                  list(trans_probs_agg.flatten()))  # (n, )\n",
    "            sampled_states = candidate_roads[sampled_states_idx]  # (n, 2)\n",
    "        else:\n",
    "            sampled_states = candidate_roads\n",
    "        \n",
    "        # Get sensor probs\n",
    "        sensor_probs = self.estimate_weights_per_particle(sampled_states, obs_coords)  # (n, 1)\n",
    "        \n",
    "        # Joint prob, for viterbi backtracking. \n",
    "        if num_iter > 1:\n",
    "            joint_prob = np.multiply(\n",
    "                np.multiply(sensor_probs, np.max(trans_probs_split, axis=1)[sampled_states_idx]),  # (n, 1)\n",
    "                self.weights\n",
    "                )\n",
    "        else:\n",
    "            joint_prob = sensor_probs\n",
    "        self.weights = joint_prob  # Effectively setting prior probabilites in iter 1\n",
    "        \n",
    "        # Sample new particles\n",
    "        new_particles_idx = np.random.choice(a=range(sampled_states.shape[0]), \n",
    "                                             size=n, \n",
    "                                             p=sensor_probs.flatten())  # (1, n)\n",
    "        new_particles = sampled_states[new_particles_idx]  # (n, 2)\n",
    "        \n",
    "        # Best prior state/particle for a given candidate state, for viterbi backtracking. \n",
    "        if num_iter > 1:\n",
    "            best_prior_state_idx = np.argmax(trans_probs_split, axis=1)[sampled_states_idx[new_particles_idx]]  # (1, n)\n",
    "            best_prior_state = self.particles[best_prior_state_idx]  # (n, 2)\n",
    "            self.viterbi_trellis.append(best_prior_state)\n",
    "            self.viterbi_trellis_idx.append(best_prior_state_idx)\n",
    "        \n",
    "        self.particles = new_particles\n",
    "        \n",
    "        # Estimate current particle filter fit quality of hypotheses to data; should research good metrics more.\n",
    "            # Came up with this on my own. \n",
    "        fit_quality = [np.max(self.weights), np.mean(self.weights), np.median(self.weights)]\n",
    "        \n",
    "        return fit_quality\n",
    "        \n",
    "    def fit(self, data#, max_iter=100, conv_tol=0.001\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Iterate over rows in data, training particle filter. \n",
    "        Rows assumed to be sequentially ordered. \n",
    "        \n",
    "        data, numpy array (num obs, data dim)\n",
    "        data cols include lat, long, dist_from_prev_m, time_delta_sec (all floats)\n",
    "        \n",
    "        Note, calling fit multiple times would result in nonsense results, so this is prevented. \n",
    "        \"\"\"\n",
    "        if self.fitted:\n",
    "            print(\"Already fit - Cannot fit multiple times.\")\n",
    "            return None\n",
    "        data = data.copy()\n",
    "        num_iter = 0\n",
    "        converged = False  # Is this relevant for particle filters?\n",
    "#         while num_iter < max_iter and not converged:\n",
    "#             num_iter += 1\n",
    "        for obs in data:\n",
    "            num_iter += 1\n",
    "            fit_quality = self.update_dist(obs, num_iter)\n",
    "            if fit_quality == \"Aborted\":\n",
    "                return \"Aborted\"\n",
    "            # Shouldn't I use DP/Viterbi for this?? \n",
    "            # Nearest neighbor filter, hungarian algorithm?? p. 601\n",
    "            # Best at any given point in time will suffer in the beginning before particle dist has converged, ie\n",
    "            # during the burn-in period.\n",
    "            # So, seems can still use DP, but proceed backward in time. Wait, that is Viterbi :) \n",
    "                # Happily I have a finite state space. \n",
    "            print(\"On iteration %d, fit quality of MAX %3.2f, MEAN %3.2f, MEDIAN %3.2f\" % \n",
    "                  (num_iter, fit_quality[0], fit_quality[1], fit_quality[2]))\n",
    "        print(\"Done.\")\n",
    "        self.fitted = True\n",
    "        return fit_quality\n",
    "    \n",
    "    def viterbi(self, y_vector_arr):\n",
    "        \"\"\"\n",
    "        Returns list of backtracked states (float lat long),\n",
    "        the imputed GPS trace that has been 'snapped' to roads.\n",
    "        Use Viterbi to get optimal path via DP - \n",
    "        happily I have a finite state space due to my constraint \n",
    "        to only maintain n particles. \n",
    "        \n",
    "        Note1, Viterbi relies on the Markov property, which\n",
    "        can apply here. \n",
    "        Note2, if particle filter was aborted due to no matching candidate roads,\n",
    "        Viterbi will still be able to backtrack the trace up until that point. \n",
    "        \"\"\"\n",
    "        # Start with the last observation to the viterbi trellis\n",
    "        best_last_state_idx = np.argmax(self.weights)\n",
    "        best_last_state = self.particles[best_last_state_idx]\n",
    "        backtracked_states = [best_last_state]\n",
    "        # Backtrack through the viterbi trellis (#obs, n, 2) actual lat/long states\n",
    "        for j in range(len(self.viterbi_trellis) - 1, -1, -1):\n",
    "            best_last_state_idx = self.viterbi_trellis_idx[j][best_last_state_idx]\n",
    "            best_last_state = self.viterbi_trellis[j][best_last_state_idx]\n",
    "            backtracked_states.append(best_last_state)\n",
    "        # Put in chronological order\n",
    "        backtracked_states = backtracked_states[::-1]\n",
    "        return backtracked_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## it seems i only have beijing data (10K) or sf taxi data (500)\n",
    "# Read in taxi data\n",
    "# Each data of different length; ideal use case for pyspark\n",
    "# Note, the OSM extract basemap data has POI info as well (https://download.bbbike.org)\n",
    "# also try uber h3 spatial index\n",
    "trace_dir = '/Volumes/LaCie/datasets/ms_taxi/taxi_log_2008_by_id/roads.shp'   # MS Taxi\n",
    "basemap_dir = '/Volumes/LaCie/datasets/Beijing-shp/shape/'\n",
    "\n",
    "trace_dir = '/Volumes/LaCie/datasets/cabspottingdata/'   # CRAWDAD cabspotting\n",
    "basemap_dir = '/Volumes/LaCie/datasets/SanFrancisco-shp/shape/roads.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 ms, sys: 4.34 ms, total: 6.61 ms\n",
      "Wall time: 56.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_files = [f for f in os.listdir(trace_dir) if re.match(r'new_.*\\.txt', f)]  # glob.glob(trace_dir + \"new_*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.85 s, sys: 1.96 s, total: 9.81 s\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 20.9s cabspottingdata; can try spark or dask? or multiprocessing, joblib\n",
    "trace_list = []\n",
    "for file_ in all_files:\n",
    "    file_df = pd.read_csv(os.path.join(trace_dir, file_), sep=\" \", index_col=None, header=None, \n",
    "                          names=['lat', 'long', 'occupancy', 'time'])\n",
    "    trace_list.append(file_df)\n",
    "\n",
    "# concatenate all dfs into one\n",
    "trace_df = pd.concat(trace_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace_df.loc[:, [\"time\"]] = pd.to_datetime(trace_df.time, origin=\"unix\", unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat                 float64\n",
       "long                float64\n",
       "occupancy             int64\n",
       "time         datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cabspotting:\n",
    "\n",
    "latitude and longitude are in decimal degrees, \n",
    "occupancy shows if a cab has a fare (1 = occupied, 0 = free) and \n",
    "time is in UNIX epoch format\n",
    "\"\"\"\n",
    "trace_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:58:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:57:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:55:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:54:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.75015</td>\n",
       "      <td>-122.39256</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:50:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       long  occupancy                time\n",
       "0  37.75134 -122.39488          0 2008-06-10 07:58:07\n",
       "1  37.75136 -122.39527          0 2008-06-10 07:57:39\n",
       "2  37.75199 -122.39460          0 2008-06-10 07:55:40\n",
       "3  37.75080 -122.39346          0 2008-06-10 07:54:49\n",
       "4  37.75015 -122.39256          0 2008-06-10 07:50:37"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in ...shapefiles? GeoJSON? Which format is best? For parallelization may be one thing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_traces(df, sigma=4.07):\n",
    "    \"\"\"\n",
    "    pandas df, trace_df above with float columns lat, long in degrees\n",
    "    sigma float, representing std dev of measurement error (see above in particle filter class)\n",
    "    \n",
    "    \"The justification for this step is that until we see a point that is at least 2𝜎𝑧 \n",
    "    away from its temporal predecessor, our confidence is low that the apparent movement\n",
    "    is due to actual vehicle movement and not noise.\" (N&K)\n",
    "    \n",
    "    -remove obs that are not 2*meas dist sigma from previous obs  (eliminate 39% of data in N&K)\n",
    "    -Letting sigma = 4.07 meters as in N&K\n",
    "    \"\"\"\n",
    "    MILES_PER_METER = 0.000621371\n",
    "    HOURS_PER_SECOND = 3600.0\n",
    "    data = df.copy()\n",
    "    data[[\"lat_prev\", \"long_prev\"]] = data[[\"lat\", \"long\"]].shift(1)\n",
    "    # Ignore warning about invalid value in arcsin (nan)\n",
    "    data[\"dist_from_prev_m\"] = great_circle_dist(data[[\"lat_prev\", \"long_prev\", \"lat\", \"long\"]].values)  # 1.93 sec\n",
    "    # Add speed so can later filter on unreasonably high speeds\n",
    "    data[\"time_delta_sec\"] = data.time.subtract(data.time.shift(1)) / np.timedelta64(1, 's')  # get seconds\n",
    "    data[\"speed_mph\"] = (data.dist_from_prev_m * MILES_PER_METER).divide(data.time_delta_sec * HOURS_PER_SECOND)\n",
    "    # Take cumsum of dist\n",
    "    dist_cum = data.dist_from_prev_m.cumsum()\n",
    "    # Select points closest to multiples of 2*sigma, in cumsum dist\n",
    "    dist_cum_idx = dist_cum // (2 * sigma)\n",
    "    filter_idx = np.subtract(dist_cum_idx, dist_cum_idx.shift(1)) == 0  # 12% of rows eliminated \n",
    "    data = data[~filter_idx]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/LaCie/anaconda/envs/map_matching_particle_filter/lib/python3.7/site-packages/ipykernel/__main__.py:24: RuntimeWarning: invalid value encountered in arcsin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.72 s, sys: 4.04 s, total: 9.76 s\n",
      "Wall time: 8.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 11.8 s\n",
    "# Ignore RuntimeWarning: invalid value encountered in arcsin\n",
    "trace_df = preprocess_traces(trace_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>time</th>\n",
       "      <th>lat_prev</th>\n",
       "      <th>long_prev</th>\n",
       "      <th>dist_from_prev_m</th>\n",
       "      <th>time_delta_sec</th>\n",
       "      <th>speed_mph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:58:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:57:39</td>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "      <td>34.360479</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-2.118116e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:55:40</td>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "      <td>91.527346</td>\n",
       "      <td>-119.0</td>\n",
       "      <td>-1.327555e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:54:49</td>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "      <td>165.996049</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>-5.617927e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.75015</td>\n",
       "      <td>-122.39256</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:50:37</td>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "      <td>107.168918</td>\n",
       "      <td>-252.0</td>\n",
       "      <td>-7.340350e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       long  occupancy                time  lat_prev  long_prev  \\\n",
       "0  37.75134 -122.39488          0 2008-06-10 07:58:07       NaN        NaN   \n",
       "1  37.75136 -122.39527          0 2008-06-10 07:57:39  37.75134 -122.39488   \n",
       "2  37.75199 -122.39460          0 2008-06-10 07:55:40  37.75136 -122.39527   \n",
       "3  37.75080 -122.39346          0 2008-06-10 07:54:49  37.75199 -122.39460   \n",
       "4  37.75015 -122.39256          0 2008-06-10 07:50:37  37.75080 -122.39346   \n",
       "\n",
       "   dist_from_prev_m  time_delta_sec     speed_mph  \n",
       "0               NaN             NaN           NaN  \n",
       "1         34.360479           -28.0 -2.118116e-07  \n",
       "2         91.527346          -119.0 -1.327555e-07  \n",
       "3        165.996049           -51.0 -5.617927e-07  \n",
       "4        107.168918          -252.0 -7.340350e-08  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "long   -122.395\n",
       "lat     37.7514\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_df.loc[1, ['long', 'lat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fiona uses (long, lat) format\n",
    "obs = Point(trace_df.loc[1, ['long', 'lat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-123.39527 36.75136 2.0 2.0\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,75.50272)\"><circle cx=\"-122.39527\" cy=\"37.75136\" r=\"0.06\" stroke=\"#555555\" stroke-width=\"0.02\" fill=\"#66cc99\" opacity=\"0.6\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.point.Point at 0x1a39dace90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 ms, sys: 11.7 ms, total: 15.8 ms\n",
      "Wall time: 278 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "roads_shp = fiona.open(basemap_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init': 'epsg:4326'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roads_shp.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## https://gis.stackexchange.com/questions/80881/what-is-unit-of-shapely-length-attribute\n",
    "# line1 = LineString([(15.799406, 40.636069), (15.810173,40.640246)])\n",
    "# print str(line1.length) + \" degrees\"\n",
    "# # 0.0115488362184 degrees\n",
    "\n",
    "# # Geometry transform function based on pyproj.transform\n",
    "# project_m = partial(\n",
    "#     pyproj.transform,\n",
    "#     pyproj.Proj(init='EPSG:4326'),\n",
    "#     pyproj.Proj(init='EPSG:32633'))\n",
    "\n",
    "# line2 = transform(project_m, line1)\n",
    "# print str(line2.length) + \" meters\"\n",
    "# # 1021.77585965 meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': OrderedDict([('osm_id', 'int:11'),\n",
       "              ('name', 'str:48'),\n",
       "              ('ref', 'str:16'),\n",
       "              ('type', 'str:16'),\n",
       "              ('oneway', 'int:1'),\n",
       "              ('bridge', 'int:1'),\n",
       "              ('maxspeed', 'int:3')]),\n",
       " 'geometry': 'LineString'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roads_shp.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LineString'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Types of geometries\n",
    "set([r['geometry']['type'] for r in list(roads_shp)])\n",
    "# sf_shp: {'LineString'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Feature',\n",
       " 'id': '0',\n",
       " 'properties': OrderedDict([('osm_id', 4311275),\n",
       "              ('name', 'Bayshore Freeway'),\n",
       "              ('ref', 'US 101'),\n",
       "              ('type', 'motorway'),\n",
       "              ('oneway', 1),\n",
       "              ('bridge', 0),\n",
       "              ('maxspeed', 65)]),\n",
       " 'geometry': {'type': 'LineString',\n",
       "  'coordinates': [(-122.4067318, 37.6552091),\n",
       "   (-122.4066705, 37.6554806),\n",
       "   (-122.4066, 37.6557348),\n",
       "   (-122.4065183, 37.6560026),\n",
       "   (-122.4064254, 37.6562575),\n",
       "   (-122.4063177, 37.6565034),\n",
       "   (-122.4061926, 37.6567579),\n",
       "   (-122.4055559, 37.6579996),\n",
       "   (-122.4053176, 37.6584642),\n",
       "   (-122.4051888, 37.658704),\n",
       "   (-122.405056, 37.6589448),\n",
       "   (-122.4049166, 37.659174),\n",
       "   (-122.4047675, 37.6593902),\n",
       "   (-122.4046031, 37.6596144),\n",
       "   (-122.4044278, 37.6598328),\n",
       "   (-122.404206, 37.660081),\n",
       "   (-122.4039932, 37.6603049),\n",
       "   (-122.4037717, 37.6605162),\n",
       "   (-122.4035097, 37.6607433),\n",
       "   (-122.4032666, 37.6609362),\n",
       "   (-122.4030265, 37.6611163),\n",
       "   (-122.402762, 37.661301),\n",
       "   (-122.4025091, 37.6614653),\n",
       "   (-122.3996913, 37.6632717),\n",
       "   (-122.399424, 37.6634432),\n",
       "   (-122.3991719, 37.6636088),\n",
       "   (-122.3989093, 37.663784),\n",
       "   (-122.398657, 37.6639545),\n",
       "   (-122.3984076, 37.6641304),\n",
       "   (-122.3981713, 37.6643095),\n",
       "   (-122.3979449, 37.664483),\n",
       "   (-122.3977008, 37.6646782),\n",
       "   (-122.3974726, 37.6648686),\n",
       "   (-122.3972573, 37.6650546),\n",
       "   (-122.3970163, 37.6652751),\n",
       "   (-122.396798, 37.6654781),\n",
       "   (-122.3965933, 37.6656734),\n",
       "   (-122.3963931, 37.6658756),\n",
       "   (-122.3961956, 37.6660806),\n",
       "   (-122.3956858, 37.6666107),\n",
       "   (-122.3948905, 37.6674396),\n",
       "   (-122.3927279, 37.6696896)]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roads_shp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 37.2 ms, total: 1.51 s\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 1.79 s\n",
    "# Just preserving geometries for my purpose\n",
    "road_geoms = [shape(shp['geometry']) for shp in roads_shp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 250 ms, sys: 11.3 ms, total: 261 ms\n",
      "Wall time: 267 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 282 ms\n",
    "# Create STR-tree\n",
    "road_strtree = STRtree(road_geoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = trace_df[['lat', 'long', 'lat_prev', 'long_prev', 'dist_from_prev_m', 'time_delta_sec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  IGNORE BELOW THIS POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/4102520/how-to-transform-a-distance-from-degrees-to-metres\n",
    "\"\"\"\n",
    "@winwaed\n",
    "The transformation between degrees and metres varies across the Earth's surface.\n",
    "Assuming a spherical Earth, degrees latitude = distance * 360 / (2*PI * 6400000)\n",
    "Note that longitude will vary according to the latitude:\n",
    "Degrees longitude = distance *360 * / (2*PI* cos(latitude) )\n",
    "\n",
    "The above is for the Earth's surface, and does not use the Mercator projection. \n",
    "If you wish to work with projected linear distance, then you will need to use the Mercator projection.\n",
    "\"\"\"\n",
    "# RADIUS_OF_EARTH_M = 6371000\n",
    "# long, lat = list(obs.coords)[0]\n",
    "# max_dist_deg = max_dist * 360 / (2 * np.pi * RADIUS_OF_EARTH_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:map_matching_particle_filter]",
   "language": "python",
   "name": "conda-env-map_matching_particle_filter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
