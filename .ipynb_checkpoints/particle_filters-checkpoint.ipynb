{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment specified in environment.yml (environment name map_matching_particle_filter)\n",
    "# conda env create --prefix ./envs -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd #'0.24.2'\n",
    "import numpy as np # '1.16.4'\n",
    "import os, sys\n",
    "import glob, re, time\n",
    "\n",
    "import shapely  #'1.7.0'\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.geometry import shape, Point, LineString, MultiLineString\n",
    "from shapely.ops import nearest_points\n",
    "import folium\n",
    "import fiona\n",
    "import pyproj  # need version 2.2.1?\n",
    "from functools import partial\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "from sklearn.metrics.pairwise import haversine_distances  # Assumes (lat, long) in radians; version 0.22.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/LaCie/anaconda/envs/map_matching_particle_filter/share/proj'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PROJ_LIB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# os.environ['PROJ_LIB'] = \\\n",
    "# \"/Volumes/LaCie/anaconda/envs/map_matching_particle_filter/lib/python3.7/site-packages/fiona/proj_data/\"\n",
    "# os.environ['PROJ_LIB'] = \"/Volumes/LaCie/anaconda/share/proj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/LaCie/Documents/repos/particle_filter'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applications of particle filters: \n",
    "1. Car positioning by map matching, as in http://www.diva-portal.org/smash/get/diva2:316556/FULLTEXT01.pdf\n",
    "    Essentially same approach is described in Davidson, Collin, and Takala (2011). Application of particle filters to map-matching algorithm\n",
    "    Idea also similar to Newson and Krumm (2009), though HMM is used there.\n",
    "    More recent resource: Murphy, Pao, Yuen (2019). Lyft; Map matching when the map is wrong: Efficient on/off road vehicle tracking and map learning\n",
    "\n",
    "Ideas\n",
    "Rao-Blackwellization (use Kalman filter for the linear part of the dynamics model)\n",
    "\n",
    "Initial Approach:\n",
    "Use Newson and Krumm, but modify it to use particle filters instead of HMM.\n",
    "\n",
    "So, for a given route, proceed sequentially over obs, maintaining dist of probable road segments\n",
    "\n",
    "Assumptions (N&K):\n",
    "-remove obs that are not 2*meas dist sigma from previous obs  (eliminate 39% of data in N&K)\n",
    "-ignore roads 200m from obs\n",
    "-zeroize very unlikely particles\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def great_circle_dist(df):\n",
    "    \"\"\"\n",
    "    USED BELOW IN PREPROCESSING\n",
    "    df numpy array [lat, long, lat_prev, long_prev] in degrees\n",
    "    \n",
    "    Returns great-circle distance between point (lat,long) columns and (lat_prev, long_prev) columns, \n",
    "    in meters.\n",
    "    \n",
    "    Modified from https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    df = df.copy()\n",
    "    df = np.deg2rad(df)\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = df[:, 3] - df[:, 1] # lon2 - lon1 \n",
    "    dlat = df[:, 2] - df[:, 0] # lat2 - lat1 \n",
    "    a = np.add(np.square(np.sin(dlat / 2)),\n",
    "               np.multiply(np.cos(df[:, 0]), \n",
    "                           np.multiply(np.cos(df[:, 2]), np.square(np.sin(dlon / 2)))\n",
    "                          )\n",
    "              )\n",
    "    # a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = np.arcsin(np.sqrt(a)) * 2\n",
    "    # c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return r * c * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gps_transition_model:\n",
    "    \"\"\"\n",
    "    Transition model to predict probability of next state given current state; simulate next state.\n",
    "    GPS trace map matching use case. \n",
    "    \n",
    "    N&K use as a proxy an exponential function of the difference in the great-circle distance \n",
    "    between previous observations and previous road points (similar route distance).\n",
    "    Road points use map/ground-truth data. \n",
    "    -Ignore roads 200m from obs\n",
    "    -If a calculated route would require the vehicle to exceed a speed of 50 m/s (112 miles per hour), zeroize\n",
    "    \n",
    "    !!! TODO: Get driving distance from route planner??? Results anticipated to significantly suffer otherwise. !!! \n",
    "    \n",
    "    Other ideas: using dead reckoning.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta=3.0\n",
    "#                  , prob_floor=0.05\n",
    "                 , normalize=True\n",
    "                ):\n",
    "        \"\"\"\n",
    "        beta, float, parametrizes transition probability function\n",
    "        prob_floor, float between 0 and 1, is probability below which road choices are given zero probability\n",
    "        \n",
    "        # beta = 3 in mapzen, https://www.mapzen.com/blog/data-driven-map-matching/\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "#         self.prob_floor = min(max(prob_floor, 0), 1)\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def estimate_beta(self, data):\n",
    "        \"\"\"\n",
    "        NOT USED\n",
    "        \n",
    "        Using rescaled median absolute deviation (MAD). \n",
    "        Ideally using ground truth data (none here).\n",
    "        \"\"\" \n",
    "        self.beta = 3.0  # None\n",
    "        \n",
    "    def prob_transition(self, state_last2, obs_last2, dist_obs_m_prev, time_delta_sec_prev, \n",
    "                        max_allowable_diff_m=2000.0, max_allowable_mph=112.0):\n",
    "        \"\"\"\n",
    "        state_last2, float numpy array, (n * c x 4 lat, long, lat_prev, long_prev), c candidate roads and n particles\n",
    "        obs_last2, float numpy array, (1 x 4 lat, long, lat_prev, long_prev) \n",
    "        dist_obs_m_prev float distance in meters between last two obs.\n",
    "        time_delta_sec_prev float time delta in sec between last two obs.\n",
    "        max_allowable_diff_m float, ignore low probability routes as in N&K; in N&K 2000 meters.\n",
    "        max_allowable_mph, float; ignore routes resulting in unreasonable speeds. 112mph in N&K.\n",
    "        Where present, lat/long in degrees.\n",
    "        \n",
    "        returns probs of transition, (n * c x 1)\n",
    "        \"\"\"\n",
    "        RADIUS_OF_EARTH_M = 6371000\n",
    "        MILES_PER_METER = 0.000621371\n",
    "        HOURS_PER_SECOND = 3600.0\n",
    "        state_last2 = np.deg2rad(state_last2)\n",
    "        obs_last2 = np.deg2rad(obs_last2)\n",
    "        # dist_obs = abs(haversine_distances(obs_last2[:, :2], obs_last2[0, 2:]))  # (1, 1)\n",
    "        # !! TODO - Supposed to be driving distance, not haversine. N&K use a route planner. Can I use shapely???\n",
    "            # Probably fine as generally GPS measurements are taken close together in time- \n",
    "            # Otherwise would have to snap to roads using Shapely and get the distance between the two points. \n",
    "            # How does using driving distance even compare to haversine for obs? Isn't it better to compare apples to apples?\n",
    "        dist_road = \\\n",
    "        np.abs(np.array([haversine_distances(state_last2[r, :2].reshape(1,2), state_last2[r, 2:].reshape(1,2))[0] \n",
    "                         for r in range(state_last2.shape[0])]))  # (n * c, 1)\n",
    "        dist_road = dist_road * RADIUS_OF_EARTH_M  # multiply by Earth radius to get result in meters\n",
    "        diff_dist = np.abs(np.subtract(dist_road, dist_obs_m_prev))  # (n * c, 1)\n",
    "        \n",
    "        # Ignore cases where diff_dist >= max_allowable_diff_m\n",
    "        diff_dist[diff_dist >= max_allowable_diff_m] = np.float(\"inf\")  # zero out these transition probs\n",
    "        # Ignore cases where implied speed of route is >= max_allowable_mph; calculate speed in candidate routes\n",
    "        implied_speed_road = np.divide(dist_road * MILES_PER_METER, \n",
    "                               time_delta_sec_prev * HOURS_PER_SECOND)  # (n * c, 1), meters\n",
    "        diff_dist[implied_speed_road >= max_allowable_mph] = np.float(\"inf\")  # zero out these transition probs\n",
    "        \n",
    "        probs = np.exp(-diff_dist / self.beta) * (1 / self.beta)\n",
    "        # Normalize result?\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gps_sensor_model:\n",
    "    \"\"\"\n",
    "    Sensor model to predict likelihood of an obs given a particle.\n",
    "    GPS trace map matching use case. \n",
    "    \n",
    "    I don't have ground truth, so can't use ML easily, but\n",
    "    N&K use as a proxy a normal distribution of great-circle distance between observation and road, with sigma \n",
    "    estimated from the data.\n",
    "    Road points use map data. \n",
    "    -Zerioze low probability particles (diff in route distance of 2000 m. or more). \n",
    "    \n",
    "    Other ideas: semi-supervised learning. \n",
    "    \n",
    "    Thoughts: need efficient representation for road network that returns connecting roads/nodes for a given node\n",
    "    Want to represent it as a graph/network. \n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=4.07, normalize=True):\n",
    "        self.sigma = sigma\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def estimate_sigma(self, data):\n",
    "        \"\"\"\n",
    "        NOT USED \n",
    "        \n",
    "        Using Gather and Schultze, median-based. \n",
    "        Ideally using ground truth data (none here).\n",
    "        \n",
    "        I guess I could manually match some trips, then use that??\n",
    "        \"\"\" \n",
    "        self.sigma = 4.07 # meters; I have no ground truth so using N&K's empirical estimate\n",
    "#         self.sigma = None # median absolute deviation (MAD) formula adjusted \n",
    "    \n",
    "    def estimate_likelihood(self, states, obs_coords):\n",
    "        \"\"\"\n",
    "        return probability of seeing that obs given potential road state hypotheses, (k x 1)\n",
    "        \n",
    "        states Particles numpy array representing road position hypotheses, (k x 2 lat long).\n",
    "        obs_coords (1 x 2 lat long)\n",
    "        lat long in degrees\n",
    "        \"\"\"\n",
    "        RADIUS_OF_EARTH_M = 6371000\n",
    "        dist_obs_roads = np.abs(haversine_distances(np.deg2rad(states), np.deg2rad(obs_coords)))  # (k x 1)\n",
    "        dist_obs_roads = dist_obs_roads * RADIUS_OF_EARTH_M  # multiply by Earth radius to get result in meters\n",
    "        \n",
    "        probs = \\\n",
    "        np.exp(np.power(dist_obs_roads / self.sigma, 2) * (-0.5)) * \\\n",
    "        (1 / (math.sqrt(2 * math.pi) * self.sigma))\n",
    "        \n",
    "        # Normalize result?\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEGIN DEBUGGING\n",
    "road_strtree; n=50; max_road_dist_m=200; prob_floor=0.0; \n",
    "sensor_model=gps_sensor_model(); transition_model=gps_transition_model(); \n",
    "viterbi_trellis=[]; viterbi_trellis_idx=[]\n",
    "\n",
    "prob_floor = min(max(prob_floor, 0), 1)\n",
    "particles = np.array([])\n",
    "weights = np.ones((1, n))\n",
    "\n",
    "fitted = False\n",
    "        \n",
    "obs_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_candidate_roads(obs):\n",
    "    \"\"\"\n",
    "    obs list of floats [lat, long]\n",
    "    max_dist maximum allowable closest distance for prospective matched roads; those whose \n",
    "    closest point is >= max_dist meters away are ignored (200 in N&K).\n",
    "    Returns numpy array float of lat,long closest point to road from obs (N&K).\n",
    "\n",
    "    Leverages shapely and shapefiles for cabspotting data.\n",
    "    Relies on great_circle_dist() helper function. \n",
    "    \"\"\"\n",
    "    # Fiona uses (long, lat) format\n",
    "    obs = Point(obs[::-1])\n",
    "    # Want to consider only matches within max_dist meters\n",
    "    # Need to transform decimal degrees into meters\n",
    "    # Should I deviate from the default value of buffer for a Point??\n",
    "    # According to 200 * 360 / (2 * np.pi * 6371000), a distance of 200m corresponds to 0.0017986432118374611 decimal degrees\n",
    "    # obs.buffer(1.0)\n",
    "    matching_roads = road_strtree.query(obs.buffer(0.002))\n",
    "    if len(matching_roads) == 0:\n",
    "        matching_roads = road_strtree.query(obs.buffer(1.0))  # This should be a big buffer\n",
    "    # If matching roads is still empty, return None\n",
    "    if len(matching_roads) == 0:\n",
    "        print(\"No matching roads found! Aborting particle filter.\")\n",
    "        return None\n",
    "    # Ordered as (obs, road); just take road's coords\n",
    "    # Take only the closest point on the candidate road (1st index)\n",
    "    closest_pts = np.array([[list(p.coords)[0] for p in nearest_points(obs, r)][1] for r in matching_roads])\n",
    "    # Calculate distance between obs and candidate_roads, to filter them\n",
    "    candidate_dist_coords = \\\n",
    "    np.concatenate([closest_pts, np.tile(np.array(obs), (closest_pts.shape[0], 1))], axis=1)\n",
    "    # Convert back to lat, long from long, lat\n",
    "    candidate_dist_coords = candidate_dist_coords[:, [1, 0, 3, 2]]\n",
    "    candidate_dists = great_circle_dist(candidate_dist_coords)\n",
    "    # Filtered result of closest point on matching candidate roads\n",
    "    # no. matches 2 (default buffer); 52 (buffer of 0.002); 74 (buffer of 1.0)\n",
    "        # Reverse order to be lat, long\n",
    "    candidate_roads = closest_pts[candidate_dists <= max_road_dist_m][:, [1,0]]\n",
    "    return candidate_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_weights_per_particle(sampled_states, obs_coords):\n",
    "    \"\"\"\n",
    "    Estimate likelihood of particle given evidence, P(evidence|particle).\n",
    "    Uses sensor model.\n",
    "\n",
    "    obs_coords is single observation at time t, pandas series. \n",
    "    \"\"\"\n",
    "    probs = sensor_model.estimate_likelihood(sampled_states, obs_coords.values.reshape(1, 2))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Crossproduct for each particle, all roads\n",
    "state_last2 = np.concatenate([np.concatenate((particles, \n",
    "                                              np.tile(c, (n, 1))), \n",
    "                                             axis=1) \n",
    "                              for c in candidate_roads], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 4)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_last2.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_last2 = np.array(obs_history[-2:]).reshape((1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  37.75134, -122.39488,   37.75136, -122.39527]])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_last2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_obs_m_prev=dist_prev; time_delta_sec_prev=time_delta_prev; max_allowable_diff_m=2000.0; max_allowable_mph=112.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.360479144304016"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_obs_m_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.0"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_delta_sec_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RADIUS_OF_EARTH_M = 6371000\n",
    "MILES_PER_METER = 0.000621371\n",
    "HOURS_PER_SECOND = 3600.0\n",
    "state_last2 = np.deg2rad(state_last2)\n",
    "obs_last2 = np.deg2rad(obs_last2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65888492, -2.13619363,  0.65885333, -2.13619095],\n",
       "       [ 0.65888522, -2.13619324,  0.65885333, -2.13619095],\n",
       "       [ 0.65888522, -2.13619324,  0.65885333, -2.13619095],\n",
       "       ...,\n",
       "       [ 0.65888492, -2.13619363,  0.65891336, -2.13618996],\n",
       "       [ 0.65888522, -2.13619324,  0.65891336, -2.13618996],\n",
       "       [ 0.65888522, -2.13619324,  0.65891336, -2.13618996]])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_last2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_road = \\\n",
    "np.abs(np.array([haversine_distances(state_last2[r, :2].reshape(1,2), state_last2[r, 2:].reshape(1,2))[0] \n",
    " for r in range(state_last2.shape[0])]))  # (n * c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_road = dist_road * RADIUS_OF_EARTH_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_dist = np.abs(np.subtract(dist_road, dist_obs_m_prev))  # (n * c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[167.33782252],\n",
       "       [169.1049927 ],\n",
       "       [169.1049927 ],\n",
       "       ...,\n",
       "       [147.78968822],\n",
       "       [145.7182921 ],\n",
       "       [145.7182921 ]])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 1)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46792129264360227"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(diff_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore cases where diff_dist >= max_allowable_diff_m\n",
    "diff_dist[diff_dist >= max_allowable_diff_m] = np.float(\"inf\")  # zero out these transition probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ignore cases where implied speed of route is >= max_allowable_mph; calculate speed in candidate routes\n",
    "implied_speed_road = np.divide(dist_road * MILES_PER_METER, \n",
    "                               time_delta_sec_prev * HOURS_PER_SECOND)  # (n * c, 1), meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_dist[implied_speed_road >= max_allowable_mph] = np.float(\"inf\")  # zero out these transition probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = np.exp(-diff_dist / beta) * (1 / beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = probs / np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_transition(self, state_last2, obs_last2, dist_obs_m_prev, time_delta_sec_prev, \n",
    "                        max_allowable_diff_m=2000.0, max_allowable_mph=112.0):\n",
    "        \"\"\"\n",
    "        state_last2, float numpy array, (n * c x 4 lat, long, lat_prev, long_prev), c candidate roads and n particles\n",
    "        obs_last2, float numpy array, (1 x 4 lat, long, lat_prev, long_prev) \n",
    "        dist_obs_m_prev float distance in meters between last two obs.\n",
    "        time_delta_sec_prev float time delta in sec between last two obs.\n",
    "        max_allowable_diff_m float, ignore low probability routes as in N&K; in N&K 2000 meters.\n",
    "        max_allowable_mph, float; ignore routes resulting in unreasonable speeds. 112mph in N&K.\n",
    "        Where present, lat/long in degrees.\n",
    "        \n",
    "        returns probs of transition, (n * c x 1)\n",
    "        \"\"\"\n",
    "        RADIUS_OF_EARTH_M = 6371000\n",
    "        MILES_PER_METER = 0.000621371\n",
    "        HOURS_PER_SECOND = 3600.0\n",
    "        state_last2 = np.deg2rad(state_last2)\n",
    "        obs_last2 = np.deg2rad(obs_last2)\n",
    "        # dist_obs = abs(haversine_distances(obs_last2[:, :2], obs_last2[0, 2:]))  # (1, 1)\n",
    "        # !! TODO - Supposed to be driving distance, not haversine. N&K use a route planner. Can I use shapely???\n",
    "            # Probably fine as generally GPS measurements are taken close together in time- \n",
    "            # Otherwise would have to snap to roads using Shapely and get the distance between the two points. \n",
    "            # How does using driving distance even compare to haversine for obs? Isn't it better to compare apples to apples?\n",
    "        dist_road = \\\n",
    "        np.abs(np.array([haversine_distances(state_last2[r, :2].reshape(1,2), state_last2[r, 2:].reshape(1,2))[0] \n",
    "                         for r in range(state_last2.shape[0])]))  # (n * c, 1)\n",
    "        dist_road = dist_road * RADIUS_OF_EARTH_M  # multiply by Earth radius to get result in meters\n",
    "        diff_dist = np.abs(np.subtract(dist_road, dist_obs_m_prev))  # (n * c, 1)\n",
    "        \n",
    "        # Ignore cases where diff_dist >= max_allowable_diff_m\n",
    "        diff_dist[diff_dist >= max_allowable_diff_m] = np.float(\"inf\")  # zero out these transition probs\n",
    "        # Ignore cases where implied speed of route is >= max_allowable_mph; calculate speed in candidate routes\n",
    "        implied_speed_road = np.divide(dist_road * MILES_PER_METER, \n",
    "                               time_delta_sec_prev * HOURS_PER_SECOND)  # (n * c, 1), meters\n",
    "        diff_dist[implied_speed_road >= max_allowable_mph] = np.float(\"inf\")  # zero out these transition probs\n",
    "        \n",
    "        probs = np.exp(-diff_dist / self.beta) * (1 / self.beta)\n",
    "        # Normalize result?\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = transition_model.prob_transition(state_last2, obs_last2, \n",
    "                                         dist_prev, time_delta_prev)  # (n * c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015])]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(probs)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(probs <= prob_floor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs[probs <= prob_floor] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = probs / np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015]),\n",
       " array([0.00898015])]"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(probs)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_last2 = np.concatenate([np.concatenate((particles, \n",
    "                                                  np.tile(c, (n, 1))), \n",
    "                                                 axis=1) \n",
    "                                  for c in candidate_roads], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  37.75134205, -122.39485713,   37.7497545 , -122.3962862 ],\n",
       "       [  37.75134205, -122.39485713,   37.7497545 , -122.3962862 ],\n",
       "       [  37.75132504, -122.39487933,   37.7497545 , -122.3962862 ],\n",
       "       ...,\n",
       "       [  37.75132504, -122.39487933,   37.7529547 , -122.3946691 ],\n",
       "       [  37.75134205, -122.39485713,   37.7529547 , -122.3946691 ],\n",
       "       [  37.75132504, -122.39487933,   37.7529547 , -122.3946691 ]])"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_last2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_last2 = np.array(obs_history[-2:]).reshape((1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  37.75134, -122.39488,   37.75136, -122.39527]])"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_last2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = transition_model.prob_transition(state_last2, obs_last2, \n",
    "                                             dist_prev, time_delta_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.17545221e-29],\n",
       "       [4.17545221e-29],\n",
       "       [1.01753694e-28],\n",
       "       ...,\n",
       "       [4.15709313e-24],\n",
       "       [8.29191906e-24],\n",
       "       [4.15709313e-24]])"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs[probs <= prob_floor] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_transition_model(candidate_roads, dist_prev, time_delta_prev):\n",
    "    \"\"\"\n",
    "    returns probabilities of transition, given current particles\n",
    "    numpy array of floats between 0 and 1, normalized, (n * c, 1)\n",
    "\n",
    "    candidate_roads numpy array float of lat,long closest point to road from obs (N&K)\n",
    "    dist_prev float, meters\n",
    "    time_delta_prev float, sec\n",
    "\n",
    "    Zeroize very small probability candidates, but don't change shape of particles.\n",
    "    Then, if some particles are very unlikely, we cull them and resample from more likely particles. \n",
    "    \"\"\"\n",
    "    # Crossproduct for each particle, all roads\n",
    "    state_last2 = np.concatenate([np.concatenate((particles, \n",
    "                                                  np.tile(c, (n, 1))), \n",
    "                                                 axis=1) \n",
    "                                  for c in candidate_roads], axis=0)  # (n * c, 4)\n",
    "    obs_last2 = np.array(obs_history[-2:]).reshape((1, 4))\n",
    "    probs = transition_model.prob_transition(state_last2, obs_last2, \n",
    "                                             dist_prev, time_delta_prev)  # (n * c, 1)\n",
    "    # Effectively, candidate roads will be possible from different original particle hypotheses\n",
    "    # Zeroise small probs, without changing array shape\n",
    "    probs[probs <= prob_floor] = 0.0\n",
    "    # Re-normalize\n",
    "    probs = probs / np.sum(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat                  37.751360\n",
       "long               -122.395270\n",
       "lat_prev             37.751340\n",
       "long_prev          -122.394880\n",
       "dist_from_prev_m     34.360479\n",
       "time_delta_sec      -28.000000\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_coords = obs[['lat', 'long']] # obs[:2]\n",
    "dist_prev, time_delta_prev = obs[['dist_from_prev_m', 'time_delta_sec']] # obs[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.0"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_delta_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_history.append(obs_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[lat      37.75134\n",
       " long   -122.39488\n",
       " Name: 0, dtype: float64,\n",
       " lat      37.75136\n",
       " long   -122.39527\n",
       " Name: 1, dtype: float64]"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidate_roads = get_candidate_roads(obs_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  37.7497545 , -122.3962862 ],\n",
       "       [  37.7497851 , -122.3957776 ],\n",
       "       [  37.74982094, -122.39517989],\n",
       "       [  37.7498409 , -122.3948389 ],\n",
       "       [  37.749877  , -122.3942223 ],\n",
       "       [  37.7499376 , -122.3965082 ],\n",
       "       [  37.749877  , -122.3942223 ],\n",
       "       [  37.7500518 , -122.3946562 ],\n",
       "       [  37.7501218 , -122.3965221 ],\n",
       "       [  37.7500518 , -122.3946562 ],\n",
       "       [  37.7501218 , -122.3965221 ],\n",
       "       [  37.7504893 , -122.3966962 ],\n",
       "       [  37.750809  , -122.395448  ],\n",
       "       [  37.7508977 , -122.3934267 ],\n",
       "       [  37.7505055 , -122.3963543 ],\n",
       "       [  37.7510282 , -122.3947529 ],\n",
       "       [  37.7505509 , -122.3947056 ],\n",
       "       [  37.7507095 , -122.3966147 ],\n",
       "       [  37.7506223 , -122.3935102 ],\n",
       "       [  37.7507267 , -122.394723  ],\n",
       "       [  37.7512915 , -122.3952268 ],\n",
       "       [  37.7508102 , -122.3935227 ],\n",
       "       [  37.7509976 , -122.3963989 ],\n",
       "       [  37.7508768 , -122.3962566 ],\n",
       "       [  37.75092   , -122.3956145 ],\n",
       "       [  37.75126053, -122.39642241],\n",
       "       [  37.7511897 , -122.3964163 ],\n",
       "       [  37.7514762 , -122.396441  ],\n",
       "       [  37.75146304, -122.39363914],\n",
       "       [  37.75138233, -122.39501493],\n",
       "       [  37.75139659, -122.39486202],\n",
       "       [  37.75141136, -122.39469148],\n",
       "       [  37.75142206, -122.39451986],\n",
       "       [  37.75136889, -122.39520357],\n",
       "       [  37.75144751, -122.39434619],\n",
       "       [  37.75148118, -122.39400795],\n",
       "       [  37.75146467, -122.39417731],\n",
       "       [  37.75151271, -122.39383611],\n",
       "       [  37.752311  , -122.396513  ],\n",
       "       [  37.7521861 , -122.393509  ],\n",
       "       [  37.7522902 , -122.3968573 ],\n",
       "       [  37.75238646, -122.39533587],\n",
       "       [  37.752432  , -122.3946262 ],\n",
       "       [  37.7523714 , -122.3968168 ],\n",
       "       [  37.752656  , -122.3964826 ],\n",
       "       [  37.75253   , -122.3946416 ],\n",
       "       [  37.752485  , -122.393619  ],\n",
       "       [  37.752311  , -122.396513  ],\n",
       "       [  37.752432  , -122.3946262 ],\n",
       "       [  37.7529547 , -122.3946691 ]])"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_probs = apply_transition_model(candidate_roads, \n",
    "                                      dist_prev, \n",
    "                                      time_delta_prev) # (n * c, 1) first n rows for first candidate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.17545221e-29],\n",
       "       [4.17545221e-29],\n",
       "       [1.01753694e-28],\n",
       "       ...,\n",
       "       [4.15709313e-24],\n",
       "       [8.29191906e-24],\n",
       "       [4.15709313e-24]])"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregate probs by candidate roads\n",
    "trans_probs_split = np.array(np.split(trans_probs, n, axis=0))  # (n, c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 1)"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(trans_probs_split), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_probs_agg = np.sum(trans_probs_split, axis=0)  # (c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.02126911]),\n",
       " array([0.02126911]),\n",
       " array([0.02126911]),\n",
       " array([0.02126911]),\n",
       " array([0.02126911]),\n",
       " array([0.02126911]),\n",
       " array([0.02126911]),\n",
       " array([0.02126911]),\n",
       " array([0.02126911]),\n",
       " array([0.02126911])]"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(trans_probs_agg)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01882851, 0.01882851, 0.02126911, 0.02126911, 0.01882851,\n",
       "       0.02126911, 0.01882851, 0.02126911, 0.01882851, 0.02126911,\n",
       "       0.02126911, 0.01882851, 0.01882851, 0.02126911, 0.02126911,\n",
       "       0.02126911, 0.01882851, 0.02126911, 0.02126911, 0.01882851,\n",
       "       0.01882851, 0.01882851, 0.01882851, 0.01882851, 0.01882851,\n",
       "       0.02126911, 0.02126911, 0.02126911, 0.02126911, 0.01882851,\n",
       "       0.02126911, 0.01882851, 0.01882851, 0.02126911, 0.01882851,\n",
       "       0.01882851, 0.01882851, 0.01882851, 0.01882851, 0.01882851,\n",
       "       0.02126911, 0.02126911, 0.02126911, 0.02126911, 0.01882851,\n",
       "       0.02126911, 0.01882851, 0.02126911, 0.01882851, 0.02126911])"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_agg.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample new states\n",
    "sampled_states_idx = np.random.choice(range(len(candidate_roads)), \n",
    "                                      n, \n",
    "                                      list(trans_probs_agg.flatten()))  # (n, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_states_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampled_states = candidate_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_states = candidate_roads[sampled_states_idx]  # (n, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampled_states = candidate_roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 µs, sys: 31 µs, total: 199 µs\n",
      "Wall time: 205 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sensor_probs = estimate_weights_per_particle(sampled_states, obs_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24459976, 0.75539978])"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_probs[sensor_probs > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 1)"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(trans_probs_split, axis=0)[sampled_states_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joint_prob = sensor_probs[new_particles_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joint_prob = sensor_probs\n",
    "joint_prob = np.multiply(\n",
    "        np.multiply(sensor_probs, np.max(trans_probs_split, axis=0)[sampled_states_idx]),  # (n, 1)\n",
    "        weights\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = joint_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=pd.DataFrame(np.array([[1,3],[1,2], [1,6], [2,5], [2,5],[2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  3\n",
       "1  1  2\n",
       "2  1  6\n",
       "3  2  5\n",
       "4  2  5\n",
       "5  2  3"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampled_states_idx=np.array([2, 1]); new_particles_idx=np.array([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2, 2])]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sampled_states_idx[new_particles_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 5, 5])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([(np.array([sampled_states_idx[new_particles_idx]]) + 3 * (i-1)) for i in range(1, 3)]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "2  1  6\n",
       "2  1  6\n",
       "5  2  3\n",
       "5  2  3"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[np.array([(np.array([sampled_states_idx[new_particles_idx]]) + 3 * (i-1)) for i in range(1, 3)]).flatten(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use trans_probs!!! (n * c, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_prior_state_idx = \\\n",
    "pd.DataFrame(test).loc[np.array([(np.array([sampled_states_idx[new_particles_idx]]) + 3 * (i-1)) \n",
    "                                        for i in range(1, 3)]).flatten(), :\n",
    "                             ].groupby([0])[1].idxmax().values.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_prior_state_idx % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 6],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.values[best_prior_state_idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trans_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-d184d7adc60d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_prior_state_idx\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m pd.DataFrame(trans_probs).loc[np.array([(np.array([sampled_states_idx[new_particles_idx]]) + 3 * (i - 1)) \n\u001b[0m\u001b[1;32m      3\u001b[0m                                         for i in range(1, 3)]).flatten(), :\n\u001b[1;32m      4\u001b[0m                              ].groupby([0])[1].idxmax().values.reshape(1,-1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trans_probs' is not defined"
     ]
    }
   ],
   "source": [
    "best_prior_state_idx = \\\n",
    "pd.DataFrame(trans_probs).loc[np.array([(np.array([sampled_states_idx[new_particles_idx]]) + \n",
    "                                         len(candidate_roads) * (i - 1)) \n",
    "                                        for i in range(1, (self.n + 1))]).flatten(), :\n",
    "                             ].groupby([0])[1].idxmax().values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_prior_state_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0,\n",
       "        0, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2,\n",
       "        0, 2, 0, 2, 0, 2]])"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_prior_state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_prior_state = particles[best_prior_state_idx][0]  # (n, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_prior_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viterbi_trellis.append(best_prior_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viterbi_trellis_idx.append(best_prior_state_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample new particles\n",
    "new_particles_idx = np.random.choice(a=range(sampled_states.shape[0]), \n",
    "                                     size=n, \n",
    "                                     p=sensor_probs.flatten())  # (1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_particles = sampled_states[new_particles_idx]  # (n, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "particles = new_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimate current particle filter fit quality of hypotheses to data; should research good metrics more.\n",
    "    # Came up with this on my own. \n",
    "fit_quality = [np.max(weights), np.mean(weights), np.median(weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0032343141665107955, 8.015634433554048e-05, 2.8602818874616793e-174]"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def update_dist(obs, num_iter, max_dist2=None):\n",
    "\"\"\"\n",
    "Re-sample particles from transition model given re-estimated likelihood of existing particles.\n",
    "Weighted sample with replacement. \n",
    "\n",
    "numpy array of floats [lat, long, dist_from_prev_m, time_delta_sec]\n",
    "\n",
    "Note: if find no solutions, need to remove points in a signal break until HMM 'heals.'\n",
    "If break > 180 sec, separate into two trips.\n",
    "\"\"\"\n",
    "# Update stored obs\n",
    "obs_coords = obs[['lat', 'long']] # obs[:2]\n",
    "dist_prev, time_delta_prev = obs[['dist_from_prev_m', 'time_delta_sec']] # obs[2:]\n",
    "obs_history.append(obs_coords)\n",
    "# Get candidate roads given obs\n",
    "candidate_roads = get_candidate_roads(obs_coords) # ignore distance\n",
    "# What if candidate_roads is empty?? Then, seems logical to return No Match, and abort. \n",
    "if candidate_roads is None:\n",
    "    return 'Aborted'\n",
    "# Get transition probs; for first observation, ignore this part of the algorithm \n",
    "# and treat sensor probs as prior probabilities (N&K.\n",
    "if num_iter > 1:\n",
    "    trans_probs = apply_transition_model(candidate_roads, \n",
    "                                              dist_prev, \n",
    "                                              time_delta_prev) # (n * c, 1) first n rows for first candidate, etc.\n",
    "    # Aggregate probs by candidate roads\n",
    "    trans_probs_split = np.array(np.split(trans_probs, n, axis=0))  # (n, c, 1)\n",
    "    trans_probs_agg = np.sum(trans_probs_split, axis=0)  # (c, 1)\n",
    "    # Sample new states\n",
    "    # Sample new states\n",
    "    sampled_states_idx = np.random.choice(range(len(candidate_roads)), \n",
    "                                      n, \n",
    "                                      list(trans_probs_agg.flatten()))  # (n, )\n",
    "    sampled_states = candidate_roads[sampled_states_idx]  # (n, 2)\n",
    "else:\n",
    "    sampled_states = candidate_roads\n",
    "\n",
    "# Get sensor probs\n",
    "sensor_probs = estimate_weights_per_particle(sampled_states, obs_coords)  # (n, 1)\n",
    "\n",
    "# Joint prob, for viterbi backtracking. \n",
    "if num_iter > 1:\n",
    "    joint_prob = np.multiply(\n",
    "        np.multiply(sensor_probs, np.max(trans_probs_split, axis=0)[sampled_states_idx]),  # (n, 1)\n",
    "        weights\n",
    "        )\n",
    "# else:\n",
    "#     joint_prob = sensor_probs\n",
    "# weights = joint_prob  # Effectively setting prior probabilites in iter 1\n",
    "\n",
    "# Sample new particles\n",
    "new_particles_idx = np.random.choice(a=range(sampled_states.shape[0]), \n",
    "                                     size=n, \n",
    "                                     p=sensor_probs.flatten())  # (1, n) if not iter 1 else (1, c) \n",
    "new_particles = sampled_states[new_particles_idx]  # (n, 2)\n",
    "\n",
    "# Best prior state/particle for a given candidate state, for viterbi backtracking. \n",
    "if num_iter > 1:\n",
    "    best_prior_state_idx = \\\n",
    "    pd.DataFrame(trans_probs).loc[np.array([(np.array([sampled_states_idx[new_particles_idx]]) + \n",
    "                                         len(candidate_roads) * (i - 1)) \n",
    "                                        for i in range(1, (n + 1))]).flatten(), :\n",
    "                             ].groupby([0])[1].idxmax().values.reshape(1, -1) # (1, n)\n",
    "    best_prior_state_idx = best_prior_state_idx % len(candidate_roads)  # get back to particle-space\n",
    "    best_prior_state = particles[best_prior_state_idx][0]  # (n, 2)\n",
    "    viterbi_trellis.append(best_prior_state)\n",
    "    viterbi_trellis_idx.append(best_prior_state_idx)\n",
    "else:\n",
    "    joint_prob = sensor_probs[new_particles_idx]  # Effectively setting prior probabilites in iter 1\n",
    "    \n",
    "weights = joint_prob\n",
    "\n",
    "particles = new_particles\n",
    "\n",
    "# Estimate current particle filter fit quality of hypotheses to data; should research good metrics more.\n",
    "    # Came up with this on my own. \n",
    "fit_quality = [np.max(weights), np.mean(weights), np.median(weights)]\n",
    "\n",
    "return fit_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_iter = 0\n",
    "converged = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>lat_prev</th>\n",
       "      <th>long_prev</th>\n",
       "      <th>dist_from_prev_m</th>\n",
       "      <th>time_delta_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "      <td>34.360479</td>\n",
       "      <td>-28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "      <td>91.527346</td>\n",
       "      <td>-119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "      <td>165.996049</td>\n",
       "      <td>-51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.75015</td>\n",
       "      <td>-122.39256</td>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "      <td>107.168918</td>\n",
       "      <td>-252.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       long  lat_prev  long_prev  dist_from_prev_m  time_delta_sec\n",
       "0  37.75134 -122.39488       NaN        NaN               NaN             NaN\n",
       "1  37.75136 -122.39527  37.75134 -122.39488         34.360479           -28.0\n",
       "2  37.75199 -122.39460  37.75136 -122.39527         91.527346          -119.0\n",
       "3  37.75080 -122.39346  37.75199 -122.39460        165.996049           -51.0\n",
       "4  37.75015 -122.39256  37.75080 -122.39346        107.168918          -252.0"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obs = data.iloc[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat                  37.751360\n",
       "long               -122.395270\n",
       "lat_prev             37.751340\n",
       "long_prev          -122.394880\n",
       "dist_from_prev_m     34.360479\n",
       "time_delta_sec      -28.000000\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def fit(data#, max_iter=100, conv_tol=0.001\n",
    "#            ):\n",
    "\"\"\"\n",
    "Iterate over rows in data, training particle filter. \n",
    "Rows assumed to be sequentially ordered. \n",
    "\n",
    "data, numpy array (num obs, data dim)\n",
    "data cols include lat, long, dist_from_prev_m, time_delta_sec (all floats)\n",
    "\n",
    "Note, calling fit multiple times would result in nonsense results, so this is prevented. \n",
    "\"\"\"\n",
    "if fitted:\n",
    "    print(\"Already fit - Cannot fit multiple times.\")\n",
    "    return None\n",
    "data = data.copy()\n",
    "num_iter = 0\n",
    "converged = False  # Is this relevant for particle filters?\n",
    "#         while num_iter < max_iter and not converged:\n",
    "#             num_iter += 1\n",
    "for obs in data:\n",
    "    num_iter += 1\n",
    "    fit_quality = update_dist(obs, num_iter)\n",
    "    if fit_quality == \"Aborted\":\n",
    "        return \"Aborted\"\n",
    "    # Shouldn't I use DP/Viterbi for this?? \n",
    "    # Nearest neighbor filter, hungarian algorithm?? p. 601\n",
    "    # Best at any given point in time will suffer in the beginning before particle dist has converged, ie\n",
    "    # during the burn-in period.\n",
    "    # So, seems can still use DP, but proceed backward in time. Wait, that is Viterbi :) \n",
    "        # Happily I have a finite state space. \n",
    "    print(\"On iteration %d, fit quality of MAX %3.2f, MEAN %3.2f, MEDIAN %3.2f\" % \n",
    "          (num_iter, fit_quality[0], fit_quality[1], fit_quality[2]))\n",
    "print(\"Done.\")\n",
    "fitted = True\n",
    "return fit_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_last_state_idx = np.argmax(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_last_state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_last_state = particles[best_last_state_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  37.75134205, -122.39485713])"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backtracked_states = [best_last_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  37.75134205, -122.39485713])]"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtracked_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j=len(viterbi_trellis) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_trellis_idx[j].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_last_state_idx = viterbi_trellis_idx[j].flatten()[best_last_state_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_last_state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_last_state = viterbi_trellis[j][best_last_state_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  37.75134205, -122.39485713])"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backtracked_states.append(best_last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  37.75134205, -122.39485713]), array([  37.75134205, -122.39485713])]"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtracked_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(len(viterbi_trellis) - 1, -1, -1):\n",
    "    best_last_state_idx = viterbi_trellis_idx[j].flatten()[best_last_state_idx]\n",
    "    best_last_state = viterbi_trellis[j][best_last_state_idx]\n",
    "    backtracked_states.append(best_last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put in chronological order\n",
    "backtracked_states = backtracked_states[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  37.75134205, -122.39485713]),\n",
       " array([  37.75134205, -122.39485713]),\n",
       " array([  37.75134205, -122.39485713]),\n",
       " array([  37.75134205, -122.39485713])]"
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtracked_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi():\n",
    "    \"\"\"\n",
    "    Returns list of backtracked states (float lat long),\n",
    "    the imputed GPS trace that has been 'snapped' to roads.\n",
    "    Use Viterbi to get optimal path via DP - \n",
    "    happily I have a finite state space due to my constraint \n",
    "    to only maintain n particles. \n",
    "\n",
    "    Note1, Viterbi relies on the Markov property, which\n",
    "    can apply here. \n",
    "    Note2, if particle filter was aborted due to no matching candidate roads,\n",
    "    Viterbi will still be able to backtrack the trace up until that point. \n",
    "    \"\"\"\n",
    "    # Start with the last observation to the viterbi trellis\n",
    "    best_last_state_idx = np.argmax(weights)\n",
    "    best_last_state = particles[best_last_state_idx]\n",
    "    backtracked_states = [best_last_state]\n",
    "    # Backtrack through the viterbi trellis (#obs, n, 2) actual lat/long states\n",
    "    for j in range(len(viterbi_trellis) - 1, -1, -1):\n",
    "        best_last_state_idx = viterbi_trellis_idx[j].flatten()[best_last_state_idx]\n",
    "        best_last_state = viterbi_trellis[j][best_last_state_idx]\n",
    "        backtracked_states.append(best_last_state)\n",
    "    # Put in chronological order\n",
    "    backtracked_states = backtracked_states[::-1]\n",
    "    return backtracked_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# END DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range(5*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([i for i in range(5)] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([j for l in [[i]*3 for i in range(0,5)] for j in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [ 0,  1],\n",
       "       [ 0,  2],\n",
       "       [ 1,  3],\n",
       "       [ 1,  4],\n",
       "       [ 1,  5],\n",
       "       [ 2,  6],\n",
       "       [ 2,  7],\n",
       "       [ 2,  8],\n",
       "       [ 3,  9],\n",
       "       [ 3, 10],\n",
       "       [ 3, 11],\n",
       "       [ 4, 12],\n",
       "       [ 4, 13],\n",
       "       [ 4, 14]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((np.array([j for l in [[i]*3 for i in range(0,5)] for j in l]).reshape(15,1),\n",
    "                np.array(range(5*3)).reshape(15,1)\n",
    "               ), axis=1\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class particle_filter:\n",
    "    \"\"\"\n",
    "    Fit a particle filter to data given a transition and sensor model. \n",
    "    \n",
    "    n, integer, number of particles to maintain at each step in time.\n",
    "    max_road_dist_m, float, max road distance above which candidate roads aren't considered (meters). \n",
    "    prob_floor, float between 0 and 1, min probability below which candidate roads aren't considered (not in N&K).\n",
    "    Sensor model, , is some model object we will use to predict the likelihood.\n",
    "    Transition model, , is some model object we will use to predict next state.\n",
    "    ## max_iter, integer, the maximum iterations used in fitting the particle filter.\n",
    "    ## conv_tol, the convergence tolerance that will trigger early termination of fitting the particle filter. \n",
    "    \"\"\"\n",
    "    def __init__(self, road_strtree, n=50, max_road_dist_m=200, prob_floor=0.0, \n",
    "                 sensor_model=gps_sensor_model(), transition_model=gps_transition_model(), \n",
    "                 viterbi_trellis=[], viterbi_trellis_idx=[]):\n",
    "        self.road_strtree = road_strtree\n",
    "        self.n = n\n",
    "        self.max_road_dist_m = max_road_dist_m\n",
    "        self.prob_floor = min(max(prob_floor, 0), 1)\n",
    "        self.particles = np.array([])\n",
    "        self.weights = np.zeros((1, self.n))  # Log-transform probabilities\n",
    "        \n",
    "        self.sensor_model = sensor_model\n",
    "        self.transition_model = transition_model\n",
    "        self.viterbi_trellis = viterbi_trellis  # list of particle np arrays\n",
    "        self.viterbi_trellis_idx = viterbi_trellis_idx  # list of np arrays, columns candidate road indices, particle indices\n",
    "        \n",
    "        self.fitted = False\n",
    "        \n",
    "        self.obs_history = []  # Record history of observations (lat, long)\n",
    "        \n",
    "    def get_candidate_roads(self, obs):\n",
    "        \"\"\"\n",
    "        obs list of floats [lat, long]\n",
    "        max_dist maximum allowable closest distance for prospective matched roads; those whose \n",
    "        closest point is >= max_dist meters away are ignored (200 in N&K).\n",
    "        Returns numpy array float of lat,long closest point to road from obs (N&K).\n",
    "\n",
    "        Leverages shapely and shapefiles for cabspotting data.\n",
    "        Relies on great_circle_dist() helper function. \n",
    "        \"\"\"\n",
    "        # Fiona uses (long, lat) format\n",
    "        obs = Point(obs[::-1])\n",
    "        # Want to consider only matches within max_dist meters\n",
    "        # Need to transform decimal degrees into meters\n",
    "        # Should I deviate from the default value of buffer for a Point??\n",
    "        # According to 200 * 360 / (2 * np.pi * 6371000), a distance of 200m corresponds to 0.0017986432118374611 decimal degrees\n",
    "        # obs.buffer(1.0)\n",
    "        matching_roads = self.road_strtree.query(obs.buffer(0.002))\n",
    "        if len(matching_roads) == 0:\n",
    "            matching_roads = self.road_strtree.query(obs.buffer(1.0))  # This should be a big buffer\n",
    "        # If matching roads is still empty, return None\n",
    "        if len(matching_roads) == 0:\n",
    "            print(\"No matching roads found! Aborting particle filter.\")\n",
    "            return None\n",
    "        # Ordered as (obs, road); just take road's coords\n",
    "        # Take only the closest point on the candidate road (1st index)\n",
    "        closest_pts = np.array([[list(p.coords)[0] for p in nearest_points(obs, r)][1] for r in matching_roads])\n",
    "        # Calculate distance between obs and candidate_roads, to filter them\n",
    "        candidate_dist_coords = \\\n",
    "        np.concatenate([closest_pts, np.tile(np.array(obs), (closest_pts.shape[0], 1))], axis=1)\n",
    "        # Convert back to lat, long from long, lat\n",
    "        candidate_dist_coords = candidate_dist_coords[:, [1, 0, 3, 2]]\n",
    "        candidate_dists = great_circle_dist(candidate_dist_coords)\n",
    "        # Filtered result of closest point on matching candidate roads\n",
    "        # no. matches 2 (default buffer); 52 (buffer of 0.002); 74 (buffer of 1.0)\n",
    "            # Reverse order to be lat, long\n",
    "        candidate_roads = closest_pts[candidate_dists <= self.max_road_dist_m][:, [1,0]]\n",
    "        if len(candidate_roads) == 0:\n",
    "            print(\"No matching roads found within max road distance! Aborting particle filter.\")\n",
    "            return None\n",
    "        return candidate_roads\n",
    "    \n",
    "    def estimate_weights_per_particle(self, sampled_states, obs_coords):\n",
    "        \"\"\"\n",
    "        Estimate likelihood of particle given evidence, P(evidence|particle).\n",
    "        Uses sensor model.\n",
    "        \n",
    "        obs_coords is single observation at time t, pandas series. \n",
    "        \"\"\"\n",
    "        probs = self.sensor_model.estimate_likelihood(sampled_states, obs_coords.values.reshape(1, 2))\n",
    "        return probs\n",
    "        \n",
    "    def apply_transition_model(self, candidate_roads, dist_prev, time_delta_prev):\n",
    "        \"\"\"\n",
    "        returns probabilities of transition, given current particles\n",
    "        numpy array of floats between 0 and 1, normalized, (n * c, 1)\n",
    "        \n",
    "        candidate_roads numpy array float of lat,long closest point to road from obs (N&K)\n",
    "        dist_prev float, meters\n",
    "        time_delta_prev float, sec\n",
    "        \n",
    "        Zeroize very small probability candidates, but don't change shape of particles.\n",
    "        Then, if some particles are very unlikely, we cull them and resample from more likely particles. \n",
    "        \"\"\"\n",
    "        # Crossproduct for each particle, all roads\n",
    "        state_last2 = np.concatenate([np.concatenate((self.particles, \n",
    "                                                      np.tile(c, (self.n, 1))), \n",
    "                                                     axis=1) \n",
    "                                      for c in candidate_roads], axis=0)  # (n * c, 4)\n",
    "        obs_last2 = np.array(self.obs_history[-2:]).reshape((1, 4))\n",
    "        probs = self.transition_model.prob_transition(state_last2, obs_last2, \n",
    "                                                      dist_prev, time_delta_prev)  # (n * c, 1)\n",
    "        # Effectively, candidate roads will be possible from different original particle hypotheses\n",
    "        # Zeroise small probs, without changing array shape\n",
    "        probs[probs <= self.prob_floor] = 0.0\n",
    "        # Re-normalize\n",
    "        probs = probs / np.sum(probs)\n",
    "        return probs\n",
    "        \n",
    "    def update_dist(self, obs, num_iter, max_dist2=None):\n",
    "        \"\"\"\n",
    "        Re-sample particles from transition model given re-estimated likelihood of existing particles.\n",
    "        Weighted sample with replacement. \n",
    "        \n",
    "        obs numpy array of floats [lat, long, dist_from_prev_m, time_delta_sec]\n",
    "        num_iter integer, iteration count\n",
    "        max_dist2 not used\n",
    "\n",
    "        Note: if find no solutions, need to remove points in a signal break until HMM 'heals.'\n",
    "        If break > 180 sec, separate into two trips.\n",
    "        \"\"\"\n",
    "        # Update stored obs\n",
    "        obs_coords = obs[['lat', 'long']] # obs[:2]\n",
    "        dist_prev, time_delta_prev = obs[['dist_from_prev_m', 'time_delta_sec']] # obs[2:]\n",
    "        self.obs_history.append(obs_coords)\n",
    "        # Get candidate roads given obs\n",
    "        candidate_roads = self.get_candidate_roads(obs_coords) # ignore distance\n",
    "        # What if candidate_roads is empty?? Then, seems logical to return No Match, and abort. \n",
    "        if candidate_roads is None:\n",
    "            return 'Aborted'\n",
    "        # Get transition probs; for first observation, ignore this part of the algorithm \n",
    "        # and treat sensor probs as prior probabilities (N&K.\n",
    "        if num_iter > 1:\n",
    "            trans_probs = self.apply_transition_model(candidate_roads, \n",
    "                                                      dist_prev, \n",
    "                                                      time_delta_prev) # (n * c, 1) first n rows for first candidate, etc.\n",
    "            # Aggregate probs by candidate roads\n",
    "            trans_probs_split = np.array(np.split(trans_probs, self.n, axis=0))  # (n, c, 1)\n",
    "            trans_probs_agg = np.sum(trans_probs_split, axis=0)  # (c, 1)\n",
    "            # Sample new states\n",
    "            # Sample new states; index into candidate roads\n",
    "            sampled_states_idx = np.random.choice(range(len(candidate_roads)), \n",
    "                                                  self.n, \n",
    "                                                  list(trans_probs_agg.flatten()))  # (n, )\n",
    "            sampled_states = candidate_roads[sampled_states_idx]  # (n, 2)\n",
    "            sampled_states_best_prior = np.argmax(trans_probs_split[:, sampled_states_idx, :], axis=0)  # (n, 1)\n",
    "        else:\n",
    "            sampled_states = candidate_roads\n",
    "            sampled_states_best_prior = np.zeros((self.n, 1))  # dummy value\n",
    "        \n",
    "        # Get sensor probs\n",
    "        sensor_probs = self.estimate_weights_per_particle(sampled_states, obs_coords)  # (n, 1) \n",
    "        \n",
    "        # Joint prob, for viterbi backtracking. Do this in the log domain.  \n",
    "        if num_iter > 1:\n",
    "            # May want to do this in the log domain?\n",
    "            joint_prob = np.add(\n",
    "                np.add(np.log(sensor_probs), np.max(np.log(trans_probs_split), axis=0)[sampled_states_idx]),  # (n, 1)\n",
    "                self.weights\n",
    "                )\n",
    "#         else:\n",
    "#             joint_prob = sensor_probs\n",
    "#         self.weights = joint_prob  # Effectively setting prior probabilites in iter 1\n",
    "        \n",
    "        # Sample new particles, from the n sampled states\n",
    "        new_particles_idx = np.random.choice(a=range(sampled_states.shape[0]), \n",
    "                                             size=self.n, \n",
    "                                             p=sensor_probs.flatten())  # (1, n) if not iter 1 else (1, c)\n",
    "        new_particles = sampled_states[new_particles_idx]  # (n, 2)\n",
    "        \n",
    "        # Best prior state/particle for a given candidate state, for viterbi backtracking. \n",
    "        if num_iter > 1:\n",
    "#             # Tie together candidate road indices, particle indices, trans probs, and particles\n",
    "#             trans_probs_stack = \\\n",
    "#             np.concatenate((np.array([[i] * self.n for i in range(len(candidate_roads))]\n",
    "#                                     ).reshape(len(candidate_roads) * self.n, 1), \n",
    "#                             np.array([j for j in range(self.n)] * len(candidate_roads)\n",
    "#                                     ).reshape(len(candidate_roads) * self.n, 1), \n",
    "#                             trans_probs.reshape(len(candidate_roads) * self.n, 1),\n",
    "#                             np.tile(self.particles, len(candidate_roads)).reshape(len(candidate_roads) * self.n, -1)\n",
    "#                            ), axis=1)  # (n * c, 5)   cols: particle_id and transition probs\n",
    "#             # For each candidate road, max-prob prev state.\n",
    "#             # Given candidate roads are sampled...\n",
    "#             trans_prob_idx = \\\n",
    "#             np.array([(np.array([sampled_states_idx[new_particles_idx]]) + \n",
    "#                                          len(candidate_roads) * (i - 1)) \n",
    "#                                         for i in range(1, (self.n + 1))]).flatten()  # (n * c, 1)\n",
    "#             # ...of sampled roads, which prior states are most likely?\n",
    "#                 # Not all roads may have been sampled.\n",
    "#                 # idxmax is in the range <= c.\n",
    "#                 # best_prior_state_idx is filtered particle indices\n",
    "#             best_prior_state_idx = \\\n",
    "#             pd.DataFrame(trans_probs_stack).loc[trans_prob_idx, :\n",
    "#                              ].groupby([0])[2].idxmax().values.reshape(1, -1)  # (1, <= c)\n",
    "#             # get back to particle-space\n",
    "# #             best_prior_state = trans_probs_stack[best_prior_state_idx][0][:, -2:]  # (<= c, 2)\n",
    "#             prior_particles = self.particles  # best_prior_state_idx indexes into self.particles\n",
    "#             best_prior_state_idx = np.concatenate((best_prior_state_idx.reshape(-1, 1), # indexed cand roads (next particles idx)\n",
    "#                                                    trans_probs_stack[best_prior_state_idx][0][:, 1],  # particles (cur particles idx)\n",
    "#                                                   )\n",
    "#                                                   , axis=1)  # (<= c, 2); cand roads, particles\n",
    "# #             best_prior_state_idx = trans_probs_stack[best_prior_state_idx][0][:, :2]  # (<= c, 2); cand roads, particles\n",
    "# #             best_prior_state_idx = math.floor(best_prior_state_idx / len(candidate_roads))\n",
    "# #             best_prior_state = self.particles[best_prior_state_idx][0]  # (n, 2)\n",
    "            self.viterbi_trellis.append(new_particles)\n",
    "            self.viterbi_trellis_idx.append(sampled_states_best_prior)\n",
    "        else:\n",
    "            self.viterbi_trellis.append(new_particles)\n",
    "            self.viterbi_trellis_idx.append(sampled_states_best_prior)\n",
    "            joint_prob = np.log(sensor_probs[new_particles_idx])  # Effectively setting prior probabilites in iter 1\n",
    "            \n",
    "        self.weights = joint_prob\n",
    "        \n",
    "        self.particles = new_particles\n",
    "        \n",
    "        # Estimate current particle filter fit quality of hypotheses to data; should research good metrics more.\n",
    "            # Came up with this on my own. \n",
    "        fit_quality = [np.max(self.weights), np.mean(self.weights), np.median(self.weights)]\n",
    "        \n",
    "        return fit_quality\n",
    "        \n",
    "    def fit(self, data#, max_iter=100, conv_tol=0.001\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Iterate over rows in data, training particle filter. \n",
    "        Rows assumed to be sequentially ordered. \n",
    "        \n",
    "        data, numpy array: cols include lat, long, dist_from_prev_m, time_delta_sec (all floats).\n",
    "        lat, long in decimal degrees.\n",
    "        \n",
    "        Returns fit_quality, list of floats: max, mean, and median probability of most recent particles.\n",
    "        \n",
    "        Note, calling fit multiple times would result in nonsense results, so this is prevented. \n",
    "        \"\"\"\n",
    "        if self.fitted:\n",
    "            print(\"Already fit - Cannot fit multiple times.\")\n",
    "            return None\n",
    "        data = data.copy()\n",
    "        num_iter = 0\n",
    "        converged = False  # Is this relevant for particle filters?\n",
    "#         while num_iter < max_iter and not converged:\n",
    "#             num_iter += 1\n",
    "        for obs in data.iterrows():\n",
    "            num_iter += 1\n",
    "            fit_quality = self.update_dist(obs[1], num_iter)\n",
    "            if fit_quality == \"Aborted\":\n",
    "                return \"Aborted\"\n",
    "            # Shouldn't I use DP/Viterbi for this?? \n",
    "            # Nearest neighbor filter, hungarian algorithm?? p. 601\n",
    "            # Best at any given point in time will suffer in the beginning before particle dist has converged, ie\n",
    "            # during the burn-in period.\n",
    "            # So, seems can still use DP, but proceed backward in time. Wait, that is Viterbi :) \n",
    "                # Happily I have a finite state space. \n",
    "            print(\"On iteration %d, fit quality of MAX %3.2f, MEAN %3.2f, MEDIAN %3.2f\" % \n",
    "                  (num_iter, fit_quality[0], fit_quality[1], fit_quality[2]))\n",
    "        print(\"Done.\")\n",
    "        self.fitted = True\n",
    "        return fit_quality\n",
    "    \n",
    "    def viterbi(self):\n",
    "        \"\"\"\n",
    "        Returns list of backtracked states (float lat long),\n",
    "        the imputed GPS trace that has been 'snapped' to roads.\n",
    "        Use Viterbi to get optimal path via DP - \n",
    "        happily I have a finite state space due to my constraint \n",
    "        to only maintain n particles. \n",
    "        \n",
    "        Note1, Viterbi relies on the Markov property, which\n",
    "        can apply here. \n",
    "        Note2, if particle filter was aborted due to no matching candidate roads,\n",
    "        Viterbi will still be able to backtrack the trace up until that point. \n",
    "        \"\"\"\n",
    "        # Start with the last observation to the viterbi trellis\n",
    "        best_last_state_idx = np.argmax(self.weights)\n",
    "        backtracked_states = []\n",
    "#         best_last_state = self.particles[best_last_state_idx]\n",
    "#         backtracked_states = [best_last_state]\n",
    "        # Backtrack through the viterbi trellis (#obs, n, 2) actual lat/long states\n",
    "        for j in range(len(self.viterbi_trellis) - 1, -1, -1):\n",
    "            # for that 'candidate road,' the particle index (col 1)\n",
    "            best_last_state = self.viterbi_trellis[j][best_last_state_idx]\n",
    "            best_last_state_idx = int(self.viterbi_trellis_idx[j][best_last_state_idx])\n",
    "            backtracked_states.append(best_last_state)\n",
    "        # Put in chronological order\n",
    "        backtracked_states = backtracked_states[::-1]\n",
    "        return backtracked_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.array(range(5)).reshape(5,1)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=5; candidate_roads=np.array([1,2,3]); particles=np.array([1,2,3,4,5]); trans_probs = np.array(range(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[i] * n for i in range(len(candidate_roads))]\n",
    "                                    ).reshape(len(candidate_roads) * n, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([j for j in range(n)] * len(candidate_roads)).flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_probs=trans_probs.reshape(15,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_probs_split = np.array(np.split(trans_probs, n, axis=0))  # (n, c, 1)\n",
    "trans_probs_agg = np.sum(trans_probs_split, axis=0)  # (c, 1)\n",
    "# Sample new states\n",
    "# Sample new states; index into candidate roads\n",
    "sampled_states_idx = np.random.choice(range(len(candidate_roads)), \n",
    "                                      n, \n",
    "                                      list(trans_probs_agg.flatten()))  # (n, )\n",
    "sampled_states = candidate_roads[sampled_states_idx]  # (n, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 1)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_states_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0],\n",
       "        [ 1],\n",
       "        [ 2]],\n",
       "\n",
       "       [[ 3],\n",
       "        [ 4],\n",
       "        [ 5]],\n",
       "\n",
       "       [[ 6],\n",
       "        [ 7],\n",
       "        [ 8]],\n",
       "\n",
       "       [[ 9],\n",
       "        [10],\n",
       "        [11]],\n",
       "\n",
       "       [[12],\n",
       "        [13],\n",
       "        [14]]])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_states_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2],\n",
       "        [ 1],\n",
       "        [ 1],\n",
       "        [ 1],\n",
       "        [ 1]],\n",
       "\n",
       "       [[ 5],\n",
       "        [ 4],\n",
       "        [ 4],\n",
       "        [ 4],\n",
       "        [ 4]],\n",
       "\n",
       "       [[ 8],\n",
       "        [ 7],\n",
       "        [ 7],\n",
       "        [ 7],\n",
       "        [ 7]],\n",
       "\n",
       "       [[11],\n",
       "        [10],\n",
       "        [10],\n",
       "        [10],\n",
       "        [10]],\n",
       "\n",
       "       [[14],\n",
       "        [13],\n",
       "        [13],\n",
       "        [13],\n",
       "        [13]]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_split[:, sampled_states_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_states_best_prior = np.argmax(trans_probs_split[:, sampled_states_idx, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_states_best_prior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(particles, len(candidate_roads)).reshape(len(candidate_roads) * n, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_probs_stack = \\\n",
    "            np.concatenate((np.array([[i] * n for i in range(len(candidate_roads))]\n",
    "                                    ).reshape(len(candidate_roads) * n, 1), \n",
    "                            np.array([j for j in range(n)] * len(candidate_roads)).reshape(len(candidate_roads) * n, 1), \n",
    "                            trans_probs.reshape(len(candidate_roads) * n, 1),\n",
    "                            np.tile(particles, len(candidate_roads)).reshape(len(candidate_roads) * n, -1)\n",
    "                           ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 4)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  1],\n",
       "       [ 0,  1,  1,  2],\n",
       "       [ 0,  2,  2,  3],\n",
       "       [ 0,  3,  3,  4],\n",
       "       [ 0,  4,  4,  5],\n",
       "       [ 1,  0,  5,  1],\n",
       "       [ 1,  1,  6,  2],\n",
       "       [ 1,  2,  7,  3],\n",
       "       [ 1,  3,  8,  4],\n",
       "       [ 1,  4,  9,  5],\n",
       "       [ 2,  0, 10,  1],\n",
       "       [ 2,  1, 11,  2],\n",
       "       [ 2,  2, 12,  3],\n",
       "       [ 2,  3, 13,  4],\n",
       "       [ 2,  4, 14,  5]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 5]])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_prior_state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_stack[best_prior_state_idx][0][:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_probs_stack[best_prior_state_idx][0][:, :2][1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## it seems i only have beijing data (10K) or sf taxi data (500)\n",
    "# Read in taxi data\n",
    "# Each data of different length; ideal use case for pyspark\n",
    "# Note, the OSM extract basemap data has POI info as well (https://download.bbbike.org)\n",
    "# also try uber h3 spatial index\n",
    "trace_dir = '/Volumes/LaCie/datasets/ms_taxi/taxi_log_2008_by_id/roads.shp'   # MS Taxi\n",
    "basemap_dir = '/Volumes/LaCie/datasets/Beijing-shp/shape/'\n",
    "\n",
    "trace_dir = '/Volumes/LaCie/datasets/cabspottingdata/'   # CRAWDAD cabspotting\n",
    "basemap_dir = '/Volumes/LaCie/datasets/SanFrancisco-shp/shape/roads.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.45 ms, sys: 4.22 ms, total: 6.67 ms\n",
      "Wall time: 69.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_files = [f for f in os.listdir(trace_dir) if re.match(r'new_.*\\.txt', f)]  # glob.glob(trace_dir + \"new_*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 20.9s cabspottingdata; can try spark or dask? or multiprocessing, joblib\n",
    "# trace_list = []\n",
    "# for file_ in all_files:\n",
    "#     file_df = pd.read_csv(os.path.join(trace_dir, file_), sep=\" \", index_col=None, header=None, \n",
    "#                           names=['lat', 'long', 'occupancy', 'time'])\n",
    "#     trace_list.append(file_df)\n",
    "\n",
    "# # concatenate all dfs into one\n",
    "# trace_df = pd.concat(trace_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new_abcoij.txt'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files_idx = 1\n",
    "all_files[all_files_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just try one trace...\n",
    "trace_df = pd.read_csv(os.path.join(trace_dir, all_files[all_files_idx]), sep=\" \", index_col=None, header=None, \n",
    "                       names=['lat', 'long', 'occupancy', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace_df.loc[:, [\"time\"]] = pd.to_datetime(trace_df.time, origin=\"unix\", unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Order by time\n",
    "trace_df = trace_df.sort_values(by=\"time\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat                 float64\n",
       "long                float64\n",
       "occupancy             int64\n",
       "time         datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cabspotting:\n",
    "\n",
    "latitude and longitude are in decimal degrees, \n",
    "occupancy shows if a cab has a fare (1 = occupied, 0 = free) and \n",
    "time is in UNIX epoch format\n",
    "\"\"\"\n",
    "trace_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.80346</td>\n",
       "      <td>-122.41466</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-17 23:11:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.80519</td>\n",
       "      <td>-122.41773</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-17 23:12:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.80491</td>\n",
       "      <td>-122.41971</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-17 23:13:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.80470</td>\n",
       "      <td>-122.42117</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-17 23:14:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.80448</td>\n",
       "      <td>-122.42295</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-17 23:15:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       long  occupancy                time\n",
       "0  37.80346 -122.41466          1 2008-05-17 23:11:30\n",
       "1  37.80519 -122.41773          1 2008-05-17 23:12:30\n",
       "2  37.80491 -122.41971          1 2008-05-17 23:13:30\n",
       "3  37.80470 -122.42117          1 2008-05-17 23:14:30\n",
       "4  37.80448 -122.42295          1 2008-05-17 23:15:31"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in ...shapefiles? GeoJSON? Which format is best? For parallelization may be one thing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_traces(df, sigma=4.07):\n",
    "    \"\"\"\n",
    "    pandas df, trace_df above with float columns lat, long in degrees\n",
    "    sigma float, representing std dev of measurement error (see above in particle filter class)\n",
    "    \n",
    "    \"The justification for this step is that until we see a point that is at least 2𝜎𝑧 \n",
    "    away from its temporal predecessor, our confidence is low that the apparent movement\n",
    "    is due to actual vehicle movement and not noise.\" (N&K)\n",
    "    \n",
    "    -remove obs that are not 2*meas dist sigma from previous obs  (eliminate 39% of data in N&K)\n",
    "    -Letting sigma = 4.07 meters as in N&K\n",
    "    \"\"\"\n",
    "    MILES_PER_METER = 0.000621371\n",
    "    HOURS_PER_SECOND = 3600.0\n",
    "    data = df.copy()\n",
    "    data[[\"lat_prev\", \"long_prev\"]] = data[[\"lat\", \"long\"]].shift(1)\n",
    "    # Ignore warning about invalid value in arcsin (nan)\n",
    "    data[\"dist_from_prev_m\"] = great_circle_dist(data[[\"lat_prev\", \"long_prev\", \"lat\", \"long\"]].values)  # 1.93 sec\n",
    "    # Add speed so can later filter on unreasonably high speeds\n",
    "    data[\"time_delta_sec\"] = data.time.subtract(data.time.shift(1)) / np.timedelta64(1, 's')  # get seconds\n",
    "    data[\"speed_mph\"] = (data.dist_from_prev_m * MILES_PER_METER).divide(data.time_delta_sec * HOURS_PER_SECOND)\n",
    "    # Take cumsum of dist\n",
    "    dist_cum = data.dist_from_prev_m.cumsum()\n",
    "    # Select points closest to multiples of 2*sigma, in cumsum dist\n",
    "    dist_cum_idx = dist_cum // (2 * sigma)\n",
    "    filter_idx = np.subtract(dist_cum_idx, dist_cum_idx.shift(1)) == 0  # 12% of rows eliminated \n",
    "    data = data[~filter_idx]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 ms, sys: 2.91 ms, total: 16.8 ms\n",
      "Wall time: 17.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 11.8 s\n",
    "# Ignore RuntimeWarning: invalid value encountered in arcsin\n",
    "trace_df = preprocess_traces(trace_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>time</th>\n",
       "      <th>lat_prev</th>\n",
       "      <th>long_prev</th>\n",
       "      <th>dist_from_prev_m</th>\n",
       "      <th>time_delta_sec</th>\n",
       "      <th>speed_mph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.80346</td>\n",
       "      <td>-122.41466</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-17 23:11:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.80519</td>\n",
       "      <td>-122.41773</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-17 23:12:30</td>\n",
       "      <td>37.80346</td>\n",
       "      <td>-122.41466</td>\n",
       "      <td>331.289969</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9.530277e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.80491</td>\n",
       "      <td>-122.41971</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-17 23:13:30</td>\n",
       "      <td>37.80519</td>\n",
       "      <td>-122.41773</td>\n",
       "      <td>176.717645</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.083668e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.80470</td>\n",
       "      <td>-122.42117</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-17 23:14:30</td>\n",
       "      <td>37.80491</td>\n",
       "      <td>-122.41971</td>\n",
       "      <td>130.377203</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.750584e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.80448</td>\n",
       "      <td>-122.42295</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-17 23:15:31</td>\n",
       "      <td>37.80470</td>\n",
       "      <td>-122.42117</td>\n",
       "      <td>158.285057</td>\n",
       "      <td>61.0</td>\n",
       "      <td>4.478768e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       long  occupancy                time  lat_prev  long_prev  \\\n",
       "0  37.80346 -122.41466          1 2008-05-17 23:11:30       NaN        NaN   \n",
       "1  37.80519 -122.41773          1 2008-05-17 23:12:30  37.80346 -122.41466   \n",
       "2  37.80491 -122.41971          1 2008-05-17 23:13:30  37.80519 -122.41773   \n",
       "3  37.80470 -122.42117          1 2008-05-17 23:14:30  37.80491 -122.41971   \n",
       "4  37.80448 -122.42295          1 2008-05-17 23:15:31  37.80470 -122.42117   \n",
       "\n",
       "   dist_from_prev_m  time_delta_sec     speed_mph  \n",
       "0               NaN             NaN           NaN  \n",
       "1        331.289969            60.0  9.530277e-07  \n",
       "2        176.717645            60.0  5.083668e-07  \n",
       "3        130.377203            60.0  3.750584e-07  \n",
       "4        158.285057            61.0  4.478768e-07  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trace_df needs to be pre-processed to separate out actual trips.\n",
    "It is a concatenation of multiple days' worth of trips for a single cab. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace_time_diff = \\\n",
    "np.abs(np.subtract(trace_df.time, trace_df.time.shift(1)) / np.timedelta64(1, 's')).fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2495,    0,    2,    2,    0,    1,    1,    2,    0,    2]),\n",
       " array([     0. ,  18571.2,  37142.4,  55713.6,  74284.8,  92856. ,\n",
       "        111427.2, 129998.4, 148569.6, 167140.8, 185712. ]))"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(trace_time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2486,    2,    0,    1,    2,    1,    1,    0,    0,    1]),\n",
       " array([   0. ,  968.3, 1936.6, 2904.9, 3873.2, 4841.5, 5809.8, 6778.1,\n",
       "        7746.4, 8714.7, 9683. ]))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(trace_time_diff[trace_time_diff < 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1227,  519,  503,  105,   73,   38,   11,    9,    0,    1]),\n",
       " array([  0. ,  84.4, 168.8, 253.2, 337.6, 422. , 506.4, 590.8, 675.2,\n",
       "        759.6, 844. ]))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(trace_time_diff[trace_time_diff < 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_secdiff_thesh_s = 10 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trip_starts = trace_df.index[trace_time_diff > trip_secdiff_thesh_s]  # I guess I could just call these trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_starts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  73,  177,  192,  426,  491, 1218, 1441, 2161, 2249, 2371, 2581,\n",
       "            2605, 2723, 2806, 3650, 3873, 4323, 4425, 4449, 4503, 4569, 4754,\n",
       "            4836, 5376, 5407, 5449],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips_list = []\n",
    "i_prev = 0\n",
    "for i in trip_starts:\n",
    "    trips_list.append(trace_df.loc[i_prev:i-1, :])\n",
    "    i_prev = i\n",
    "trips_list.append(trace_df.loc[i_prev:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 9)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([d.shape[0] for d in trips_list]) == trace_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trace_df.index[np.concatenate((np.array([True]), \n",
    "#                                ~np.equal(np.subtract(trace_df.index.values[:-1], trace_df.time.index.values[1:]), \n",
    "#                         np.full((2503,), -1))), axis=None)\n",
    "#               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "long   -122.418\n",
       "lat     37.8052\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_df.loc[1, ['long', 'lat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fiona uses (long, lat) format\n",
    "obs = Point(trace_df.loc[1, ['long', 'lat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-123.39527 36.75136 2.0 2.0\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,75.50272)\"><circle cx=\"-122.39527\" cy=\"37.75136\" r=\"0.06\" stroke=\"#555555\" stroke-width=\"0.02\" fill=\"#66cc99\" opacity=\"0.6\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.point.Point at 0x1a3db4f050>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.03 ms, sys: 5.03 ms, total: 7.05 ms\n",
      "Wall time: 358 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "roads_shp = fiona.open(basemap_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init': 'epsg:4326'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roads_shp.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## https://gis.stackexchange.com/questions/80881/what-is-unit-of-shapely-length-attribute\n",
    "# line1 = LineString([(15.799406, 40.636069), (15.810173,40.640246)])\n",
    "# print str(line1.length) + \" degrees\"\n",
    "# # 0.0115488362184 degrees\n",
    "\n",
    "# # Geometry transform function based on pyproj.transform\n",
    "# project_m = partial(\n",
    "#     pyproj.transform,\n",
    "#     pyproj.Proj(init='EPSG:4326'),\n",
    "#     pyproj.Proj(init='EPSG:32633'))\n",
    "\n",
    "# line2 = transform(project_m, line1)\n",
    "# print str(line2.length) + \" meters\"\n",
    "# # 1021.77585965 meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': OrderedDict([('osm_id', 'int:11'),\n",
       "              ('name', 'str:48'),\n",
       "              ('ref', 'str:16'),\n",
       "              ('type', 'str:16'),\n",
       "              ('oneway', 'int:1'),\n",
       "              ('bridge', 'int:1'),\n",
       "              ('maxspeed', 'int:3')]),\n",
       " 'geometry': 'LineString'}"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roads_shp.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LineString'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Types of geometries\n",
    "set([r['geometry']['type'] for r in list(roads_shp)])\n",
    "# sf_shp: {'LineString'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Feature',\n",
       " 'id': '0',\n",
       " 'properties': OrderedDict([('osm_id', 4311275),\n",
       "              ('name', 'Bayshore Freeway'),\n",
       "              ('ref', 'US 101'),\n",
       "              ('type', 'motorway'),\n",
       "              ('oneway', 1),\n",
       "              ('bridge', 0),\n",
       "              ('maxspeed', 65)]),\n",
       " 'geometry': {'type': 'LineString',\n",
       "  'coordinates': [(-122.4067318, 37.6552091),\n",
       "   (-122.4066705, 37.6554806),\n",
       "   (-122.4066, 37.6557348),\n",
       "   (-122.4065183, 37.6560026),\n",
       "   (-122.4064254, 37.6562575),\n",
       "   (-122.4063177, 37.6565034),\n",
       "   (-122.4061926, 37.6567579),\n",
       "   (-122.4055559, 37.6579996),\n",
       "   (-122.4053176, 37.6584642),\n",
       "   (-122.4051888, 37.658704),\n",
       "   (-122.405056, 37.6589448),\n",
       "   (-122.4049166, 37.659174),\n",
       "   (-122.4047675, 37.6593902),\n",
       "   (-122.4046031, 37.6596144),\n",
       "   (-122.4044278, 37.6598328),\n",
       "   (-122.404206, 37.660081),\n",
       "   (-122.4039932, 37.6603049),\n",
       "   (-122.4037717, 37.6605162),\n",
       "   (-122.4035097, 37.6607433),\n",
       "   (-122.4032666, 37.6609362),\n",
       "   (-122.4030265, 37.6611163),\n",
       "   (-122.402762, 37.661301),\n",
       "   (-122.4025091, 37.6614653),\n",
       "   (-122.3996913, 37.6632717),\n",
       "   (-122.399424, 37.6634432),\n",
       "   (-122.3991719, 37.6636088),\n",
       "   (-122.3989093, 37.663784),\n",
       "   (-122.398657, 37.6639545),\n",
       "   (-122.3984076, 37.6641304),\n",
       "   (-122.3981713, 37.6643095),\n",
       "   (-122.3979449, 37.664483),\n",
       "   (-122.3977008, 37.6646782),\n",
       "   (-122.3974726, 37.6648686),\n",
       "   (-122.3972573, 37.6650546),\n",
       "   (-122.3970163, 37.6652751),\n",
       "   (-122.396798, 37.6654781),\n",
       "   (-122.3965933, 37.6656734),\n",
       "   (-122.3963931, 37.6658756),\n",
       "   (-122.3961956, 37.6660806),\n",
       "   (-122.3956858, 37.6666107),\n",
       "   (-122.3948905, 37.6674396),\n",
       "   (-122.3927279, 37.6696896)]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roads_shp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.29 s, sys: 46.8 ms, total: 1.33 s\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 1.79 s\n",
    "# Just preserving geometries for my purpose\n",
    "road_geoms = [shape(shp['geometry']) for shp in roads_shp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 254 ms, sys: 28.3 ms, total: 282 ms\n",
      "Wall time: 272 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 282 ms\n",
    "# Create STR-tree\n",
    "road_strtree = STRtree(road_geoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Particle Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del my_particle_filter\n",
    "my_particle_filter = particle_filter(road_strtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select a trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_idx = 6\n",
    "trip_df = trips_list[trip_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>time</th>\n",
       "      <th>lat_prev</th>\n",
       "      <th>long_prev</th>\n",
       "      <th>dist_from_prev_m</th>\n",
       "      <th>time_delta_sec</th>\n",
       "      <th>speed_mph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>37.75200</td>\n",
       "      <td>-122.39424</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 16:51:31</td>\n",
       "      <td>37.75190</td>\n",
       "      <td>-122.39408</td>\n",
       "      <td>17.931029</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>2.738894e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>37.75133</td>\n",
       "      <td>-122.39533</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 16:52:31</td>\n",
       "      <td>37.75200</td>\n",
       "      <td>-122.39424</td>\n",
       "      <td>121.383655</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.491865e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39545</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 16:53:31</td>\n",
       "      <td>37.75133</td>\n",
       "      <td>-122.39533</td>\n",
       "      <td>11.065101</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.183117e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>37.75121</td>\n",
       "      <td>-122.39542</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 16:54:31</td>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39545</td>\n",
       "      <td>16.886498</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.857769e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>37.74989</td>\n",
       "      <td>-122.39581</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 16:55:31</td>\n",
       "      <td>37.75121</td>\n",
       "      <td>-122.39542</td>\n",
       "      <td>150.729224</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.336054e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>37.74977</td>\n",
       "      <td>-122.39696</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 16:58:32</td>\n",
       "      <td>37.74989</td>\n",
       "      <td>-122.39584</td>\n",
       "      <td>99.371313</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.858632e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>37.74948</td>\n",
       "      <td>-122.40114</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 16:59:18</td>\n",
       "      <td>37.74977</td>\n",
       "      <td>-122.39696</td>\n",
       "      <td>368.922254</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.384285e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>37.74836</td>\n",
       "      <td>-122.40856</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:00:19</td>\n",
       "      <td>37.74948</td>\n",
       "      <td>-122.40114</td>\n",
       "      <td>664.161587</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.879284e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>37.74908</td>\n",
       "      <td>-122.41376</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:01:48</td>\n",
       "      <td>37.74836</td>\n",
       "      <td>-122.40856</td>\n",
       "      <td>464.152218</td>\n",
       "      <td>89.0</td>\n",
       "      <td>9.001583e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>37.75216</td>\n",
       "      <td>-122.41411</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:02:42</td>\n",
       "      <td>37.74908</td>\n",
       "      <td>-122.41376</td>\n",
       "      <td>343.860031</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.099098e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>37.75223</td>\n",
       "      <td>-122.41424</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:04:42</td>\n",
       "      <td>37.75216</td>\n",
       "      <td>-122.41410</td>\n",
       "      <td>14.563138</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.189404e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>37.75233</td>\n",
       "      <td>-122.41415</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:05:42</td>\n",
       "      <td>37.75223</td>\n",
       "      <td>-122.41424</td>\n",
       "      <td>13.647440</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.925983e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>37.75231</td>\n",
       "      <td>-122.41417</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:06:42</td>\n",
       "      <td>37.75233</td>\n",
       "      <td>-122.41415</td>\n",
       "      <td>2.835057</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.155658e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>37.75212</td>\n",
       "      <td>-122.41405</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:07:11</td>\n",
       "      <td>37.75231</td>\n",
       "      <td>-122.41417</td>\n",
       "      <td>23.614776</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.405511e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>37.75630</td>\n",
       "      <td>-122.41440</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:08:10</td>\n",
       "      <td>37.75212</td>\n",
       "      <td>-122.41405</td>\n",
       "      <td>465.812218</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.362722e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>37.76159</td>\n",
       "      <td>-122.41496</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:09:08</td>\n",
       "      <td>37.75630</td>\n",
       "      <td>-122.41440</td>\n",
       "      <td>590.277639</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.756616e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>37.76531</td>\n",
       "      <td>-122.41510</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:10:09</td>\n",
       "      <td>37.76159</td>\n",
       "      <td>-122.41496</td>\n",
       "      <td>413.828159</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.170951e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>37.76570</td>\n",
       "      <td>-122.41083</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:11:09</td>\n",
       "      <td>37.76531</td>\n",
       "      <td>-122.41510</td>\n",
       "      <td>377.839470</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.086937e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>37.76576</td>\n",
       "      <td>-122.40820</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:11:34</td>\n",
       "      <td>37.76570</td>\n",
       "      <td>-122.41083</td>\n",
       "      <td>231.278447</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.596775e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>37.76581</td>\n",
       "      <td>-122.40676</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:12:35</td>\n",
       "      <td>37.76576</td>\n",
       "      <td>-122.40820</td>\n",
       "      <td>126.700794</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.585073e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>37.76720</td>\n",
       "      <td>-122.40608</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:13:31</td>\n",
       "      <td>37.76581</td>\n",
       "      <td>-122.40676</td>\n",
       "      <td>165.716217</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.107701e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>37.76857</td>\n",
       "      <td>-122.40779</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:14:36</td>\n",
       "      <td>37.76720</td>\n",
       "      <td>-122.40608</td>\n",
       "      <td>214.007176</td>\n",
       "      <td>61.0</td>\n",
       "      <td>6.055458e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>37.77121</td>\n",
       "      <td>-122.40556</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:15:34</td>\n",
       "      <td>37.76857</td>\n",
       "      <td>-122.40779</td>\n",
       "      <td>352.979286</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.050436e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>37.77303</td>\n",
       "      <td>-122.40326</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:16:36</td>\n",
       "      <td>37.77121</td>\n",
       "      <td>-122.40556</td>\n",
       "      <td>286.047251</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.963327e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>37.77522</td>\n",
       "      <td>-122.40615</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:17:34</td>\n",
       "      <td>37.77303</td>\n",
       "      <td>-122.40326</td>\n",
       "      <td>351.881336</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.047169e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>37.77742</td>\n",
       "      <td>-122.40877</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:18:28</td>\n",
       "      <td>37.77522</td>\n",
       "      <td>-122.40615</td>\n",
       "      <td>335.957783</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.073840e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>37.77915</td>\n",
       "      <td>-122.41088</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 17:19:22</td>\n",
       "      <td>37.77742</td>\n",
       "      <td>-122.40877</td>\n",
       "      <td>267.196117</td>\n",
       "      <td>54.0</td>\n",
       "      <td>8.540531e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>37.78059</td>\n",
       "      <td>-122.41277</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:20:16</td>\n",
       "      <td>37.77915</td>\n",
       "      <td>-122.41088</td>\n",
       "      <td>230.713745</td>\n",
       "      <td>54.0</td>\n",
       "      <td>7.374425e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>37.78320</td>\n",
       "      <td>-122.41406</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:21:16</td>\n",
       "      <td>37.78059</td>\n",
       "      <td>-122.41277</td>\n",
       "      <td>311.575679</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.963152e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>37.78497</td>\n",
       "      <td>-122.41455</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-05-22 17:22:11</td>\n",
       "      <td>37.78320</td>\n",
       "      <td>-122.41406</td>\n",
       "      <td>201.470657</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.322627e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>37.75149</td>\n",
       "      <td>-122.39496</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:35:21</td>\n",
       "      <td>37.75155</td>\n",
       "      <td>-122.39492</td>\n",
       "      <td>7.541822</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.169569e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>37.75156</td>\n",
       "      <td>-122.39496</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:38:22</td>\n",
       "      <td>37.75150</td>\n",
       "      <td>-122.39497</td>\n",
       "      <td>6.729375</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.904116e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>37.75152</td>\n",
       "      <td>-122.39500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:41:25</td>\n",
       "      <td>37.75154</td>\n",
       "      <td>-122.39495</td>\n",
       "      <td>4.926463</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.417204e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>37.75147</td>\n",
       "      <td>-122.39501</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:43:26</td>\n",
       "      <td>37.75151</td>\n",
       "      <td>-122.39501</td>\n",
       "      <td>4.447797</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.258530e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>37.75157</td>\n",
       "      <td>-122.39501</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:45:27</td>\n",
       "      <td>37.75150</td>\n",
       "      <td>-122.39502</td>\n",
       "      <td>7.833141</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.216433e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>37.75157</td>\n",
       "      <td>-122.39498</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:47:29</td>\n",
       "      <td>37.75156</td>\n",
       "      <td>-122.39498</td>\n",
       "      <td>1.111949</td>\n",
       "      <td>59.0</td>\n",
       "      <td>3.252980e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>37.75166</td>\n",
       "      <td>-122.39507</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:49:30</td>\n",
       "      <td>37.75156</td>\n",
       "      <td>-122.39491</td>\n",
       "      <td>17.931080</td>\n",
       "      <td>61.0</td>\n",
       "      <td>5.073704e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>37.75153</td>\n",
       "      <td>-122.39498</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:50:30</td>\n",
       "      <td>37.75166</td>\n",
       "      <td>-122.39507</td>\n",
       "      <td>16.479306</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.740631e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>37.75160</td>\n",
       "      <td>-122.39504</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:51:35</td>\n",
       "      <td>37.75153</td>\n",
       "      <td>-122.39498</td>\n",
       "      <td>9.402771</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2.496842e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>37.75173</td>\n",
       "      <td>-122.39508</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:53:35</td>\n",
       "      <td>37.75160</td>\n",
       "      <td>-122.39500</td>\n",
       "      <td>16.075662</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.335482e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>37.75166</td>\n",
       "      <td>-122.39504</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:54:35</td>\n",
       "      <td>37.75173</td>\n",
       "      <td>-122.39508</td>\n",
       "      <td>8.541231</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.457071e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>37.75155</td>\n",
       "      <td>-122.39510</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:55:35</td>\n",
       "      <td>37.75166</td>\n",
       "      <td>-122.39504</td>\n",
       "      <td>13.320478</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.831925e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>37.75164</td>\n",
       "      <td>-122.39506</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:56:35</td>\n",
       "      <td>37.75155</td>\n",
       "      <td>-122.39510</td>\n",
       "      <td>10.607472</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.051470e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>37.75167</td>\n",
       "      <td>-122.39506</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:57:35</td>\n",
       "      <td>37.75164</td>\n",
       "      <td>-122.39506</td>\n",
       "      <td>3.335848</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9.596292e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>37.75180</td>\n",
       "      <td>-122.39514</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 19:58:36</td>\n",
       "      <td>37.75167</td>\n",
       "      <td>-122.39506</td>\n",
       "      <td>16.075660</td>\n",
       "      <td>61.0</td>\n",
       "      <td>4.548702e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>37.75166</td>\n",
       "      <td>-122.39506</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:00:37</td>\n",
       "      <td>37.75178</td>\n",
       "      <td>-122.39511</td>\n",
       "      <td>14.048854</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.975205e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>37.75158</td>\n",
       "      <td>-122.39502</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:01:38</td>\n",
       "      <td>37.75166</td>\n",
       "      <td>-122.39506</td>\n",
       "      <td>9.565517</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.706619e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>37.75168</td>\n",
       "      <td>-122.39507</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:02:40</td>\n",
       "      <td>37.75158</td>\n",
       "      <td>-122.39502</td>\n",
       "      <td>11.956897</td>\n",
       "      <td>62.0</td>\n",
       "      <td>3.328705e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>37.75161</td>\n",
       "      <td>-122.39512</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:03:39</td>\n",
       "      <td>37.75168</td>\n",
       "      <td>-122.39507</td>\n",
       "      <td>8.939205</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.615142e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>37.75138</td>\n",
       "      <td>-122.39502</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:04:41</td>\n",
       "      <td>37.75161</td>\n",
       "      <td>-122.39512</td>\n",
       "      <td>27.043842</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7.528790e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>37.75154</td>\n",
       "      <td>-122.39500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:05:44</td>\n",
       "      <td>37.75138</td>\n",
       "      <td>-122.39502</td>\n",
       "      <td>17.877871</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4.898056e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>37.75154</td>\n",
       "      <td>-122.39500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:08:43</td>\n",
       "      <td>37.75151</td>\n",
       "      <td>-122.39501</td>\n",
       "      <td>3.449761</td>\n",
       "      <td>61.0</td>\n",
       "      <td>9.761301e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>37.75156</td>\n",
       "      <td>-122.39505</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:10:45</td>\n",
       "      <td>37.75156</td>\n",
       "      <td>-122.39500</td>\n",
       "      <td>4.395941</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.223795e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>37.75157</td>\n",
       "      <td>-122.39509</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:13:45</td>\n",
       "      <td>37.75154</td>\n",
       "      <td>-122.39510</td>\n",
       "      <td>3.449761</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9.923989e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>37.75158</td>\n",
       "      <td>-122.39495</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:15:46</td>\n",
       "      <td>37.75155</td>\n",
       "      <td>-122.39502</td>\n",
       "      <td>7.000250</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.083215e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>37.75156</td>\n",
       "      <td>-122.39495</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:16:46</td>\n",
       "      <td>37.75158</td>\n",
       "      <td>-122.39495</td>\n",
       "      <td>2.223899</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.397528e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>37.75151</td>\n",
       "      <td>-122.39492</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:18:47</td>\n",
       "      <td>37.75154</td>\n",
       "      <td>-122.39490</td>\n",
       "      <td>3.770911</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.084785e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>37.75151</td>\n",
       "      <td>-122.39493</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:21:52</td>\n",
       "      <td>37.75149</td>\n",
       "      <td>-122.39490</td>\n",
       "      <td>3.449997</td>\n",
       "      <td>63.0</td>\n",
       "      <td>9.452065e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>37.75148</td>\n",
       "      <td>-122.39497</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:23:51</td>\n",
       "      <td>37.75151</td>\n",
       "      <td>-122.39496</td>\n",
       "      <td>3.449761</td>\n",
       "      <td>61.0</td>\n",
       "      <td>9.761301e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>37.75124</td>\n",
       "      <td>-122.39439</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-05-22 20:25:52</td>\n",
       "      <td>37.75147</td>\n",
       "      <td>-122.39494</td>\n",
       "      <td>54.702146</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.547829e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lat       long  occupancy                time  lat_prev  long_prev  \\\n",
       "1218  37.75200 -122.39424          0 2008-05-22 16:51:31  37.75190 -122.39408   \n",
       "1219  37.75133 -122.39533          0 2008-05-22 16:52:31  37.75200 -122.39424   \n",
       "1220  37.75136 -122.39545          0 2008-05-22 16:53:31  37.75133 -122.39533   \n",
       "1221  37.75121 -122.39542          0 2008-05-22 16:54:31  37.75136 -122.39545   \n",
       "1222  37.74989 -122.39581          0 2008-05-22 16:55:31  37.75121 -122.39542   \n",
       "1225  37.74977 -122.39696          0 2008-05-22 16:58:32  37.74989 -122.39584   \n",
       "1226  37.74948 -122.40114          0 2008-05-22 16:59:18  37.74977 -122.39696   \n",
       "1227  37.74836 -122.40856          0 2008-05-22 17:00:19  37.74948 -122.40114   \n",
       "1228  37.74908 -122.41376          0 2008-05-22 17:01:48  37.74836 -122.40856   \n",
       "1229  37.75216 -122.41411          0 2008-05-22 17:02:42  37.74908 -122.41376   \n",
       "1231  37.75223 -122.41424          0 2008-05-22 17:04:42  37.75216 -122.41410   \n",
       "1232  37.75233 -122.41415          0 2008-05-22 17:05:42  37.75223 -122.41424   \n",
       "1233  37.75231 -122.41417          0 2008-05-22 17:06:42  37.75233 -122.41415   \n",
       "1234  37.75212 -122.41405          1 2008-05-22 17:07:11  37.75231 -122.41417   \n",
       "1235  37.75630 -122.41440          1 2008-05-22 17:08:10  37.75212 -122.41405   \n",
       "1236  37.76159 -122.41496          1 2008-05-22 17:09:08  37.75630 -122.41440   \n",
       "1237  37.76531 -122.41510          1 2008-05-22 17:10:09  37.76159 -122.41496   \n",
       "1238  37.76570 -122.41083          1 2008-05-22 17:11:09  37.76531 -122.41510   \n",
       "1239  37.76576 -122.40820          1 2008-05-22 17:11:34  37.76570 -122.41083   \n",
       "1240  37.76581 -122.40676          1 2008-05-22 17:12:35  37.76576 -122.40820   \n",
       "1241  37.76720 -122.40608          1 2008-05-22 17:13:31  37.76581 -122.40676   \n",
       "1243  37.76857 -122.40779          0 2008-05-22 17:14:36  37.76720 -122.40608   \n",
       "1244  37.77121 -122.40556          0 2008-05-22 17:15:34  37.76857 -122.40779   \n",
       "1245  37.77303 -122.40326          0 2008-05-22 17:16:36  37.77121 -122.40556   \n",
       "1246  37.77522 -122.40615          0 2008-05-22 17:17:34  37.77303 -122.40326   \n",
       "1247  37.77742 -122.40877          0 2008-05-22 17:18:28  37.77522 -122.40615   \n",
       "1248  37.77915 -122.41088          0 2008-05-22 17:19:22  37.77742 -122.40877   \n",
       "1249  37.78059 -122.41277          1 2008-05-22 17:20:16  37.77915 -122.41088   \n",
       "1250  37.78320 -122.41406          1 2008-05-22 17:21:16  37.78059 -122.41277   \n",
       "1251  37.78497 -122.41455          1 2008-05-22 17:22:11  37.78320 -122.41406   \n",
       "...        ...        ...        ...                 ...       ...        ...   \n",
       "1390  37.75149 -122.39496          0 2008-05-22 19:35:21  37.75155 -122.39492   \n",
       "1393  37.75156 -122.39496          0 2008-05-22 19:38:22  37.75150 -122.39497   \n",
       "1396  37.75152 -122.39500          0 2008-05-22 19:41:25  37.75154 -122.39495   \n",
       "1398  37.75147 -122.39501          0 2008-05-22 19:43:26  37.75151 -122.39501   \n",
       "1400  37.75157 -122.39501          0 2008-05-22 19:45:27  37.75150 -122.39502   \n",
       "1402  37.75157 -122.39498          0 2008-05-22 19:47:29  37.75156 -122.39498   \n",
       "1404  37.75166 -122.39507          0 2008-05-22 19:49:30  37.75156 -122.39491   \n",
       "1405  37.75153 -122.39498          0 2008-05-22 19:50:30  37.75166 -122.39507   \n",
       "1406  37.75160 -122.39504          0 2008-05-22 19:51:35  37.75153 -122.39498   \n",
       "1408  37.75173 -122.39508          0 2008-05-22 19:53:35  37.75160 -122.39500   \n",
       "1409  37.75166 -122.39504          0 2008-05-22 19:54:35  37.75173 -122.39508   \n",
       "1410  37.75155 -122.39510          0 2008-05-22 19:55:35  37.75166 -122.39504   \n",
       "1411  37.75164 -122.39506          0 2008-05-22 19:56:35  37.75155 -122.39510   \n",
       "1412  37.75167 -122.39506          0 2008-05-22 19:57:35  37.75164 -122.39506   \n",
       "1413  37.75180 -122.39514          0 2008-05-22 19:58:36  37.75167 -122.39506   \n",
       "1415  37.75166 -122.39506          0 2008-05-22 20:00:37  37.75178 -122.39511   \n",
       "1416  37.75158 -122.39502          0 2008-05-22 20:01:38  37.75166 -122.39506   \n",
       "1417  37.75168 -122.39507          0 2008-05-22 20:02:40  37.75158 -122.39502   \n",
       "1418  37.75161 -122.39512          0 2008-05-22 20:03:39  37.75168 -122.39507   \n",
       "1419  37.75138 -122.39502          0 2008-05-22 20:04:41  37.75161 -122.39512   \n",
       "1420  37.75154 -122.39500          0 2008-05-22 20:05:44  37.75138 -122.39502   \n",
       "1423  37.75154 -122.39500          0 2008-05-22 20:08:43  37.75151 -122.39501   \n",
       "1425  37.75156 -122.39505          0 2008-05-22 20:10:45  37.75156 -122.39500   \n",
       "1428  37.75157 -122.39509          0 2008-05-22 20:13:45  37.75154 -122.39510   \n",
       "1430  37.75158 -122.39495          0 2008-05-22 20:15:46  37.75155 -122.39502   \n",
       "1431  37.75156 -122.39495          0 2008-05-22 20:16:46  37.75158 -122.39495   \n",
       "1433  37.75151 -122.39492          0 2008-05-22 20:18:47  37.75154 -122.39490   \n",
       "1436  37.75151 -122.39493          0 2008-05-22 20:21:52  37.75149 -122.39490   \n",
       "1438  37.75148 -122.39497          0 2008-05-22 20:23:51  37.75151 -122.39496   \n",
       "1440  37.75124 -122.39439          0 2008-05-22 20:25:52  37.75147 -122.39494   \n",
       "\n",
       "      dist_from_prev_m  time_delta_sec     speed_mph  \n",
       "1218         17.931029          1130.0  2.738894e-09  \n",
       "1219        121.383655            60.0  3.491865e-07  \n",
       "1220         11.065101            60.0  3.183117e-08  \n",
       "1221         16.886498            60.0  4.857769e-08  \n",
       "1222        150.729224            60.0  4.336054e-07  \n",
       "1225         99.371313            60.0  2.858632e-07  \n",
       "1226        368.922254            46.0  1.384285e-06  \n",
       "1227        664.161587            61.0  1.879284e-06  \n",
       "1228        464.152218            89.0  9.001583e-07  \n",
       "1229        343.860031            54.0  1.099098e-06  \n",
       "1231         14.563138            60.0  4.189404e-08  \n",
       "1232         13.647440            60.0  3.925983e-08  \n",
       "1233          2.835057            60.0  8.155658e-09  \n",
       "1234         23.614776            29.0  1.405511e-07  \n",
       "1235        465.812218            59.0  1.362722e-06  \n",
       "1236        590.277639            58.0  1.756616e-06  \n",
       "1237        413.828159            61.0  1.170951e-06  \n",
       "1238        377.839470            60.0  1.086937e-06  \n",
       "1239        231.278447            25.0  1.596775e-06  \n",
       "1240        126.700794            61.0  3.585073e-07  \n",
       "1241        165.716217            56.0  5.107701e-07  \n",
       "1243        214.007176            61.0  6.055458e-07  \n",
       "1244        352.979286            58.0  1.050436e-06  \n",
       "1245        286.047251            62.0  7.963327e-07  \n",
       "1246        351.881336            58.0  1.047169e-06  \n",
       "1247        335.957783            54.0  1.073840e-06  \n",
       "1248        267.196117            54.0  8.540531e-07  \n",
       "1249        230.713745            54.0  7.374425e-07  \n",
       "1250        311.575679            60.0  8.963152e-07  \n",
       "1251        201.470657            55.0  6.322627e-07  \n",
       "...                ...             ...           ...  \n",
       "1390          7.541822            60.0  2.169569e-08  \n",
       "1393          6.729375            61.0  1.904116e-08  \n",
       "1396          4.926463            60.0  1.417204e-08  \n",
       "1398          4.447797            61.0  1.258530e-08  \n",
       "1400          7.833141            61.0  2.216433e-08  \n",
       "1402          1.111949            59.0  3.252980e-09  \n",
       "1404         17.931080            61.0  5.073704e-08  \n",
       "1405         16.479306            60.0  4.740631e-08  \n",
       "1406          9.402771            65.0  2.496842e-08  \n",
       "1408         16.075662            64.0  4.335482e-08  \n",
       "1409          8.541231            60.0  2.457071e-08  \n",
       "1410         13.320478            60.0  3.831925e-08  \n",
       "1411         10.607472            60.0  3.051470e-08  \n",
       "1412          3.335848            60.0  9.596292e-09  \n",
       "1413         16.075660            61.0  4.548702e-08  \n",
       "1415         14.048854            61.0  3.975205e-08  \n",
       "1416          9.565517            61.0  2.706619e-08  \n",
       "1417         11.956897            62.0  3.328705e-08  \n",
       "1418          8.939205            59.0  2.615142e-08  \n",
       "1419         27.043842            62.0  7.528790e-08  \n",
       "1420         17.877871            63.0  4.898056e-08  \n",
       "1423          3.449761            61.0  9.761301e-09  \n",
       "1425          4.395941            62.0  1.223795e-08  \n",
       "1428          3.449761            60.0  9.923989e-09  \n",
       "1430          7.000250            58.0  2.083215e-08  \n",
       "1431          2.223899            60.0  6.397528e-09  \n",
       "1433          3.770911            60.0  1.084785e-08  \n",
       "1436          3.449997            63.0  9.452065e-09  \n",
       "1438          3.449761            61.0  9.761301e-09  \n",
       "1440         54.702146            61.0  1.547829e-07  \n",
       "\n",
       "[167 rows x 9 columns]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 9)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('0 days 03:34:21')"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How long was this trip?\n",
    "trip_df.time.max() - trip_df.time.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = trip_df[['lat', 'long', 'lat_prev', 'long_prev', 'dist_from_prev_m', 'time_delta_sec']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 9)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_df.shape\n",
    "# (9846421, 9) ... this means I'd need 10 million iterations... @.@\n",
    "# Wait a sec, how do I distinguish among trips???\n",
    "# The first trace: (19500, 9)\n",
    "# Second trace: (2504, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 6)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On iteration 1, fit quality of MAX -0.59, MEAN -0.69, MEDIAN -0.59\n",
      "On iteration 2, fit quality of MAX -5.13, MEAN -inf, MEDIAN -589.82\n",
      "On iteration 3, fit quality of MAX -280.69, MEAN -inf, MEDIAN -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/LaCie/anaconda/envs/map_matching_particle_filter/lib/python3.7/site-packages/ipykernel/__main__.py:159: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On iteration 4, fit quality of MAX -510.27, MEAN -inf, MEDIAN -inf\n",
      "On iteration 5, fit quality of MAX -1131.12, MEAN -inf, MEDIAN -inf\n",
      "On iteration 6, fit quality of MAX -1427.62, MEAN -inf, MEDIAN -inf\n",
      "On iteration 7, fit quality of MAX -1594.29, MEAN -inf, MEDIAN -inf\n",
      "On iteration 8, fit quality of MAX -1931.27, MEAN -inf, MEDIAN -inf\n",
      "On iteration 9, fit quality of MAX -2354.81, MEAN -inf, MEDIAN -inf\n",
      "On iteration 10, fit quality of MAX -2404.55, MEAN -inf, MEDIAN -inf\n",
      "On iteration 11, fit quality of MAX -2436.96, MEAN -inf, MEDIAN -inf\n",
      "On iteration 12, fit quality of MAX -3007.87, MEAN -inf, MEDIAN -inf\n",
      "On iteration 13, fit quality of MAX -3246.80, MEAN -inf, MEDIAN -inf\n",
      "On iteration 14, fit quality of MAX -3527.16, MEAN -inf, MEDIAN -inf\n",
      "On iteration 15, fit quality of MAX -3844.56, MEAN -inf, MEDIAN -inf\n",
      "On iteration 16, fit quality of MAX -3942.89, MEAN -inf, MEDIAN -inf\n",
      "On iteration 17, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 18, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 19, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 20, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 21, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 22, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 23, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 24, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 25, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 26, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 27, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 28, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 29, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 30, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 31, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 32, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 33, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 34, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 35, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 36, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 37, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 38, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 39, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 40, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 41, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 42, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 43, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 44, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 45, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 46, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 47, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 48, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 49, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 50, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 51, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 52, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 53, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 54, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 55, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 56, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 57, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 58, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 59, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 60, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 61, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 62, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 63, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 64, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 65, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 66, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 67, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 68, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 69, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 70, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 71, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 72, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 73, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 74, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 75, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 76, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 77, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 78, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 79, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 80, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 81, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 82, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 83, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 84, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 85, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 86, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 87, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 88, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 89, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 90, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 91, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 92, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 93, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 94, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 95, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 96, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 97, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 98, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 99, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 100, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 101, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 102, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 103, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 104, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 105, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 106, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 107, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 108, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 109, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 110, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 111, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 112, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 113, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 114, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 115, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 116, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 117, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 118, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 119, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 120, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 121, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 122, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 123, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 124, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 125, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 126, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 127, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 128, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 129, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 130, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 131, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 132, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 133, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 134, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 135, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 136, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 137, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 138, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 139, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 140, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 141, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 142, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 143, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 144, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 145, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 146, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 147, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 148, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 149, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 150, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 151, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 152, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 153, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 154, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 155, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 156, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 157, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 158, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 159, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 160, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 161, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 162, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 163, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 164, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 165, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 166, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "On iteration 167, fit quality of MAX -inf, MEAN -inf, MEDIAN -inf\n",
      "Done.\n",
      "CPU times: user 14.3 s, sys: 180 ms, total: 14.5 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first -inf results in mean and median, not max, then in all...\n",
    "# Runtime warning, divide by zero in log\n",
    "# Error at iter 1381; 3 min\n",
    "\"\"\"\n",
    "For the first cabspotting trace, new_abboip.txt, I see why my particle filter loses the trail before it ends - \n",
    "my shapefile only covers San Francisco but the person \n",
    "drives into the East Bay. Ditto for new_abcoij.txt.\n",
    "\n",
    "Oh I see. These traces are over multiple trips and days for one cab... Need to separate them into trips first. \n",
    "\n",
    "I always have to reinstantiate the particle filter to get rid of info in viterbi_trellis! WHy???\n",
    "\"\"\"\n",
    "results = my_particle_filter.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 430 µs, sys: 16 µs, total: 446 µs\n",
      "Wall time: 438 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "backtracked_trace = my_particle_filter.viterbi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(backtracked_trace)  # actually longer than actual trace for 2nd trace, new_abcoij.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the smoothed trace, cast as linestring overlaid on SF shapefile\n",
    "# And, compare it with the raw trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-122.5040816164394 37.744000160845644 0.11614963468083772 0.06164985881586915\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,75.54965018050716)\"><polyline fill=\"none\" stroke=\"#66cc99\" stroke-width=\"0.0023229926936167546\" points=\"-122.39424183421846,37.75202646291258 -122.39501277739464,37.751357774477 -122.395261,37.751241 -122.3951949,37.7513041 -122.3957776,37.7497851 -122.3967389,37.7499267 -122.40113905700454,37.74946416404269 -122.40855900592723,37.748301999167154 -122.41377416746913,37.74907857783532 -122.41406973636946,37.75216385066837 -122.41407754848635,37.752245536276696 -122.41408619190325,37.75233608122016 -122.41408448298887,37.75231815018466 -122.41406540178012,37.75211852702931 -122.41445969991037,37.75629388801332 -122.4152201,37.761504 -122.41510173827116,37.76533994134514 -122.4110151,37.7657825 -122.40819913530147,37.76574558190494 -122.40741081111804,37.765749183912526 -122.4057234,37.7672005 -122.4078316,37.7685744 -122.4054557,37.7711581 -122.40325737124216,37.77302667452941 -122.4061545419663,37.77522367483831 -122.40873898231703,37.77745891680026 -122.41087196848714,37.77916005036309 -122.41273599737805,37.78053184901916 -122.41414682982541,37.78318248481073 -122.41450923508802,37.784978229419714 -122.41551492898218,37.78996073050396 -122.41628211170841,37.79376140222859 -122.41683498540588,37.79670951098444 -122.41753251615185,37.7999636068475 -122.41346453253405,37.80134818134 -122.4128714613288,37.800941737179336 -122.40958993789543,37.80000481171884 -122.4061077,37.7975935 -122.4047128381157,37.79650137622392 -122.40324905676084,37.79506257464437 -122.4029757,37.7937875 -122.40374286539071,37.79176235738494 -122.41232828338481,37.79087840118696 -122.4200458420247,37.78968828440788 -122.422001121402,37.7895814416879 -122.4220322386871,37.78952338632846 -122.42179051970932,37.78946671618315 -122.42209645116719,37.790054811655665 -122.42255637905338,37.791034584601064 -122.42241855804154,37.79112634185936 -122.4241848,37.7910959 -122.42900736821991,37.78995183082685 -122.43310484695508,37.78900710058993 -122.43518507902883,37.78874105103668 -122.43911901428002,37.78824209240822 -122.44454554506083,37.787565079025164 -122.44996387244328,37.78686301164412 -122.4542504,37.7863181 -122.45649516493094,37.78603201595656 -122.46191640601357,37.78534177489413 -122.46824784328625,37.784652423072906 -122.47229129592877,37.784517943487955 -122.47814739759062,37.784202721665956 -122.48133944023064,37.784057694846005 -122.48473924186791,37.78390331376939 -122.48960964068259,37.783682086107845 -122.49264405105686,37.781703080198675 -122.49431878146407,37.78159321795325 -122.49967263149682,37.78505332560116 -122.49977977811788,37.78515119126087 -122.49493679523377,37.78225144089034 -122.49441294422337,37.77994050870477 -122.49346945062463,37.77964764307932 -122.48795102646942,37.77977702337788 -122.48356520125454,37.778137771387215 -122.48430881026388,37.776313878682316 -122.49343724748633,37.77589956314762 -122.4944256576617,37.77585458752537 -122.49546186642522,37.779575969182304 -122.4912121560941,37.78023407016198 -122.4842334,37.7801854 -122.4772497,37.7803775 -122.47381112434857,37.78053464563249 -122.4710936,37.7806587 -122.46771963581904,37.78081195540549 -122.46144848393071,37.781095418673 -122.457248951581,37.78128687429624 -122.45112911482299,37.7819599617484 -122.44815896308565,37.78233108817666 -122.44768649983759,37.78237870671084 -122.44746576271386,37.78513320327415 -122.44152014711642,37.78600115991954 -122.43430735278935,37.78691928269914 -122.43132830449385,37.787296799204995 -122.4253466,37.788534 -122.42364144400717,37.78875167518991 -122.42432818961863,37.791816397187326 -122.4236091,37.7956232 -122.41870788283387,37.79618304808658 -122.40880433327159,37.797518390294265 -122.407688,37.797132 -122.407098,37.7977321 -122.40768460274386,37.7976582541269 -122.407098,37.7977321 -122.40547154577277,37.7970809560114 -122.40423030160744,37.796103442904744 -122.4032339,37.7950645 -122.4003550084857,37.79499332530272 -122.40126661823322,37.79413329504497 -122.4000662,37.793111 -122.3962895,37.7887506 -122.39814964271844,37.78682326555643 -122.39396865572705,37.78349050106551 -122.39416003849693,37.78021740067235 -122.4004173016675,37.77528353374124 -122.39603868447028,37.76750938202407 -122.39223382008007,37.756466910479666 -122.39252848281419,37.750501367116335 -122.393509,37.7521861 -122.3950812856797,37.75187210802697 -122.3951949,37.7513041 -122.39528478593112,37.75116631372343 -122.39534356312221,37.751028421821516 -122.3951949,37.7513041 -122.39484255129352,37.75132669173021 -122.39502616720888,37.75151070451269 -122.39502805714223,37.75153229014254 -122.39487484519303,37.751539565446514 -122.39502624328476,37.75151157340349 -122.39502631936064,37.751512442294285 -122.39487548356658,37.751546683196395 -122.39487208428503,37.75150878182064 -122.39487710341066,37.75156474416553 -122.39487046444094,37.7514907208515 -122.39487969277017,37.75159361505671 -122.3950281332181,37.751533159033336 -122.39487419486437,37.751532314399405 -122.39502386484,37.75148440830351 -122.39487734280075,37.75156741332174 -122.3950267758159,37.75151765563907 -122.39502250743777,37.751468904909245 -122.39487863150296,37.75158178211873 -122.39503096811812,37.7515655374781 -122.39488711775175,37.7516764022609 -122.39502749255493,37.75152584178159 -122.39503403124577,37.751600522595254 -122.39504563112963,37.75173300917206 -122.39503924459055,37.75166006614 -122.39503014324704,37.75155611631941 -122.3950376589607,37.75164195607335 -122.39488792767379,37.75168543274547 -122.39505216982046,37.75180768998571 -122.3950393967423,37.75166180392159 -122.39522810791411,37.751552138022795 -122.39504121059977,37.75168252066064 -122.39503550874358,37.75161739764575 -122.39501727234183,37.75131884885274 -122.39502851359748,37.75153750348732 -122.39502851359748,37.75153750348732 -122.39503063175846,37.75156169578955 -122.39487926987651,37.751588899868615 -122.3950316087813,37.75157285472982 -122.39487726300405,37.751566523603 -122.39487257502029,37.751514253430265 -122.394872654817,37.751515143149 -122.39502307202507,37.75147535327018 -122.3945132,37.7513415\" opacity=\"0.8\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.linestring.LineString at 0x1a2bb68e80>"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LineString(np.array(backtracked_trace)[:, [1,0]])\n",
    "# first 1381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-122.50407999999999 37.74407 0.11582999999998833 0.06114999999999782\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,75.54929)\"><polyline fill=\"none\" stroke=\"#66cc99\" stroke-width=\"0.0023165999999997665\" points=\"-122.39424,37.751999999999995 -122.39533,37.751329999999996 -122.39545,37.75136 -122.39542,37.75121 -122.39581000000001,37.74989 -122.39696,37.74977 -122.40113999999998,37.74948 -122.40856000000001,37.74836 -122.41376000000001,37.74908 -122.41411000000001,37.752159999999996 -122.41423999999999,37.75223 -122.41415,37.75233 -122.41417,37.752309999999994 -122.41405,37.75212 -122.4144,37.7563 -122.41496000000001,37.761590000000005 -122.4151,37.76531 -122.41083,37.7657 -122.4082,37.76576 -122.40676,37.765809999999995 -122.40608,37.7672 -122.40778999999999,37.768570000000004 -122.40556000000001,37.771209999999996 -122.40326,37.77303 -122.40615,37.775220000000004 -122.40876999999999,37.77742 -122.41088,37.77915 -122.41277,37.780590000000004 -122.41406,37.7832 -122.41455,37.78497 -122.41537,37.78999 -122.41618999999999,37.79378 -122.41685,37.79683 -122.4176,37.79995 -122.4134,37.80084 -122.41287,37.80093 -122.40969,37.799859999999995 -122.40636,37.79762 -122.4047,37.79652 -122.40325,37.79507 -122.40298999999999,37.793490000000006 -122.40373999999998,37.791740000000004 -122.41237,37.79087 -122.42006,37.7898 -122.42176,37.789629999999995 -122.42206999999999,37.78982 -122.42181000000001,37.78962 -122.42217,37.790040000000005 -122.42248000000001,37.79105 -122.42245,37.79112 -122.42418,37.7913 -122.42862,37.79003 -122.4331,37.78897 -122.43518999999999,37.788779999999996 -122.43911999999999,37.78825 -122.44455,37.7876 -122.44997,37.78691 -122.45445,37.78631 -122.4565,37.78607 -122.46191999999999,37.78537 -122.46825,37.7847 -122.47235,37.78452 -122.47815,37.784259999999996 -122.48133999999999,37.78407 -122.48473999999999,37.78392 -122.48961000000001,37.78369 -122.49269,37.7817 -122.49432,37.781620000000004 -122.49963000000001,37.7851 -122.49978999999999,37.785140000000006 -122.49501000000001,37.78215 -122.49441999999999,37.77994 -122.49346000000001,37.77944 -122.48791000000001,37.77978 -122.48356000000001,37.77798 -122.48431000000001,37.776340000000005 -122.49343999999999,37.77596 -122.49443000000001,37.77595 -122.49541,37.779579999999996 -122.49113,37.78024 -122.48422,37.78001 -122.4774,37.78035 -122.47381000000001,37.78051 -122.47128000000001,37.78063 -122.46772,37.78082 -122.46145,37.78113 -122.45725,37.78131 -122.45103,37.78198 -122.44815,37.78226 -122.44769,37.78243 -122.44743000000001,37.78498 -122.44152,37.786 -122.43431000000001,37.78694 -122.43133,37.78731 -122.42546000000002,37.788540000000005 -122.42363999999999,37.788740000000004 -122.42431,37.79182 -122.42374,37.79552 -122.4187,37.79612 -122.4088,37.79748 -122.40785,37.79761 -122.40777,37.79777 -122.40771000000001,37.79786 -122.40721,37.7976 -122.40546,37.79699 -122.40421,37.796079999999996 -122.40321999999999,37.79517 -122.40032,37.79483 -122.40093999999999,37.7942 -122.39986999999999,37.79292 -122.39634,37.78889 -122.3982,37.78676 -122.39381000000002,37.78369 -122.39416999999999,37.780229999999996 -122.40043,37.7753 -122.39607,37.76747 -122.39254,37.75643 -122.39254,37.7505 -122.3939,37.75206 -122.39508000000001,37.751870000000004 -122.39522,37.75122 -122.39517,37.751090000000005 -122.39536000000001,37.75103 -122.39527,37.75128 -122.39483999999999,37.75127 -122.39492,37.75152 -122.39493999999999,37.751540000000006 -122.39487,37.751540000000006 -122.39493,37.75152 -122.39493999999999,37.75152 -122.39495,37.751540000000006 -122.39497,37.7515 -122.39493,37.75156 -122.39498999999999,37.75148 -122.39492,37.75159 -122.39495,37.751540000000006 -122.3949,37.751529999999995 -122.39496000000001,37.751490000000004 -122.39496000000001,37.75156 -122.395,37.75152 -122.39501000000001,37.75147 -122.39501000000001,37.75157 -122.39498,37.75157 -122.39506999999999,37.75166 -122.39498,37.751529999999995 -122.39504,37.7516 -122.39508000000001,37.75173 -122.39504,37.75166 -122.3951,37.75155 -122.39506000000002,37.75164 -122.39506000000002,37.751670000000004 -122.39513999999998,37.7518 -122.39506000000002,37.75166 -122.39502,37.75158 -122.39506999999999,37.75168 -122.39511999999999,37.75161 -122.39502,37.75138 -122.395,37.751540000000006 -122.395,37.751540000000006 -122.39505,37.75156 -122.39509,37.75157 -122.39495,37.75158 -122.39495,37.75156 -122.39492,37.751509999999996 -122.39493,37.751509999999996 -122.39497,37.75148 -122.39438999999999,37.75124\" opacity=\"0.8\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.linestring.LineString at 0x1a2eb96dd8>"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LineString(trip_df[['long', 'lat']].values)\n",
    "# 19500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace_bounds = LineString(trip_df[['long', 'lat']].values).bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-122.49978999999999, 37.74836, -122.39254, 37.80093)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/LaCie/anaconda/envs/map_matching_particle_filter/lib/python3.7/site-packages/ipykernel/__main__.py:5: MatplotlibDeprecationWarning: \n",
      "The dedent function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use inspect.cleandoc instead.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "map = Basemap(llcrnrlon=trace_bounds[0], llcrnrlat=trace_bounds[1], \n",
    "              urcrnrlon=trace_bounds[2], urcrnrlat=trace_bounds[3],\n",
    "              resolution='i'\n",
    "#               , projection='tmerc'\n",
    "             )\n",
    "\n",
    "map.drawmapboundary(fill_color='aqua')\n",
    "map.fillcontinents(color='#ddaa66', lake_color='aqua')\n",
    "map.drawcoastlines()\n",
    "\n",
    "# map.readshapefile(basemap_dir[:-4], 'roads')\n",
    "\n",
    "roads_info = map.readshapefile(basemap_dir[:-4], 'roads')\n",
    "\n",
    "# print(roads_info)\n",
    "\n",
    "# plot the full trace\n",
    "map.plot(trip_df.long.values, trip_df.lat.values, marker=\".\", color='blue')\n",
    "\n",
    "# overlay the backtracked trace\n",
    "map.plot(np.array(backtracked_trace)[:, [1]], np.array(backtracked_trace)[:, [0]], marker=\".\", color='red')\n",
    "\n",
    "plt.savefig(\"_\".join((all_files[1][:-4], str(trip_idx))) + \".png\")\n",
    "\n",
    "# Seeing some corrections, but it doesn't fill in the gaps between missing measurements, \n",
    "# where we still see bird-flight paths from a line view. - I'm only plotting the nearest real road to observed points\n",
    "# This algorithm is not meant to fill in gaps between measurements..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  IGNORE BELOW THIS POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/4102520/how-to-transform-a-distance-from-degrees-to-metres\n",
    "\"\"\"\n",
    "@winwaed\n",
    "The transformation between degrees and metres varies across the Earth's surface.\n",
    "Assuming a spherical Earth, degrees latitude = distance * 360 / (2*PI * 6400000)\n",
    "Note that longitude will vary according to the latitude:\n",
    "Degrees longitude = distance *360 * / (2*PI* cos(latitude) )\n",
    "\n",
    "The above is for the Earth's surface, and does not use the Mercator projection. \n",
    "If you wish to work with projected linear distance, then you will need to use the Mercator projection.\n",
    "\"\"\"\n",
    "# RADIUS_OF_EARTH_M = 6371000\n",
    "# long, lat = list(obs.coords)[0]\n",
    "# max_dist_deg = max_dist * 360 / (2 * np.pi * RADIUS_OF_EARTH_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:map_matching_particle_filter]",
   "language": "python",
   "name": "conda-env-map_matching_particle_filter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
