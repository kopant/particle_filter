{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import glob, re, time\n",
    "\n",
    "import shapely\n",
    "import folium\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/LaCie/Documents/repos/particle_filter'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applications of particle filters: \n",
    "1. Car positioning by map matching, as in http://www.diva-portal.org/smash/get/diva2:316556/FULLTEXT01.pdf\n",
    "    Essentially same approach is described in Davidson, Collin, and Takala (2011). Application of particle filters to map-matching algorithm\n",
    "    Idea also similar to Newson and Krumm (2009), though HMM is used there.\n",
    "    More recent resource: Murphy, Pao, Yuen (2019). Lyft; Map matching when the map is wrong: Efficient on/off road vehicle tracking and map learning\n",
    "\n",
    "Ideas\n",
    "Rao-Blackwellization (use Kalman filter for the linear part of the dynamics model)\n",
    "\n",
    "Initial Approach:\n",
    "Use Newson and Krumm, but modify it to use particle filters instead of HMM.\n",
    "\n",
    "So, for a given route, proceed sequentially over obs, maintaining dist of probable road segments\n",
    "\n",
    "Assumptions (N&K):\n",
    "-remove obs that are not 2*meas dist sigma from previous obs  (eliminate 39% of data in N&K)\n",
    "-ignore roads 200m from obs\n",
    "-zeroize very unlikely particles\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "# Assumes (lat, long) in radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def great_circle_dist(a, b):\n",
    "    \"\"\"\n",
    "    NOT USED\n",
    "    \n",
    "    Returns great-circle distance between point a and point b\n",
    "    a, (lat, long) in radians\n",
    "    b, (lat, long) in radians\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gps_transition_model:\n",
    "    \"\"\"\n",
    "    Transition model to predict probability of next state given current state; simulate next state.\n",
    "    GPS trace map matching use case. \n",
    "    \n",
    "    N&K use as a proxy an exponential function of the difference in the great-circle distance \n",
    "    between previous observations and previous road points (similar route distance).\n",
    "    Road points use map/ground-truth data. \n",
    "    -Ignore roads 200m from obs\n",
    "    -If a calculated route would require the vehicle to exceed a speed of 50 m/s (112 miles per hour), zeroize\n",
    "    \n",
    "    !!! TODO: Get driving distance from route planner??? Results anticipated to significantly suffer otherwise. !!! \n",
    "    \n",
    "    Other ideas: using dead reckoning.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta=3.0\n",
    "#                  , prob_floor=0.05\n",
    "                 , normalize=True\n",
    "                ):\n",
    "        \"\"\"\n",
    "        beta, float, parametrizes transition probability function\n",
    "        prob_floor, float between 0 and 1, is probability below which road choices are given zero probability\n",
    "        \n",
    "        # beta = 3 in mapzen, https://www.mapzen.com/blog/data-driven-map-matching/\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "#         self.prob_floor = min(max(prob_floor, 0), 1)\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def estimate_beta(self, data):\n",
    "        \"\"\"\n",
    "        NOT USED\n",
    "        \n",
    "        Using rescaled median absolute deviation (MAD). \n",
    "        Ideally using ground truth data (none here).\n",
    "        \"\"\" \n",
    "        self.beta = 3.0  # None\n",
    "        \n",
    "    def prob_transition(self, state_last2, obs_last2):\n",
    "        \"\"\"\n",
    "        state_last2, float numpy array, (k x 4 lat long) in degrees, k candidate roads, t t-1 (pre same for all)\n",
    "        obs_last2, float numpy array, (1 x 4 lat long) in degrees, t t-1\n",
    "        \n",
    "        returns probs of transition, (k x 1)\n",
    "        \"\"\"\n",
    "        state_last2 = np.deg2rad(state_last2)\n",
    "        obs_last2 = np.deg2rad(obs_last2)\n",
    "        dist_obs = abs(haversine_distances(obs_last2[:, :2], obs_last2[0, 2:]))  # (k, 1)\n",
    "        # !! TODO - Supposed to be driving distance, not haversine. N&K use a route planner. Can I use shapely???\n",
    "        dist_road = np.abs(haversine_distances(state_last2[:, :2], state_last2[:, 2:]))  # (1, 1)\n",
    "        diff_dist = np.abs(np.subtract(dist_road, dist_obs))  # (k, 1)\n",
    "\n",
    "        probs = np.exp(-diff_dist / self.beta) * (1 / self.beta)\n",
    "        # Normalize result?\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)\n",
    "        return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gps_sensor_model:\n",
    "    \"\"\"\n",
    "    Sensor model to predict likelihood of an obs given a particle.\n",
    "    GPS trace map matching use case. \n",
    "    \n",
    "    I don't have ground truth, so can't use ML easily, but\n",
    "    N&K use as a proxy a normal distribution of great-circle distance between observation and road, with sigma \n",
    "    estimated from the data.\n",
    "    Road points use map data. \n",
    "    -Zerioze low probability particles (diff in route distance of 2000 m. or more). \n",
    "    \n",
    "    Other ideas: semi-supervised learning. \n",
    "    \n",
    "    Thoughts: need efficient representation for road network that returns connecting roads/nodes for a given node\n",
    "    Want to represent it as a graph/network. \n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=4.07, normalize=True):\n",
    "        self.sigma = sigma\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def estimate_sigma(self, data):\n",
    "        \"\"\"\n",
    "        NOT USED \n",
    "        \n",
    "        Using Gather and Schultze, median-based. \n",
    "        Ideally using ground truth data (none here).\n",
    "        \n",
    "        I guess I could manually match some trips, then use that??\n",
    "        \"\"\" \n",
    "        self.sigma = 4.07 # meters; I have no ground truth so using N&K's empirical estimate\n",
    "#         self.sigma = None # median absolute deviation (MAD) formula adjusted \n",
    "    \n",
    "    def estimate_likelihood(self, states, obs):\n",
    "        \"\"\"\n",
    "        return probability of seeing that obs given potential road state hypotheses, (k x 1)\n",
    "        \n",
    "        states Particles numpy array representing road position hypotheses, (k x 2 lat long).\n",
    "        obs (1 x 2 lat long)\n",
    "        \"\"\"\n",
    "        dist_obs_roads = np.abs(haversine_distances(states, obs))  # (k x 1)\n",
    "        \n",
    "        probs = \\\n",
    "        np.exp(np.power(dist_obs_roads / self.sigma, 2) * (-0.5)) * \\\n",
    "        (1 / (math.sqrt(2 * math.pi) * self.sigma))\n",
    "        \n",
    "        # Normalize result?\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class particle_filter:\n",
    "    \"\"\"\n",
    "    Fit a particle filter to data given a transition and sensor model. \n",
    "    \n",
    "    n, integer, number of particles to maintain at each step in time.\n",
    "    max_road_dist_m, float, max road distance above which candidate roads aren't considered (meters). \n",
    "    prob_floor, float between 0 and 1, min probability below which candidate roads aren't considered.\n",
    "    Sensor model, , is some model object we will use to predict the likelihood.\n",
    "    Transition model, , is some model object we will use to predict next state.\n",
    "    ## max_iter, integer, the maximum iterations used in fitting the particle filter.\n",
    "    ## conv_tol, the convergence tolerance that will trigger early termination of fitting the particle filter. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=50, max_road_dist_m=200, prob_floor=0.01, \n",
    "                 sensor_model=None, transition_model=None):\n",
    "        self.n = n\n",
    "        self.max_road_dist_m = max_road_dist_m\n",
    "        self.prob_floor = min(max(prob_floor, 0), 1)\n",
    "        self.particles = np.array([])\n",
    "        self.weights = np.array([])\n",
    "        \n",
    "        self.sensor_model = sensor_model\n",
    "        self.transition_model = transition_model\n",
    "        \n",
    "        self.obs = []  # Record history of observations (lat, long)\n",
    "        \n",
    "    def get_candidate_roads(self, obs):\n",
    "        \"\"\"\n",
    "        Leverage shapely.\n",
    "        \n",
    "        Ignore roads 200m from obs.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def estimate_weights_per_particle(self, sampled_states, obs):\n",
    "        \"\"\"\n",
    "        Estimate likelihood of particle given evidence, P(evidence|particle).\n",
    "        Uses sensor model.\n",
    "        \n",
    "        Obs is single observation at time t, numpy array. \n",
    "        \"\"\"\n",
    "        probs = self.sensor_model.estimate_likelihood(sampled_states, obs)\n",
    "        return probs\n",
    "        \n",
    "    def apply_transition_model(self, candidate_roads):\n",
    "        \"\"\"\n",
    "        returns probabilities of transition, given current particles\n",
    "        numpy array of floats between 0 and 1, normalized, (n * c, 1)\n",
    "        \n",
    "        candidate_roads numpy array of lat,long closest point to road from obs (N&K)\n",
    "        \n",
    "        Zeroize very small probability candidates, but don't change shape of particles.\n",
    "        Then, if some particles are very unlikely, we cull them and resample from more likely particles. \n",
    "        \"\"\"\n",
    "        # Crossproduct for each particle, all roads\n",
    "        state_last2 = np.concatenate([np.concatenate((self.particles, \n",
    "                                                      np.tile(c, (self.n, 1))), \n",
    "                                                     axis=1) \n",
    "                                      for c in candidate_roads], axis=0)  # (n * c, 4)\n",
    "        obs_last2 = np.array(self.obs[-2:]).reshape((1, 4))\n",
    "        probs = self.transition_model.prob_transition(state_last2, obs_last2)  # (n * c, 1)\n",
    "        # Effectively, candidate roads will be possible from different original particle hypotheses\n",
    "        # Zeroise small probs, without changing array shape\n",
    "        probs[probs <= self.prob_floor] = 0.0\n",
    "        # Re-normalize\n",
    "        probs = probs / np.sum(probs)\n",
    "        return probs\n",
    "        \n",
    "    def update_dist(self, obs):\n",
    "        \"\"\"\n",
    "        Re-sample particles from transition model given re-estimated likelihood of existing particles.\n",
    "        Weighted sample with replacement. \n",
    "        \n",
    "        obs list of floats [lat, long]\n",
    "        \n",
    "        Note: if find no solutions, need to remove points in a signal break until HMM 'heals.'\n",
    "        If break > 180 sec, separate into two trips.\n",
    "        \"\"\"\n",
    "        # Update stored obs\n",
    "        self.obs.append(obs)\n",
    "        # Get candidate roads given obs\n",
    "        candidate_roads = self.get_candidate_roads(obs) # c\n",
    "        # Get transition probs\n",
    "        trans_probs = self.apply_transition_model(candidate_roads) # (n * c, 1) first n rows for first candidate, etc.\n",
    "        # Aggregate probs by candidate roads\n",
    "        trans_probs_split = np.split(trans_probs, self.n, axis=0)  # (c, n)\n",
    "        trans_probs_agg = np.sum(trans_probs_split, axis=1)  # (c, 1)\n",
    "        # Sample new states\n",
    "        sampled_states_idx = np.random.choice(range(len(candidate_roads)), \n",
    "                                              self.n, \n",
    "                                              trans_probs_agg)\n",
    "        sampled_states = candidate_roads[sampled_states_idx]  # (n, 2)\n",
    "        \n",
    "        # Get sensor probs\n",
    "        sensor_probs = self.estimate_weights_per_particle(sampled_states, obs)  # (n, 1)\n",
    "        \n",
    "        # Joint prob, for viterbi backtracking\n",
    "        best_prior_state = np.argmax(trans_probs_split, axis=1)[sampled_states_idx]  # HERE\n",
    "        joint_prob = np.multiply(\n",
    "            np.multiply(sensor_probs, np.max(trans_probs_split, axis=1)[sampled_states_idx]),  # (n, 1)\n",
    "            self.weights\n",
    "            )\n",
    "        self.weights = joint_prob\n",
    "        \n",
    "        # Sample new particles\n",
    "        new_particles = sampled_states[np.random.choice(range(len(sampled_states)), \n",
    "                                                        self.n, \n",
    "                                                        sensor_probs)]  # (n, 2)\n",
    "        self.particles = new_particles\n",
    "        \n",
    "        # Estimate current particle filter fit quality of hypotheses to data\n",
    "        fit_quality = [np.max(self.weights), np.mean(self.weights), np.median(self.weights)]\n",
    "        \n",
    "        return fit_quality\n",
    "        \n",
    "    def fit(self, data#, max_iter=100, conv_tol=0.001\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Iterate over rows in data, training particle filter. \n",
    "        Rows assumed to be sequentially ordered. \n",
    "        \n",
    "        data, numpy array (num obs, data dim)\n",
    "        \"\"\"\n",
    "        num_iter = 0\n",
    "        converged = False  # Is this relevant for particle filters?\n",
    "#         while num_iter < max_iter and not converged:\n",
    "#             num_iter += 1\n",
    "        for obs in data:\n",
    "            num_iter += 1\n",
    "            fit_quality = self.update_dist(obs)\n",
    "            # Shouldn't I use DP/Viterbi for this?? \n",
    "            # Nearest neighbor filter, hungarian algorithm?? p. 601\n",
    "            # Best at any given point in time will suffer in the beginning before particle dist has converged, ie\n",
    "            # during the burn-in period.\n",
    "            # So, seems can still use DP, but proceed backward in time. Wait, that is Viterbi :) \n",
    "                # Happily I have a finite state space. \n",
    "            print(\"On iteration %d, fit quality of MAX %3.2f, MEAN %3.2f, MEDIAN %3.2f\" % \n",
    "                  (num_iter, fit_quality[0], fit_quality[1], fit_quality[2]))\n",
    "        print(\"Done.\")\n",
    "        return fit_quality\n",
    "    \n",
    "    def viterbi(self, y_vector_arr):\n",
    "        \"\"\"\n",
    "        Use Viterbi to get optimal path via DP - \n",
    "        happily I have a finite state space. \n",
    "        This returns the imputed GPS trace that has \n",
    "        been 'snapped' to roads.\n",
    "        \n",
    "        Note, Viterbi relies on the Markov property, which\n",
    "        can apply here. \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Probability of the latent variables:\n",
    "        Most Likely Explanation; Joint probability of the entire sequence of hidden states \n",
    "        that generated an entire sequence of observations. \n",
    "        \n",
    "        y_vector_arr numpy_array corresponding to the sequences of observations.\n",
    "        y_vector_arr (num_sensors, num_sequences, max time step observed so far)\n",
    "        \n",
    "        Todo: How does this extend to multiple observation sequences? \n",
    "        It seems consider all sequences at each step when running forward through time. \n",
    "        Maybe, take the maximal previous state across the product of all sequences, such that\n",
    "        decoded sequence may not be the viterbi solution for any individual sequence. \n",
    "        Or, use the product of the sequences (going with this).\n",
    "        \n",
    "        This assumes sequences are independent, and gives equal importance/weight to each sequence.\n",
    "        Equal weight might not be the best idea if different sequences have different noise levels/\n",
    "        accuracy. \n",
    "        Todo: flexible weights, in this function and throughout class. \n",
    "        Alternatively, first PCA/ICA transform the sequences. (Perhaps a better idea? \n",
    "        Otherwise, the estimation of relative noise levels for each sequence would be manual/\n",
    "        judgmental process)\n",
    "        Still nice to have a manual over-ride. \n",
    "        \"\"\"\n",
    "        num_obs_t = y_vector_arr.shape[2]\n",
    "        if num_obs_t < 1:\n",
    "            print(\"Error: Empty observation vector in viterbi algorithm\")\n",
    "            return None\n",
    "        else:\n",
    "            # Dynamic programming \n",
    "            # Maximal prior sequence given current evidence, conditioned on current state\n",
    "            # Note: to extent to multiple sequences of different types, need to learn different B matrices!\n",
    "            alpha_prev = np.multiply(self.pi, \n",
    "                                     np.prod([self.B[s, :, y_vector_arr[s, :, 0]] \n",
    "                                              for s in range(self.num_sensors)], axis=0))  # (num_states x num_seq)\n",
    "#                                      self.B[:, y_vector_arr[:, 0]])  # (num_states x num_seq)\n",
    "            alpha_prev = np.log(np.prod(alpha_prev, axis=1).reshape(self.num_states, 1))  # aggregate across sequences\n",
    "            result = alpha_prev.copy().T  # one entry for each time t\n",
    "            for h in range(1, num_obs_t):\n",
    "                alpha_prev_new = np.prod([self.B[s, :, y_vector_arr[s, :, h]] \n",
    "                                          for s in range(self.num_sensors)], axis=(0, 1)).reshape(1, self.num_states)\n",
    "#                 alpha_prev_new = np.prod(self.B[:, y_vector_arr[:, h]], axis=1).reshape(1, self.B.shape[0])\n",
    "                alpha_prev = np.add(np.max(np.add(np.log(self.A), alpha_prev), axis=0),  # (1 x num_states)\n",
    "                                    np.log(alpha_prev_new)).T\n",
    "                result = np.vstack((result, alpha_prev.T))\n",
    "        # Backtrack and return the most likely sequence\n",
    "        most_likely_seq = np.argmax(result, axis=1).tolist()\n",
    "        return most_likely_seq\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## it seems i only have beijing data (10K) or sf taxi data (500)\n",
    "# Read in taxi data\n",
    "# Each data of different length; ideal use case for pyspark\n",
    "# Note, the OSM extract basemap data has POI info as well (https://download.bbbike.org)\n",
    "# also try uber h3 spatial index\n",
    "trace_dir = '/Volumes/LaCie/datasets/ms_taxi/taxi_log_2008_by_id/'   # MS Taxi\n",
    "basemap_dir = '/Volumes/LaCie/datasets/Beijing-shp/shape/'\n",
    "\n",
    "trace_dir = '/Volumes/LaCie/datasets/cabspottingdata/'   # CRAWDAD cabspotting\n",
    "basemap_dir = '/Volumes/LaCie/datasets/SanFrancisco-shp/shape/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.07 ms, sys: 906 Âµs, total: 2.97 ms\n",
      "Wall time: 2.31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_files = [f for f in os.listdir(trace_dir) if re.match(r'new_.*\\.txt', f)]  # glob.glob(trace_dir + \"new_*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.28 s, sys: 1.87 s, total: 10.2 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 20.9s cabspottingdata; can try spark or dask? or multiprocessing, joblib\n",
    "trace_list = []\n",
    "for file_ in all_files:\n",
    "    file_df = pd.read_csv(file_, sep=\" \", index_col=None, header=None, \n",
    "                          names=['lat', 'long', 'occupancy', 'time'])\n",
    "    trace_list.append(file_df)\n",
    "\n",
    "# concatenate all dfs into one\n",
    "trace_df = pd.concat(trace_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace_df.loc[:, [\"time\"]] = pd.to_datetime(trace_df.time, origin=\"unix\", unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat                 float64\n",
       "long                float64\n",
       "occupancy             int64\n",
       "time         datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cabspotting:\n",
    "\n",
    "latitude and longitude are in decimal degrees, \n",
    "occupancy shows if a cab has a fare (1 = occupied, 0 = free) and \n",
    "time is in UNIX epoch format\n",
    "\"\"\"\n",
    "trace_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:58:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:57:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:55:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:54:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.75015</td>\n",
       "      <td>-122.39256</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:50:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       long  occupancy                time\n",
       "0  37.75134 -122.39488          0 2008-06-10 07:58:07\n",
       "1  37.75136 -122.39527          0 2008-06-10 07:57:39\n",
       "2  37.75199 -122.39460          0 2008-06-10 07:55:40\n",
       "3  37.75080 -122.39346          0 2008-06-10 07:54:49\n",
       "4  37.75015 -122.39256          0 2008-06-10 07:50:37"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in ...shapefiles? GeoJSON? Which format is best? For parallelization may be one thing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_traces():\n",
    "    \"\"\"\n",
    "    -remove obs that are not 2*meas dist sigma from previous obs  (eliminate 39% of data in N&K)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
