{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment specified in environment.yml (environment name map_matching_particle_filter)\n",
    "# conda env create --prefix ./envs -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import glob, re, time\n",
    "\n",
    "import shapely\n",
    "import folium\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics.pairwise import haversine_distances  # Assumes (lat, long) in radians; version 0.22.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/LaCie/Documents/repos/particle_filter'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applications of particle filters: \n",
    "1. Car positioning by map matching, as in http://www.diva-portal.org/smash/get/diva2:316556/FULLTEXT01.pdf\n",
    "    Essentially same approach is described in Davidson, Collin, and Takala (2011). Application of particle filters to map-matching algorithm\n",
    "    Idea also similar to Newson and Krumm (2009), though HMM is used there.\n",
    "    More recent resource: Murphy, Pao, Yuen (2019). Lyft; Map matching when the map is wrong: Efficient on/off road vehicle tracking and map learning\n",
    "\n",
    "Ideas\n",
    "Rao-Blackwellization (use Kalman filter for the linear part of the dynamics model)\n",
    "\n",
    "Initial Approach:\n",
    "Use Newson and Krumm, but modify it to use particle filters instead of HMM.\n",
    "\n",
    "So, for a given route, proceed sequentially over obs, maintaining dist of probable road segments\n",
    "\n",
    "Assumptions (N&K):\n",
    "-remove obs that are not 2*meas dist sigma from previous obs  (eliminate 39% of data in N&K)\n",
    "-ignore roads 200m from obs\n",
    "-zeroize very unlikely particles\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>time</th>\n",
       "      <th>lat_prev</th>\n",
       "      <th>long_prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:58:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:57:39</td>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:55:40</td>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:54:49</td>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.75015</td>\n",
       "      <td>-122.39256</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:50:37</td>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       long  occupancy                time  lat_prev  long_prev\n",
       "0  37.75134 -122.39488          0 2008-06-10 07:58:07       NaN        NaN\n",
       "1  37.75136 -122.39527          0 2008-06-10 07:57:39  37.75134 -122.39488\n",
       "2  37.75199 -122.39460          0 2008-06-10 07:55:40  37.75136 -122.39527\n",
       "3  37.75080 -122.39346          0 2008-06-10 07:54:49  37.75199 -122.39460\n",
       "4  37.75015 -122.39256          0 2008-06-10 07:50:37  37.75080 -122.39346"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = data[[\"lat\", \"long\", \"lat_prev\", \"long_prev\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  37.75134, -122.39488,        nan,        nan],\n",
       "       [  37.75136, -122.39527,   37.75134, -122.39488],\n",
       "       [  37.75199, -122.3946 ,   37.75136, -122.39527],\n",
       "       [  37.7508 , -122.39346,   37.75199, -122.3946 ],\n",
       "       [  37.75015, -122.39256,   37.7508 , -122.39346]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan, -122.39488, -122.39527, -122.3946 , -122.39346])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:,3][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def great_circle_dist(df):\n",
    "    \"\"\"\n",
    "    USED BELOW IN PREPROCESSING\n",
    "    df numpy array [lat, long, lat_prev, long_prev] in degrees\n",
    "    \n",
    "    Returns great-circle distance between point (lat,long) columns and (lat_prev, long_prev) columns, \n",
    "    in meters.\n",
    "    \n",
    "    Modified from https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    df = df.copy()\n",
    "    df = np.deg2rad(df)\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = df[:, 3] - df[:, 1] # lon2 - lon1 \n",
    "    dlat = df[:, 2] - df[:, 0] # lat2 - lat1 \n",
    "    a = np.add(np.square(np.sin(dlat / 2)),\n",
    "               np.multiply(np.cos(df[:, 0]), \n",
    "                           np.multiply(np.cos(df[:, 2]), np.square(np.sin(dlon / 2)))\n",
    "                          )\n",
    "              )\n",
    "    # a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = np.arcsin(np.sqrt(a)) * 2\n",
    "    # c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return r * c * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gps_transition_model:\n",
    "    \"\"\"\n",
    "    Transition model to predict probability of next state given current state; simulate next state.\n",
    "    GPS trace map matching use case. \n",
    "    \n",
    "    N&K use as a proxy an exponential function of the difference in the great-circle distance \n",
    "    between previous observations and previous road points (similar route distance).\n",
    "    Road points use map/ground-truth data. \n",
    "    -Ignore roads 200m from obs\n",
    "    -If a calculated route would require the vehicle to exceed a speed of 50 m/s (112 miles per hour), zeroize\n",
    "    \n",
    "    !!! TODO: Get driving distance from route planner??? Results anticipated to significantly suffer otherwise. !!! \n",
    "    \n",
    "    Other ideas: using dead reckoning.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta=3.0\n",
    "#                  , prob_floor=0.05\n",
    "                 , normalize=True\n",
    "                ):\n",
    "        \"\"\"\n",
    "        beta, float, parametrizes transition probability function\n",
    "        prob_floor, float between 0 and 1, is probability below which road choices are given zero probability\n",
    "        \n",
    "        # beta = 3 in mapzen, https://www.mapzen.com/blog/data-driven-map-matching/\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "#         self.prob_floor = min(max(prob_floor, 0), 1)\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def estimate_beta(self, data):\n",
    "        \"\"\"\n",
    "        NOT USED\n",
    "        \n",
    "        Using rescaled median absolute deviation (MAD). \n",
    "        Ideally using ground truth data (none here).\n",
    "        \"\"\" \n",
    "        self.beta = 3.0  # None\n",
    "        \n",
    "    def prob_transition(self, state_last2, obs_last2, dist_obs_m_prev, time_delta_sec_prev, \n",
    "                        max_allowable_diff_m=2000.0, max_allowable_mph=112.0):\n",
    "        \"\"\"\n",
    "        state_last2, float numpy array, (k x 4 lat, long, lat_prev, long_prev), k candidate roads\n",
    "        obs_last2, float numpy array, (1 x 4 lat, long, lat_prev, long_prev) \n",
    "        dist_obs_m_prev float distance in meters between last two obs.\n",
    "        time_delta_sec_prev float time delta in sec between last two obs.\n",
    "        max_allowable_diff_m float, ignore low probability routes as in N&K; in N&K 2000 meters.\n",
    "        max_allowable_mph, float; ignore routes resulting in unreasonable speeds. 112mph in N&K.\n",
    "        Where present, lat/long in degrees.\n",
    "        \n",
    "        returns probs of transition, (k x 1)\n",
    "        \"\"\"\n",
    "        RADIUS_OF_EARTH_M = 6371000\n",
    "        MILES_PER_METER = 0.000621371\n",
    "        HOURS_PER_SECOND = 3600.0\n",
    "        state_last2 = np.deg2rad(state_last2)\n",
    "        obs_last2 = np.deg2rad(obs_last2)\n",
    "        # dist_obs = abs(haversine_distances(obs_last2[:, :2], obs_last2[0, 2:]))  # (1, 1)\n",
    "        # !! TODO - Supposed to be driving distance, not haversine. N&K use a route planner. Can I use shapely???\n",
    "        dist_road = np.abs(haversine_distances(state_last2[:, :2], state_last2[:, 2:]))  # (k, 1)\n",
    "        diff_dist = np.abs(np.subtract(dist_road, dist_obs_m_prev))  # (k, 1)\n",
    "        diff_dist = diff_dist * RADIUS_OF_EARTH_M  # multiply by Earth radius to get result in meters\n",
    "        \n",
    "        # Ignore cases where diff_dist >= max_allowable_diff_m\n",
    "        diff_dist[diff_dist >= max_allowable_diff_m] = np.float(\"inf\")  # zero out these transition probs\n",
    "        # Ignore cases where implied speed of route is >= max_allowable_mph; calculate speed in candidate routes\n",
    "        implied_speed_road = (dist_road * RADIUS_OF_EARTH_M * MILES_PER_METER).divide(\n",
    "            time_delta_sec_prev * HOURS_PER_SECOND)  # (k, 1), meters\n",
    "        diff_dist[implied_speed_road >= max_allowable_mph] = np.float(\"inf\")  # zero out these transition probs\n",
    "        \n",
    "        probs = np.exp(-diff_dist / self.beta) * (1 / self.beta)\n",
    "        # Normalize result?\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(np.float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gps_sensor_model:\n",
    "    \"\"\"\n",
    "    Sensor model to predict likelihood of an obs given a particle.\n",
    "    GPS trace map matching use case. \n",
    "    \n",
    "    I don't have ground truth, so can't use ML easily, but\n",
    "    N&K use as a proxy a normal distribution of great-circle distance between observation and road, with sigma \n",
    "    estimated from the data.\n",
    "    Road points use map data. \n",
    "    -Zerioze low probability particles (diff in route distance of 2000 m. or more). \n",
    "    \n",
    "    Other ideas: semi-supervised learning. \n",
    "    \n",
    "    Thoughts: need efficient representation for road network that returns connecting roads/nodes for a given node\n",
    "    Want to represent it as a graph/network. \n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=4.07, normalize=True):\n",
    "        self.sigma = sigma\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def estimate_sigma(self, data):\n",
    "        \"\"\"\n",
    "        NOT USED \n",
    "        \n",
    "        Using Gather and Schultze, median-based. \n",
    "        Ideally using ground truth data (none here).\n",
    "        \n",
    "        I guess I could manually match some trips, then use that??\n",
    "        \"\"\" \n",
    "        self.sigma = 4.07 # meters; I have no ground truth so using N&K's empirical estimate\n",
    "#         self.sigma = None # median absolute deviation (MAD) formula adjusted \n",
    "    \n",
    "    def estimate_likelihood(self, states, obs):\n",
    "        \"\"\"\n",
    "        return probability of seeing that obs given potential road state hypotheses, (k x 1)\n",
    "        \n",
    "        states Particles numpy array representing road position hypotheses, (k x 2 lat long).\n",
    "        obs (1 x 2 lat long)\n",
    "        \"\"\"\n",
    "        RADIUS_OF_EARTH_M = 6371000\n",
    "        dist_obs_roads = np.abs(haversine_distances(states, obs))  # (k x 1)\n",
    "        dist_obs_roads = dist_obs_roads * RADIUS_OF_EARTH_M  # multiply by Earth radius to get result in meters\n",
    "        \n",
    "        probs = \\\n",
    "        np.exp(np.power(dist_obs_roads / self.sigma, 2) * (-0.5)) * \\\n",
    "        (1 / (math.sqrt(2 * math.pi) * self.sigma))\n",
    "        \n",
    "        # Normalize result?\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class particle_filter:\n",
    "    \"\"\"\n",
    "    Fit a particle filter to data given a transition and sensor model. \n",
    "    \n",
    "    n, integer, number of particles to maintain at each step in time.\n",
    "    max_road_dist_m, float, max road distance above which candidate roads aren't considered (meters). \n",
    "    prob_floor, float between 0 and 1, min probability below which candidate roads aren't considered.\n",
    "    Sensor model, , is some model object we will use to predict the likelihood.\n",
    "    Transition model, , is some model object we will use to predict next state.\n",
    "    ## max_iter, integer, the maximum iterations used in fitting the particle filter.\n",
    "    ## conv_tol, the convergence tolerance that will trigger early termination of fitting the particle filter. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=50, max_road_dist_m=200, prob_floor=0.01, \n",
    "                 sensor_model=None, transition_model=None, \n",
    "                 viterbi_trellis=[], viterbi_trellis_idx=[]):\n",
    "        self.n = n\n",
    "        self.max_road_dist_m = max_road_dist_m\n",
    "        self.prob_floor = min(max(prob_floor, 0), 1)\n",
    "        self.particles = np.array([])\n",
    "        self.weights = np.ones((1, self.n))\n",
    "        \n",
    "        self.sensor_model = sensor_model\n",
    "        self.transition_model = transition_model\n",
    "        self.viterbi_trellis = viterbi_trellis\n",
    "        self.viterbi_trellis_idx = viterbi_trellis_idx\n",
    "        \n",
    "        self.obs = []  # Record history of observations (lat, long)\n",
    "        \n",
    "    def get_candidate_roads(self, obs):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        Leverage shapely.\n",
    "        \n",
    "        Ignore roads 200m from obs.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def estimate_weights_per_particle(self, sampled_states, obs):\n",
    "        \"\"\"\n",
    "        Estimate likelihood of particle given evidence, P(evidence|particle).\n",
    "        Uses sensor model.\n",
    "        \n",
    "        Obs is single observation at time t, numpy array. \n",
    "        \"\"\"\n",
    "        probs = self.sensor_model.estimate_likelihood(sampled_states, obs)\n",
    "        return probs\n",
    "        \n",
    "    def apply_transition_model(self, candidate_roads, dist_prev, time_delta_prev):\n",
    "        \"\"\"\n",
    "        returns probabilities of transition, given current particles\n",
    "        numpy array of floats between 0 and 1, normalized, (n * c, 1)\n",
    "        \n",
    "        candidate_roads numpy array float of lat,long closest point to road from obs (N&K)\n",
    "        dist_prev float, meters\n",
    "        time_delta_prev float, sec\n",
    "        \n",
    "        Zeroize very small probability candidates, but don't change shape of particles.\n",
    "        Then, if some particles are very unlikely, we cull them and resample from more likely particles. \n",
    "        \"\"\"\n",
    "        # Crossproduct for each particle, all roads\n",
    "        state_last2 = np.concatenate([np.concatenate((self.particles, \n",
    "                                                      np.tile(c, (self.n, 1))), \n",
    "                                                     axis=1) \n",
    "                                      for c in candidate_roads], axis=0)  # (n * c, 4)\n",
    "        obs_last2 = np.array(self.obs[-2:]).reshape((1, 4))\n",
    "        probs = self.transition_model.prob_transition(state_last2, obs_last2, \n",
    "                                                      dist_prev, time_delta_prev)  # (n * c, 1)\n",
    "        # Effectively, candidate roads will be possible from different original particle hypotheses\n",
    "        # Zeroise small probs, without changing array shape\n",
    "        probs[probs <= self.prob_floor] = 0.0\n",
    "        # Re-normalize\n",
    "        probs = probs / np.sum(probs)\n",
    "        return probs\n",
    "        \n",
    "    def update_dist(self, obs, num_iter):\n",
    "        \"\"\"\n",
    "        Re-sample particles from transition model given re-estimated likelihood of existing particles.\n",
    "        Weighted sample with replacement. \n",
    "        \n",
    "        obs list of floats [lat, long, dist_from_prev_m, time_delta_sec_prev]\n",
    "        \n",
    "        Note: if find no solutions, need to remove points in a signal break until HMM 'heals.'\n",
    "        If break > 180 sec, separate into two trips.\n",
    "        \"\"\"\n",
    "        # Update stored obs\n",
    "        obs_coords = obs[:2]\n",
    "        dist_prev, time_delta_prev = obs[2:]\n",
    "        self.obs.append(obs_coords)\n",
    "        # Get candidate roads given obs\n",
    "        candidate_roads = self.get_candidate_roads(obs_coords) # ignore distance\n",
    "        # Get transition probs; for first observation, ignore this part of the algorithm \n",
    "        # and treat sensor probs as prior probabilities (N&K.\n",
    "        if num_iter > 1:\n",
    "            trans_probs = self.apply_transition_model(candidate_roads, \n",
    "                                                      dist_prev, \n",
    "                                                      time_delta_prev) # (n * c, 1) first n rows for first candidate, etc.\n",
    "            # Aggregate probs by candidate roads\n",
    "            trans_probs_split = np.split(trans_probs, self.n, axis=0)  # (c, n)\n",
    "            trans_probs_agg = np.sum(trans_probs_split, axis=1)  # (c, 1)\n",
    "            # Sample new states\n",
    "            sampled_states_idx = np.random.choice(range(len(candidate_roads)), \n",
    "                                                  self.n, \n",
    "                                                  trans_probs_agg)  # (1, n)\n",
    "            sampled_states = candidate_roads[sampled_states_idx]  # (n, 2)\n",
    "        else:\n",
    "            sampled_states = candidate_roads\n",
    "        \n",
    "        # Get sensor probs\n",
    "        sensor_probs = self.estimate_weights_per_particle(sampled_states, obs_coords)  # (n, 1)\n",
    "        \n",
    "        # Joint prob, for viterbi backtracking. \n",
    "        if num_iter > 1:\n",
    "            joint_prob = np.multiply(\n",
    "                np.multiply(sensor_probs, np.max(trans_probs_split, axis=1)[sampled_states_idx]),  # (n, 1)\n",
    "                self.weights\n",
    "                )\n",
    "        else:\n",
    "            joint_prob = sensor_probs\n",
    "        self.weights = joint_prob  # Effectively setting prior probabilites in iter 1\n",
    "        \n",
    "        # Sample new particles\n",
    "        new_particles_idx = np.random.choice(range(sampled_states.shape[0]), \n",
    "                                             self.n, \n",
    "                                             sensor_probs)  # (1, n)\n",
    "        new_particles = sampled_states[new_particles_idx]  # (n, 2)\n",
    "        \n",
    "        # Best prior state/particle for a given candidate state, for viterbi backtracking. \n",
    "        if num_iter > 1:\n",
    "            best_prior_state_idx = np.argmax(trans_probs_split, axis=1)[sampled_states_idx[new_particles_idx]]  # (1, n)\n",
    "            best_prior_state = self.particles[best_prior_state_idx]  # (n, 2)\n",
    "            self.viterbi_trellis.append(best_prior_state)\n",
    "            self.viterbi_trellis_idx.append(best_prior_state_idx)\n",
    "        \n",
    "        self.particles = new_particles\n",
    "        \n",
    "        # Estimate current particle filter fit quality of hypotheses to data; should research good metrics more.\n",
    "            # Came up with this on my own. \n",
    "        fit_quality = [np.max(self.weights), np.mean(self.weights), np.median(self.weights)]\n",
    "        \n",
    "        return fit_quality\n",
    "        \n",
    "    def fit(self, data#, max_iter=100, conv_tol=0.001\n",
    "           ):\n",
    "        \"\"\"\n",
    "        Iterate over rows in data, training particle filter. \n",
    "        Rows assumed to be sequentially ordered. \n",
    "        \n",
    "        data, numpy array (num obs, data dim)\n",
    "        data cols include exactly lat, long, lat_prev, long_prev, dist_from_prev_m, time_delta_sec_prev (all floats)\n",
    "        \"\"\"\n",
    "        num_iter = 0\n",
    "        converged = False  # Is this relevant for particle filters?\n",
    "#         while num_iter < max_iter and not converged:\n",
    "#             num_iter += 1\n",
    "        for obs in data:\n",
    "            num_iter += 1\n",
    "            fit_quality = self.update_dist(obs, num_iter)\n",
    "            # Shouldn't I use DP/Viterbi for this?? \n",
    "            # Nearest neighbor filter, hungarian algorithm?? p. 601\n",
    "            # Best at any given point in time will suffer in the beginning before particle dist has converged, ie\n",
    "            # during the burn-in period.\n",
    "            # So, seems can still use DP, but proceed backward in time. Wait, that is Viterbi :) \n",
    "                # Happily I have a finite state space. \n",
    "            print(\"On iteration %d, fit quality of MAX %3.2f, MEAN %3.2f, MEDIAN %3.2f\" % \n",
    "                  (num_iter, fit_quality[0], fit_quality[1], fit_quality[2]))\n",
    "        print(\"Done.\")\n",
    "        return fit_quality\n",
    "    \n",
    "    def viterbi(self, y_vector_arr):\n",
    "        \"\"\"\n",
    "        Returns list of backtracked states (float lat long),\n",
    "        the imputed GPS trace that has been 'snapped' to roads.\n",
    "        Use Viterbi to get optimal path via DP - \n",
    "        happily I have a finite state space due to my constraint \n",
    "        to only maintain n particles. \n",
    "        \n",
    "        Note, Viterbi relies on the Markov property, which\n",
    "        can apply here. \n",
    "        \"\"\"\n",
    "        # Start with the last observation to the viterbi trellis\n",
    "        best_last_state_idx = np.argmax(self.weights)\n",
    "        best_last_state = self.particles[best_last_state_idx]\n",
    "        backtracked_states = [best_last_state]\n",
    "        # Backtrack through the viterbi trellis (#obs, n, 2) actual lat/long states\n",
    "        for j in range(len(self.viterbi_trellis) - 1, -1, -1):\n",
    "            best_last_state_idx = self.viterbi_trellis_idx[j][best_last_state_idx]\n",
    "            best_last_state = self.viterbi_trellis[j][best_last_state_idx]\n",
    "            backtracked_states.append(best_last_state)\n",
    "        # Put in chronological order\n",
    "        backtracked_states = backtracked_states[::-1]\n",
    "        return backtracked_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## it seems i only have beijing data (10K) or sf taxi data (500)\n",
    "# Read in taxi data\n",
    "# Each data of different length; ideal use case for pyspark\n",
    "# Note, the OSM extract basemap data has POI info as well (https://download.bbbike.org)\n",
    "# also try uber h3 spatial index\n",
    "trace_dir = '/Volumes/LaCie/datasets/ms_taxi/taxi_log_2008_by_id/'   # MS Taxi\n",
    "basemap_dir = '/Volumes/LaCie/datasets/Beijing-shp/shape/'\n",
    "\n",
    "trace_dir = '/Volumes/LaCie/datasets/cabspottingdata/'   # CRAWDAD cabspotting\n",
    "basemap_dir = '/Volumes/LaCie/datasets/SanFrancisco-shp/shape/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 ms, sys: 3.62 ms, total: 5.76 ms\n",
      "Wall time: 35.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_files = [f for f in os.listdir(trace_dir) if re.match(r'new_.*\\.txt', f)]  # glob.glob(trace_dir + \"new_*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.99 s, sys: 1.38 s, total: 9.36 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 20.9s cabspottingdata; can try spark or dask? or multiprocessing, joblib\n",
    "trace_list = []\n",
    "for file_ in all_files:\n",
    "    file_df = pd.read_csv(os.path.join(trace_dir, file_), sep=\" \", index_col=None, header=None, \n",
    "                          names=['lat', 'long', 'occupancy', 'time'])\n",
    "    trace_list.append(file_df)\n",
    "\n",
    "# concatenate all dfs into one\n",
    "trace_df = pd.concat(trace_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace_df.loc[:, [\"time\"]] = pd.to_datetime(trace_df.time, origin=\"unix\", unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lat                 float64\n",
       "long                float64\n",
       "occupancy             int64\n",
       "time         datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cabspotting:\n",
    "\n",
    "latitude and longitude are in decimal degrees, \n",
    "occupancy shows if a cab has a fare (1 = occupied, 0 = free) and \n",
    "time is in UNIX epoch format\n",
    "\"\"\"\n",
    "trace_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.75134</td>\n",
       "      <td>-122.39488</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:58:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.75136</td>\n",
       "      <td>-122.39527</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:57:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.75199</td>\n",
       "      <td>-122.39460</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:55:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.75080</td>\n",
       "      <td>-122.39346</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:54:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.75015</td>\n",
       "      <td>-122.39256</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-10 07:50:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat       long  occupancy                time\n",
       "0  37.75134 -122.39488          0 2008-06-10 07:58:07\n",
       "1  37.75136 -122.39527          0 2008-06-10 07:57:39\n",
       "2  37.75199 -122.39460          0 2008-06-10 07:55:40\n",
       "3  37.75080 -122.39346          0 2008-06-10 07:54:49\n",
       "4  37.75015 -122.39256          0 2008-06-10 07:50:37"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in ...shapefiles? GeoJSON? Which format is best? For parallelization may be one thing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_traces(df, sigma=4.07):\n",
    "    \"\"\"\n",
    "    pandas df, trace_df above with float columns lat, long in degrees\n",
    "    sigma float, representing std dev of measurement error (see above in particle filter class)\n",
    "    \n",
    "    \"The justification for this step is that until we see a point that is at least 2𝜎𝑧 \n",
    "    away from its temporal predecessor, our confidence is low that the apparent movement\n",
    "    is due to actual vehicle movement and not noise.\" (N&K)\n",
    "    \n",
    "    -remove obs that are not 2*meas dist sigma from previous obs  (eliminate 39% of data in N&K)\n",
    "    -Letting sigma = 4.07 meters as in N&K\n",
    "    \"\"\"\n",
    "    MILES_PER_METER = 0.000621371\n",
    "    HOURS_PER_SECOND = 3600.0\n",
    "    data = df.copy()\n",
    "    data[[\"lat_prev\", \"long_prev\"]] = data[[\"lat\", \"long\"]].shift(1)\n",
    "    data[[\"dist_from_prev_m\"]] = great_circle_dist(data[[\"lat_prev\", \"long_prev\", \"lat\", \"long\"]].values)  # 1.93 sec\n",
    "    # Add speed so can later filter on unreasonably high speeds\n",
    "    data[[\"time_delta_sec\"]] = data.time.subtract(data.time.shift(1)) / np.timedelta64(1, 's')  # get seconds\n",
    "    data[[\"speed_mph\"]] = (data.dist_from_prev_m * MILES_PER_METER).divide(data.time_delta_sec * HOURS_PER_SECOND)\n",
    "    # Take cumsum of dist\n",
    "    dist_cum = data.dist_from_prev_m.cumsum()\n",
    "    # Select points closest to multiples of 2*sigma, in cumsum dist\n",
    "    dist_cum_idx = dist_cum // (2 * sigma)\n",
    "    filter_idx = np.subtract(dist_cum_idx, dist_cum_idx.shift(1)) == 0  # 12% of rows eliminated \n",
    "    data = data[~filter_idx]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data_preprocessed = preprocess_traces(trace_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 662 ms, total: 1.88 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dist_cabspotting = great_circle_dist(data[[\"lat_prev\", \"long_prev\", \"lat\", \"long\"]].values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3350723,  982551, 1150459, 1113760,  965676,  765890,  532474,\n",
       "         399776,  272899,  202794]),\n",
       " array([  0.        ,  74.999973  , 149.999946  , 224.999919  ,\n",
       "        299.99989201, 374.99986501, 449.99983801, 524.99981101,\n",
       "        599.99978401, 674.99975701, 749.99973001]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(dist_cabspotting[dist_cabspotting <=750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:map_matching_particle_filter]",
   "language": "python",
   "name": "conda-env-map_matching_particle_filter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
